{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5qS8y5_Ewv6n"
   },
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FE8B9-L8U0aZ"
   },
   "source": [
    "#Semantic word representations\n",
    "\n",
    "\n",
    "Recall that *semantic Word Representations* are representations that are learned to capture the 'meaning' of a word. These are low-dimensional vectors that contain some semantic properties. In this notebook we are going to build state-of-the art approaches to obtain semantic word representations using the **word2vec** modelling approach. We will also use these vectors in some  tasks to understand the utility of these representations. \n",
    "\n",
    "We begin by loading some of the libraries that are necessary for building our model. We are using [pytorch](https://pytorch.org/), an open source deep learning platform, as our backbone library in the course. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XG82efAzvcbK"
   },
   "outputs": [],
   "source": [
    "#@title Loading packages\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from scipy.spatial.distance import euclidean, cosine\n",
    "from tqdm import tqdm \n",
    "import codecs\n",
    "from sklearn.metrics.pairwise import cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "abMSnMwJx8bu"
   },
   "outputs": [],
   "source": [
    "#@title Sample corpora\n",
    "\n",
    "corpus = [\n",
    "    'he is a king',\n",
    "    'she is a queen',\n",
    "    'he is a Man',\n",
    "    'she Is a woman',\n",
    "    'london is, the capital of England',\n",
    "    'Berlin is ... the capital of germany',\n",
    "    'paris is the capital of france.',\n",
    "    'He will eat cake, pie, and/or brownies',\n",
    "    \"she didn't like the brownies\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0hPlpqVFYpfL"
   },
   "source": [
    "# Tokenization\n",
    "\n",
    "Q: What is a token and why do we need to tokenize?  \n",
    "A: A token is a string of contiguous characters between two spaces, or between a space and punctuation marks. For segmentation.\n",
    "\n",
    "Q: Print the tokenized corpus above. What mistakes do you find in the code below?  \n",
    "A: `print([s.split() for s in corpus])`, capitalization inducing sparsity in the dataset. \n",
    "\n",
    "Q: What could be a nice way of fixing these mistakes?  \n",
    "A: Process of normalization (removing capitalization, etc.)\n",
    "\n",
    "##### 10 mins "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UQ5IL1e1GVn6"
   },
   "outputs": [],
   "source": [
    "tokenized_corpus = [] # Let us put the tokenized corpus in a list\n",
    "for sentence in corpus:\n",
    "  tokenized_sentence = []\n",
    "  for token in sentence.split(' '): # simplest split is \n",
    "    # Q3\n",
    "    token = token.lower()\n",
    "    tokenized_sentence.append(token)\n",
    "  tokenized_corpus.append(tokenized_sentence)\n",
    "\n",
    "print(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NPWECLTEmAG9"
   },
   "source": [
    "# Pre-processing\n",
    "\n",
    "Tokenization is a crucial pre-processing step in the NLP domain`*`. However, other pre-processing techniques also exist, many of which were extensively employed in rule-based and statistical NLP. While we don't utilise these pre-processing techniques in neural-based NLP anymore, they are still worth a recap. Typically, **stop words** and **punctuation** removal are employed, along with *either* **stemming** or **lemmatization**. However, in the following code, we will demonstrate each of the techniques separately (mainly due to our corpus being so small)\n",
    "\n",
    "### Stop Word removal\n",
    "Stop words are generally the most common words in the language which who's meaning in a sequenece is ambiguous. Some examples of stop words are: The, a, an, that.\n",
    "\n",
    "### Punctuation removal\n",
    "Old school NLP techniques (and some modern day ones) struggle to understand the semantics of punctuation. Thus, they were also removed.\n",
    "\n",
    "## Stemming and Lemmatization\n",
    "Stemming and Lemmatization are two distinct word normalization techniques. Essentially this means that, given our corpora, we wish to have variants of a word in a 'normal' form. For example, [playing, plays, played] may be normalised to \"Play\". The sentence \"the boy's cars are different colours\" may be normalised to \"the boy car be differ colour\"\n",
    "\n",
    "### Stemming\n",
    "In the case of stemming, we want to normalise all words to their stem (or root). The stem is the part of the word to which affixes (suffixes or prefixes) are assigned. Stemming a word may result in the word not actually being a word. For example, some stemming algorithms may stem [trouble, troubling, troubled] as \"troubl\".\n",
    "\n",
    "### Lemmatization\n",
    "Lemmatization attempts to properly reduce unnormalized tokens to a word that belongs in the language. The root word is called a **lemma**, and is the canonical form of a set of words. For example, [runs, running, ran] are all forms of the word \"run.\n",
    "\n",
    "\n",
    "\n",
    "Q. Think of two or three other stop words, and add them to the list of stop words below.  \n",
    "A. [\"is\", \"and\", \"or\"].  \n",
    "**N.B** when we run the punctuation removal below, we see that some of the words with apostrophes are split (e.g. \"didn't\" => \"didn\", \"t\"). Some stop word lists also add the \"t\" to their list. \n",
    "\n",
    "Q. Write some code which both removes stop words and punctuation from our corpus.  \n",
    "A. See code below\n",
    "\n",
    "Q. The examples of stemming and lemmatization below are on words/sequences not in our corpus. Extend the code so it works on our corpus.  \n",
    "A. See code below\n",
    "\n",
    "##### 10 mins \n",
    "\n",
    "N.B. We are not going to use these techniques in this file after this section, so we will demonstrate how to perform these techniques distinctly on our toy corpus.\n",
    "\n",
    "`*`Recently there has been newer approaches to \"tokenization\" which goes further than one token being one word. One example is [SentencePiece](https://github.com/google/sentencepiece). These approaches are out of scope for this lab session, but may appear in future sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k_DGoHbPmAG-"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # Download the tokenizer model\n",
    "nltk.download('wordnet') # Download the wordnet corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FuSjoUpfmAHA"
   },
   "outputs": [],
   "source": [
    "# STOP WORD REMOVAL\n",
    "stop_words_list = [\"the\", \"a\", \"an\", \"that\"]\n",
    "# Q1\n",
    "stop_words_list.extend([\"is\", \"and\", \"or\"])\n",
    "\n",
    "# SWR = stop words removed\n",
    "tokenized_corpus_SWR = []\n",
    "for sentence in corpus:\n",
    "    tokenized_sentence_SWR = []\n",
    "    \n",
    "    for token in sentence.split(\" \"):\n",
    "        if token not in stop_words_list:\n",
    "            tokenized_sentence_SWR.append(token)\n",
    "\n",
    "    if tokenized_sentence_SWR: # Only append to corpus if tokenized_sentence_SWR isn't empty\n",
    "        tokenized_corpus_SWR.append(tokenized_sentence_SWR)\n",
    "        \n",
    "print(tokenized_corpus_SWR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3VtK8AQ_mAHC"
   },
   "outputs": [],
   "source": [
    "# PUNCTUATION REMOVAL\n",
    "import re # regex\n",
    "\n",
    "re_punctuation_string = '[\\s,/.\\']'\n",
    "\n",
    "# PR = punctuation removed\n",
    "tokenized_corpus_PR = []\n",
    "for sentence in corpus:\n",
    "    tokenized_sentence_PR = re.split(re_punctuation_string, sentence) # in python's regex, [...] is an alternative to writing .|.|.\n",
    "    tokenized_sentence_PR = list(filter(None, tokenized_sentence_PR)) # remove empty strings from list \n",
    "    tokenized_corpus_PR.append(tokenized_sentence_PR)\n",
    "        \n",
    "print(tokenized_corpus_PR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YyzTW_wXmAHF"
   },
   "outputs": [],
   "source": [
    "# ANSWER Q2 HERE\n",
    "stop_words_list = stop_words_list # just for clarity ;)\n",
    "tokenized_corpus_SwPR = [] # SwPR = Stop Words and Puncutation Removal\n",
    "for sentence in corpus:\n",
    "    tokenized_sentence_PR = re.split(re_punctuation_string, sentence)\n",
    "    tokenized_sentence_PR = list(filter(None, tokenized_sentence_PR))\n",
    "    \n",
    "    # Now that punctuation has been removed, let's remove the stop workds\n",
    "    tokenized_sentence_SwPR = []\n",
    "    for token in tokenized_sentence_PR:\n",
    "        if token not in stop_words_list:\n",
    "            tokenized_sentence_SwPR.append(token)\n",
    "    \n",
    "    if tokenized_sentence_SwPR:\n",
    "        tokenized_corpus_SwPR.append(tokenized_sentence_SwPR)\n",
    "        \n",
    "print(tokenized_corpus_SwPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NZGX31WamAHH"
   },
   "outputs": [],
   "source": [
    "# STEMMING\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "stemming_word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Stemmed variant\"))\n",
    "print()\n",
    "\n",
    "for word in stemming_word_list:\n",
    "      print(\"{0:20}{1:20}\".format(word,porter.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3 - STEMMING\n",
    "# We'll use the tokenized corpus from the \"Tokenization\" section\n",
    "tokenized_corpus_stemmed = []\n",
    "for t_sentence in tokenized_corpus:\n",
    "    sentence_stemmed = []\n",
    "    for token in t_sentence:\n",
    "        sentence_stemmed.append(porter.stem(token))\n",
    "    tokenized_corpus_stemmed.append(sentence_stemmed)\n",
    "    \n",
    "print(tokenized_corpus_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Muvs2GZ4mAHK"
   },
   "outputs": [],
   "source": [
    "# LEMMATIZATION\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "to_lemmatize_sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "\n",
    "# lemmatization requires punctuation removal\n",
    "to_lemmatize_sentence = re.split(re_punctuation_string, to_lemmatize_sentence)\n",
    "to_lemmatize_sentence = list(filter(None, to_lemmatize_sentence))\n",
    "\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "print()\n",
    "\n",
    "for word in to_lemmatize_sentence:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CL6qkUJPmAHM"
   },
   "outputs": [],
   "source": [
    "# Why didn't the above do anything?\n",
    "# It's because the lemmatizer requires parts of speech (POS) context about the word it is currently parsing.\n",
    "# We would need to use a POS model to identify what the POS for a token in its context is.\n",
    "# In the above example (and for Q3), we'll just pass in the VERB context for every token\n",
    "\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "print()\n",
    "\n",
    "for word in to_lemmatize_sentence:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word, pos=\"v\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qXfoaPJ3mAHP"
   },
   "outputs": [],
   "source": [
    "# Q3 - Lemmatization\n",
    "re_punctuation_string = '[\\s,/.\\']'\n",
    "\n",
    "tokenized_corpus_to_lemmatize = []\n",
    "for sentence in corpus:\n",
    "    to_lemmatize_sentence = re.split(re_punctuation_string, sentence)\n",
    "    to_lemmatize_sentence = list(filter(None, to_lemmatize_sentence))\n",
    "    tokenized_corpus_to_lemmatize.append(to_lemmatize_sentence)\n",
    "\n",
    "tokenized_corpus_lemmatized = []\n",
    "for t_sentence in tokenized_corpus_to_lemmatize:\n",
    "    lemmatized_sentence = []\n",
    "    for token in t_sentence:\n",
    "        lemmatized_token = wordnet_lemmatizer.lemmatize(token, pos=\"v\")\n",
    "        lemmatized_sentence.append(lemmatized_token)\n",
    "    tokenized_corpus_lemmatized.append(lemmatized_sentence)\n",
    "    \n",
    "print(tokenized_corpus_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that other NLP tools, such as [SpaCy](https://spacy.io/), or [Stanza](https://stanfordnlp.github.io/stanza/) are popular alternatives which provide higher levels of abstractions than NLTK. When working on an NLP task, the use of one of those two libraries is recommended over NLTK.\n",
    "\n",
    "The code below will run through implementing a Word2Vec algorithm from scratch. A fuller and wholesome tutorial can be found in the \"DeeperDiveIntoWordEmbeddings.zip\" folder. Feel free to take a read through the notebook there if you want more information, or if you found yourself struggling to understand every concept from the taught theory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cyKCfY6QbO0r"
   },
   "source": [
    "# Vocabulary\n",
    "\n",
    "The code below obtains the vocabulary of the corpus. \n",
    "\n",
    "Q. Print the size of the vocabulary.  \n",
    "A. `print(len(vocabulary))`\n",
    "\n",
    "Q. A programatically cleaner (and shorter) way of writing the code below by using a set instead of a list. Can you implement the code below using a set?  \n",
    "A. See code below\n",
    "\n",
    "##### 3 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9bWQ3zKYKlQU"
   },
   "outputs": [],
   "source": [
    "vocabulary = [] # Let us put all the tokens (mostly words) \n",
    "                # appearing in the vocabulary in a list\n",
    "  \n",
    "for sentence in tokenized_corpus:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "\n",
    "\n",
    "# Q. what is the size of the vocabulary?\n",
    "# A. uncomment and fill below\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(\"Vocabulary size:\", vocabulary_size)\n",
    "\n",
    "\n",
    "## USING A SET\n",
    "vocab_set = set()\n",
    "for sentence in tokenized_corpus:\n",
    "    vocab_set.update(sentence)\n",
    "\n",
    "# Sanity check to ensure that the set size is the same as the list size\n",
    "print(\"Vocabulary (set) size:\", len(vocab_set))\n",
    "assert len(vocab_set) == len(vocabulary)\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xGg6CtALe8Va"
   },
   "source": [
    "# Helper functions \n",
    "\n",
    "* These are some of the common helper functions that are used for NLP models:\n",
    "\n",
    "    * `word2idx`:  Maintains a dictionary of word and the corresponding index\n",
    "    \n",
    "    * `idx2word`: Maintains a mapping from index to word \n",
    "    \n",
    "    \n",
    "* Print the word2idx and idx2word, we will be using these in future exercises. \n",
    "\n",
    "##### 3 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4LWut1gtXGQN"
   },
   "outputs": [],
   "source": [
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "print(word2idx)\n",
    "print(idx2word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1bFjK86NPChI"
   },
   "source": [
    "# Look-up table \n",
    "\n",
    "* This is a table that maps from an index to a one hot vector. \n",
    "\n",
    "Q. Print one-hot vectors corresponding to the words 'the', 'he' and ''england'  \n",
    "A. See code below\n",
    "\n",
    "##### 3 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vp8OTZI-UrYU"
   },
   "outputs": [],
   "source": [
    "\n",
    "def look_up_table(word_idx):\n",
    "    x = torch.zeros(vocabulary_size).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x\n",
    "  \n",
    "# This is a one hot representation\n",
    "\n",
    "# Q. try printing it for word_idx = 1\n",
    "\n",
    "\n",
    "word_idx = word2idx['he']\n",
    "print(look_up_table(word_idx))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_for = [\"the\", \"he\", \"england\"]\n",
    "\n",
    "for word in word_vectors_for:\n",
    "    print(\"Word vector for word _{}_: \\n {}\".format(word, look_up_table(word2idx[word])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PcAI5BtaQMFh"
   },
   "source": [
    "# Extracting contexts and the focus word\n",
    "\n",
    "\n",
    "Recall that we are building the skip-gram model. \n",
    "\n",
    "**We first begin by obtaining the set of contexts and focus words.**\n",
    "* Let's say we have a sentence (represented as vocabulary indicies): `[0, 2, 3, 6, 7]`.\n",
    "* For every word in the sentence, we want to get the words which are `window_size` around it.\n",
    "* So if `window_size==2`, for the word '0', we obtain: `[[0, 2], [0, 3]]`\n",
    "* For the word '2', we obtain: `[[2, 0], [2, 3], [2, 6]]`\n",
    "* For the word '3', we obtain: `[[3, 0], [3, 2], [3, 6], [3, 7]]`\n",
    "\n",
    "Q. Print some of the index pairs and trace them back to their words.  \n",
    "A. See code below\n",
    "\n",
    "##### 10 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wrtOwTUsyArb"
   },
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "\n",
    "idx_pairs = []\n",
    "\n",
    "# variables of interest: \n",
    "#   center_word_pos: center word position\n",
    "#   context_word_pos: context_word_position\n",
    "#   add sentence length as a constraint\n",
    "\n",
    "for sentence in tokenized_corpus:\n",
    "    indices = [word2idx[word] for word in sentence]\n",
    "    \n",
    "    for center_word_pos in range(len(indices)):\n",
    "        \n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_word_pos = center_word_pos + w\n",
    "            \n",
    "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                continue\n",
    "                \n",
    "            context_word_idx = indices[context_word_pos]\n",
    "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array\n",
    "\n",
    "print(idx_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll sample 5 elements at random and trace these back to their word pairs\n",
    "from random import sample\n",
    "\n",
    "# Hey, I'm gonna be honest here.. I didn't build this part of the lab and while casting idx_pairs\n",
    "  # to np.array is the correct thing to do, finding the original words from these elements requires a bit\n",
    "  # of a hack job if done after the cast.\n",
    "  # This means that to randomly sample, we need to cast idx_pairs back to a list.\n",
    "    # Now we have list of arrays which is a bit easier to work with\n",
    "random_pairs = sample(list(idx_pairs), 5)\n",
    "print(random_pairs)\n",
    "\n",
    "tokens_from_idx_pairs = []\n",
    "for random_pair in random_pairs:\n",
    "    focus_word_idx, context_word_idx = random_pair[0], random_pair[1]\n",
    "    focus_word = idx2word[focus_word_idx]\n",
    "    context_word = idx2word[context_word_idx]\n",
    "    tokens_from_idx_pairs.append([focus_word, context_word])\n",
    "\n",
    "print()\n",
    "print(tokens_from_idx_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rLzHkq6FRULl"
   },
   "source": [
    "# Parameters and hyperparameters \n",
    "\n",
    "* For our toy task, let us set the embedding dimensions to 5\n",
    "* Let us run the algorithm for 10 epochs (number of times the training algorithm looks at the corpus/training data)\n",
    "* Let us choose the learning rate as 0.001\n",
    "\n",
    "We have two parameter matrices $W_1$ and $W_2$ - the embedding matrix and the weight matrix. \n",
    "\n",
    "Q. What are the dimensionalities of $W_1$ and $W_2$?\n",
    "A.  \n",
    "```\n",
    "shape(W1) = [embedding_dims x vocabulary_size]\n",
    "shape(W2) = [vocabulary_size x embedding_dims]\n",
    "```\n",
    "\n",
    "\n",
    "##### 3 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1kBhE6FeRuDu"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters:\n",
    "embedding_dims = 5\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# The two weight matrices:\n",
    "W1 = torch.randn(embedding_dims, vocabulary_size, requires_grad=True)\n",
    "W2 = torch.randn(vocabulary_size, embedding_dims, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "npzWXeQATPs4"
   },
   "source": [
    "# Training the model\n",
    "\n",
    "(Refer to Lecture 2 slides 30-31)\n",
    "\n",
    "In the code below, we are going to compute the log probability of the correct context (target) given the word. \n",
    "\n",
    "Before running the code, answer the question commented in the code -> fill `y_true`.\n",
    "\n",
    "Print the loss and see if the loss goes down.\n",
    "\n",
    "###### 10 mins\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Cx2eJi1a8R6"
   },
   "outputs": [],
   "source": [
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "  \n",
    "    loss_val = 0\n",
    "    \n",
    "    for data, target in idx_pairs:\n",
    "      \n",
    "        x = torch.Tensor(look_up_table(data)) #, requires_grad=True) # x is a One-hot tensor\n",
    "\n",
    "        # Q. what would y_true be? \n",
    "        y_true = torch.Tensor([target]).long()\n",
    "\n",
    "        # A. [index] of the target word\n",
    "        \n",
    "\n",
    "        # \n",
    "        z1 = torch.matmul(W1, x) \n",
    "        # Q. what is z1? \n",
    "        \n",
    "        z2 = torch.matmul(W2, z1)\n",
    "        # Q. what is the above operation? \n",
    "    \n",
    "        # Let us obtain prediction over the vocabulary\n",
    "        log_softmax = F.log_softmax(z2, dim=0)\n",
    "        \n",
    "        \n",
    "        # Our loss is a negative log-likelihood loss \n",
    "        # (what does this mean?)\n",
    "        \n",
    "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "        \n",
    "        loss_val += loss.item()\n",
    "        \n",
    "        # propagate the error\n",
    "        loss.backward()\n",
    "        \n",
    "        # gradient descent\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "        # zero out gradient accumulation\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()\n",
    "\n",
    "print(f'\\nFinal epoch loss: {loss_val/len(idx_pairs)}')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JPsXq60HUWWq"
   },
   "source": [
    "Q. Given that we are interested in distributed representations, what is the major bottleneck in our setup? Is it the dimensionality of the representations? Is it the learning rate? Is it the corpus?  \n",
    "A. It is both the dimensionality and the corpus. We have a only few words & contexts it would be difficult to capture distributional contexts. As we increase the words or expand the corpus we would have to expand our dimensions. \n",
    "\n",
    "Q. What hyperparameters would you tune to improve the representations?  \n",
    "A. Decreasing the learning rate (this will be discussed more in further lectures)\n",
    "\n",
    "Q. Train the algorithm with a bigger corpus. \n",
    "\n",
    "(You can either copy and paste the corpus and bring it to the same format as the corpus above or use the hint below)\n",
    "\n",
    "###### 10 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ixhvaYkW_SfX"
   },
   "outputs": [],
   "source": [
    "# Example code for getting corpora from the internet\n",
    "import urllib\n",
    "txt = [line.strip() for line in urllib.request.urlopen('https://raw.githubusercontent.com/luonglearnstocode/Seinfeld-text-corpus/master/corpus.txt').readlines()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u2znwTJeU3UT"
   },
   "source": [
    "# Using word embeddings\n",
    "\n",
    "One of the simplest ways of exploiting word representations is to find similar words. There are many ways of measuring the semantic similarity between two words. As we are using word representations which are vectors in the euclidean space, distance metrics defined in the euclidean space are the most popular choice. This is because words that share common contexts in the corpus are located in close proximity to one another in the euclidean space.  One such metric is the eucldeian distance.\n",
    "\n",
    "Q. What is the euclidean distance between 'the' and 'a' (in the sample corpus and the new corpus)?  \n",
    "A. ~1.414 (see solution in code)\n",
    "\n",
    "Q. What other distance metrics can we use for two vectors?  \n",
    "A. Cosine distance is the most frequently used distance metric for high-dimensional space. Many other distance metrics exist and high-level overview can be found in this [blog post](https://towardsdatascience.com/importance-of-distance-metrics-in-machine-learning-modelling-e51395ffe60d) \n",
    "\n",
    "\n",
    "###### 10 mins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NUPETZaOgvgB"
   },
   "outputs": [],
   "source": [
    "# Let us get two vectors from the trained model\n",
    "\n",
    "x = torch.Tensor(look_up_table(0))\n",
    "x_emb = torch.matmul(W1, x).detach().numpy()\n",
    "y = torch.Tensor(look_up_table(1))\n",
    "y_emb = torch.matmul(W1, y).detach().numpy()\n",
    "\n",
    "# let us print the euclidean distance\n",
    "print(euclidean(x_emb, y_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_the = look_up_table(word2idx[\"the\"])\n",
    "vector_a = look_up_table(word2idx[\"a\"])\n",
    "print(euclidean(vector_the, vector_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7UZroMvLtFVv"
   },
   "source": [
    "# ADVANCED: Training with negative sampling \n",
    " \n",
    " \n",
    " \n",
    "\n",
    "Refer to skipgram models in the slides. \n",
    "\n",
    "Q. What happens when we have a very large vocabulary?  \n",
    "A. Computationally challenging because of the normalization factor.\n",
    "\n",
    "Q. What is a negative sample? \n",
    "\n",
    "\n",
    "Below is the code for training the model with negative sampling. \n",
    "\n",
    "\n",
    "##### 10 mins \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lq1lcWbqRwNY"
   },
   "outputs": [],
   "source": [
    "# The two weight matrices:\n",
    "W1 = torch.randn(embedding_dims, vocabulary_size, requires_grad=True)\n",
    "W2 = torch.randn(embedding_dims, vocabulary_size, requires_grad=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for data, target in idx_pairs:\n",
    "        x_var = Variable(look_up_table(data)).float() \n",
    "        \n",
    "        y_pos = Variable(torch.from_numpy(np.array([target])).long())\n",
    "        y_pos_var = Variable(look_up_table(target)).float()\n",
    "        \n",
    "        neg_sample = np.random.choice(list(range(vocabulary_size)),size=(1))[0]\n",
    "        y_neg = Variable(torch.from_numpy(np.array([neg_sample])))\n",
    "        y_neg_var = Variable(look_up_table(neg_sample)).float()\n",
    "         \n",
    "        x_emb = torch.matmul(W1, x_var) \n",
    "        y_pos_emb = torch.matmul(W2, y_pos_var)\n",
    "        y_neg_emb = torch.matmul(W2, y_neg_var)\n",
    "        \n",
    "        # get positive sample score\n",
    "        pos_loss = F.logsigmoid(torch.matmul(x_emb, y_pos_emb))\n",
    "        \n",
    "        # get negsample score\n",
    "        neg_loss = F.logsigmoid(-1 * torch.matmul(x_emb, y_neg_emb))\n",
    "        \n",
    "        loss = - (pos_loss + neg_loss)\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # propagate the error\n",
    "        loss.backward()\n",
    "        \n",
    "        # gradient descent\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "        # zero out gradient accumulation\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()\n",
    "        \n",
    "    if epoch % 10 == 0:    \n",
    "        print(f'Loss at epo {epoch}: {epoch_loss/len(idx_pairs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kGgcB8VHXvPK"
   },
   "source": [
    "* In the current setup, we are only exploiting a very small sample of negative examples. This is suboptimal. \n",
    "\n",
    "* Given a sufficiently large vocabulary, we would ideally sample the negative samples from a noise distribution whose probabilities match the frequency of vocabulary.\n",
    "\n",
    "\n",
    "Q.  Using this code as the basis, build an object oriented negative sampling based model and train it on the fairly large corpus. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y_Eknuozunq5"
   },
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f4VCX4XSqvlu"
   },
   "source": [
    "# Pre-trained representations\n",
    "\n",
    "We have seen from the above that word embeddings are learned in an unsupervised manner, i.e., we don't have any labelled data. These representations can be used to `bootstrap' models in NLP. There are many word representation inducing algorithms : [word2vec](https://arxiv.org/abs/1301.3781), [GloVe](https://nlp.stanford.edu/pubs/glove.pdf), [Fasttext](https://arxiv.org/abs/1607.04606) are some of the popular choices. There are differences in the algorithms but they are all based on the distributional hypothesis. \n",
    "\n",
    "We will now use one of these pre-trained representations: GloVe. \n",
    "\n",
    "Q. What is the dimensionality of the representations below? \n",
    "\n",
    "##### 2 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OCsV8mtBg4-Y"
   },
   "outputs": [],
   "source": [
    "w2i = [] # word2index\n",
    "i2w = [] # index2word\n",
    "wvecs = [] # word vectors\n",
    "\n",
    "# this is a large file, it will take a while to load in the memory!\n",
    "with codecs.open('glove.6B.50d.txt', 'r','utf-8') as f: \n",
    "  index = 0\n",
    "  for line in tqdm(f.readlines()):\n",
    "    # Ignore the first line - first line typically contains vocab, dimensionality\n",
    "    if len(line.strip().split()) > 3:\n",
    "      \n",
    "      (word, vec) = (line.strip().split()[0], \n",
    "                     list(map(float,line.strip().split()[1:]))) \n",
    "      \n",
    "      wvecs.append(vec)\n",
    "      w2i.append((word, index))\n",
    "      i2w.append((index, word))\n",
    "      index += 1\n",
    "\n",
    "w2i = dict(w2i)\n",
    "i2w = dict(i2w)\n",
    "wvecs = np.array(wvecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2cP-UmR5Br4_"
   },
   "source": [
    "For the following experiments, we recommend  using `wvecs` - the pretrained representations. \n",
    "\n",
    "# Evaluating word representation models\n",
    "\n",
    "## Inrtinsic Evaluation\n",
    "\n",
    "* Intrinsic evaluation of word representations involves evaluating  set of word vectors generated by an embedding technique on specific  subtasks that in someways are directly related to the distributional hypothesis. These are typically simple and fast to compute and thereby allow us to help understand representation learning algorithms.\n",
    "\n",
    "* An intrinsic evaluation should typically return to us a scalar quantity that measures the performance of those word vectors on the evaluation subtask.\n",
    "\n",
    "\n",
    "\n",
    "## Word Similarity\n",
    "\n",
    "The first task we consider is evaluating if the representations are good at computing if two words are similar. In this task, you will use both euclidean distance or cosine distance as similarity measures. \n",
    "\n",
    "* Print similarity scores for word pairs in https://github.com/iraleviant/eval-multilingual-simlex/blob/master/evaluation/ws-353/wordsim353-english-sim.txt\n",
    "\n",
    "     (Format of the file: two words and the corresponding human score for the two words)\n",
    "\n",
    "* Obtain pearson's correlation with predicted scores and the human generated scores. \n",
    "\n",
    "\n",
    "##### 15 mins\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S8Kev8-wA1QA"
   },
   "source": [
    "##  Exploring Analogies\n",
    "\n",
    "The second task we consider **completing analogies**. We are given an incomplete analogy of the form: \n",
    "\n",
    "\n",
    "* $a : b : : c :~?$\n",
    "\n",
    "\n",
    "We would then identify the word vector which maximizes the cosine similarity. \n",
    "This metric has an intuitive interpretation. Ideally, we want $\\phi(b) - \\phi(a) = \\phi(d) - \\phi(c)$ where $\\phi(.)$ is the word vector. \n",
    "For instance, \n",
    "\n",
    "* *london $-$ england = paris $-$ france* .\n",
    "\n",
    "Thus we identify the vector $\\phi(d)$ which maximizes the normalized dot-product between the two word\n",
    "vectors (i.e. cosine similarity).\n",
    "\n",
    "\n",
    "\n",
    "* You can either use your own method to compute the correct word or use the code below. \n",
    "\n",
    "* Use original analogies dataset https://github.com/svn2github/word2vec/blob/master/questions-words.txt \n",
    "\n",
    "Q. When does it fail? \n",
    "\n",
    "Q. What are the possible reasons for failure?\n",
    "\n",
    "##### 15mins\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xWVu38ePjh-i"
   },
   "outputs": [],
   "source": [
    "def cosine_distance(u, v):\n",
    "    distance = 0.0\n",
    "    dot = np.dot(u,v)\n",
    "    norm_u = np.sqrt(np.sum(u**2))\n",
    "    norm_v = np.sqrt(np.sum(v**2))\n",
    "    distance = dot/(norm_u)/norm_v\n",
    "    return distance\n",
    "  \n",
    " \n",
    "def find_analogy(word_a, word_b, word_c, word_vectors, word2index):\n",
    "    word_a = word_a.lower()\n",
    "    word_b = word_b.lower()\n",
    "    word_c = word_c.lower()\n",
    "    \n",
    "    (e_a, e_b, e_c) = (word_vectors[word2index[word_a]], \n",
    "                       word_vectors[word2index[word_b]], \n",
    "                       word_vectors[word2index[word_c]])\n",
    "    \n",
    "    \n",
    "    max_cosine_sim = -999\n",
    "    best_word = None\n",
    "    \n",
    "    for (w, i) in word2index.items():\n",
    "        if w in [word_a, word_b, word_c]:\n",
    "            continue\n",
    "        cosine_sim = cosine_distance(e_b - e_a, word_vectors[i] - e_c)\n",
    "        \n",
    "        if cosine_sim > max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            best_word = w\n",
    "            \n",
    "    return best_word\n",
    "  \n",
    "# find_analogy('france', 'paris', 'england', wvecs, w2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mqchbyIiCOcQ"
   },
   "source": [
    "# Advanced: Compositionality \n",
    "\n",
    "* Given access to only word representations, how can we build representations for phrases and sentences? \n",
    "\n",
    "  (Hint: algebraic operation is one way) \n",
    "\n",
    "\n",
    "* Compute the similarity score between two sentences on the STS.input.MSRpar.txt dataset from https://github.com/alvations/stasis/tree/master/STS-data/STS2012-train \n",
    "\n",
    "  (Please use 00-readme.txt in the corpus for details on the format)\n",
    "  \n",
    "* Measure the pearson correlation with the human scores in STS.gs.MSRpar.txt\n",
    "\n",
    "Q. What problems did you encounter when computing the scores? \n",
    "\n",
    "Q. What are alternative ways of computing the scores? \n",
    "\n",
    "Q. Using your composition method, compute representations for the following expressions and also list the top-5 most similar words: \n",
    "\n",
    "* New York \n",
    "* kick the bucket\n",
    "* post office\n",
    "\n",
    "  Does it work? What are the possible reasons? \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qUwxb7CPt_n3"
   },
   "source": [
    "# References\n",
    "\n",
    "\n",
    "* [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf): word2vec reference\n",
    "\n",
    "* [Eluciating the properties of semantic word representations](http://www.offconvex.org/2016/02/14/word-embeddings-1/): A global perspective\n",
    "\n",
    "* [Understanding the algebraic notions of semantic word representations](http://www.offconvex.org/2015/12/12/word-embeddings-1/): Why does the word-analogies task work with simple algebraic manipulations?\n",
    "\n",
    "* [Stemming And Lemmatization](https://www.datacamp.com/community/tutorials/stemming-lemmatization-python)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "01_preprocessing_and_embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
