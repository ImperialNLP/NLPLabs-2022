{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessing with torchtext.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DYq9O6F0pz4"
      },
      "source": [
        "# Preprocessing with torchext\r\n",
        "\r\n",
        "This notebook gives a quick explanation on how to use torchtext.data.Field to prepare your data.\r\n",
        "\r\n",
        "Resources to explore it further are linked at the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcblipisCj6F"
      },
      "source": [
        "## 1. Data.Field\r\n",
        "\r\n",
        "In Pytorch, torchtext.data is a module that helps you with preprocessing your data and load popular datasets.\r\n",
        "\r\n",
        "As you can find in the documentation (https://torchtext.readthedocs.io/en/latest/data.html)\r\n",
        "\r\n",
        "The data module provides the following:\r\n",
        "\r\n",
        "- Ability to define a preprocessing pipeline\r\n",
        "- Batching, padding, and numericalizing (including building a vocabulary object)\r\n",
        "- Wrapper for dataset splits (train, validation, test)\r\n",
        "- Loader a custom NLP dataset\r\n",
        "\r\n",
        "\r\n",
        "In the [second lab](https://colab.research.google.com/github/ImperialNLP/NLPLabs/blob/master/lab02/lab02.ipynb), we use the subclass [data.Field](https://torchtext.readthedocs.io/en/latest/data.html#fields) to make the preprocessing of our dataset faster, before feeding it to your model. \r\n",
        "\r\n",
        "\r\n",
        "When calling ``data.Field()`` you have many parameters that you can set to define how to process your dataset before turning it into tensors. In the example below we use :\r\n",
        "- **sequential**: If set to True, allows tokenization.\r\n",
        "- **lower**: If True, apply lowercase to all text.\r\n",
        "- **tokenizer**: Can be assigned a tokenizer function. Can be set to ``\"spacy\"``. By default is ``string.split``.   \r\n",
        "\r\n",
        "Other useful parameters are: \r\n",
        "- **eos_token**: Adds end of sentence token\r\n",
        "- **stop_words**: Takes as value list of stop words to remove from our tokens.\r\n",
        "- **preprocessing** : Takes as value a preprocessing pipeline that is called after tokenizing\r\n",
        "- **fix_length** : pads all samples to given length\r\n",
        "- **use_vocab** : If False, keeps samples as numerical data instead of creating a word2idx ``Vocab`` object.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85GC6UJFIP7R"
      },
      "source": [
        "## 2. Sentiment analysis lab code\r\n",
        "The following code follows the use of torchtext for processing from the second lab and adds some more explanation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL5qNMBTSMc-"
      },
      "source": [
        "### 2.1 Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18koMOeahzjq"
      },
      "source": [
        "import torch\r\n",
        "from torchtext import data, datasets\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "import spacy\r\n",
        "import random\r\n",
        "\r\n",
        "SEED = 42\r\n",
        "\r\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpI5FQQ3SP8J"
      },
      "source": [
        "### 2.2 Load and process the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoLOOqKikwOY"
      },
      "source": [
        "In the code below we download the IMDb dataset and splits it into the canonical train/test splits as torchtext.datasets objects. We process the data using the ``Field`` objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhTHybCT28vR",
        "outputId": "bb1c7b54-727b-428e-939a-39757a26705d"
      },
      "source": [
        "\r\n",
        "spacy_en = spacy.load('en')\r\n",
        "\r\n",
        "def tokenizer(text): # create a custom tokenizer function\r\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\r\n",
        "\r\n",
        "text_field = data.Field(sequential=True, tokenize=tokenizer, lower=True)\r\n",
        "label_field = data.Field(sequential=False, use_vocab=False)\r\n",
        "\r\n",
        "# get pre-defined split and apply Field transformations\r\n",
        "train, test_init = datasets.IMDB.splits(text_field, label_field)\r\n",
        "\r\n",
        "# define our own validation and test set (initial test set is too large)\r\n",
        "train, valid_test = train.split(split_ratio=0.9, random_state=random.seed(SEED))\r\n",
        "valid, test = valid_test.split(split_ratio=0.5, random_state=random.seed(SEED))\r\n",
        "\r\n",
        "print(f'Train size: {len(train)}')\r\n",
        "print(f'Validation size: {len(valid)}')\r\n",
        "print(f'Test size: {len(test)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:09<00:00, 8.56MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train size: 22500\n",
            "Validation size: 1250\n",
            "Test size: 1250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0qHZRJFILJi"
      },
      "source": [
        "Our ``data.Field`` object has a vocab attribute that we can build by calling the ``build_vocab()`` function with our dataset as input. This will create a lookup table for our vocabulary and their embedding ( aka numerical representation). Here we supply the parameter \"vectors\" to assign glove embeddings to id's corresponding to words in our vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH0i3CuZIJQe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b3bd943-482b-4345-d230-9e0f31f329a4"
      },
      "source": [
        "# build vocabulary with maximum size (less frequent words are not considered)\r\n",
        "# load the pre-trained word embeddings.\r\n",
        "EMBEDDING_DIM = 50\r\n",
        "\r\n",
        "text_field.build_vocab(train, max_size=25000, vectors=f\"glove.6B.{EMBEDDING_DIM}d\")\r\n",
        "label_field.build_vocab(train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:52, 2.09MB/s]                           \n",
            "100%|█████████▉| 399494/400000 [00:12<00:00, 31214.80it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4lwuh4llF88"
      },
      "source": [
        "We can check our vocabulary by printing the most common words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mDr58W7lFI4",
        "outputId": "2ff6339f-8b3f-4292-b443-607f6185a3d3"
      },
      "source": [
        "print(text_field.vocab.freqs.most_common(20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('the', 295618), (',', 247478), ('.', 212955), ('and', 146607), ('a', 145345), ('of', 130731), ('to', 121602), ('is', 99075), ('it', 84058), ('in', 83452), ('i', 74098), ('this', 65970), ('that', 65542), ('\"', 57220), (\"'s\", 55615), ('-', 47369), ('/><br', 45678), ('was', 45034), ('as', 41501), ('for', 39538)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sgHRNKGlXB_"
      },
      "source": [
        "And check our labels.\r\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STYyEJnklaMc",
        "outputId": "9550d30a-7f6a-40f4-e1cd-aa879bac236b"
      },
      "source": [
        "print(label_field.vocab.stoi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<function _default_unk_index at 0x7f9937224488>, {'<unk>': 0, 'neg': 1, 'pos': 2})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L_A-5JaNfdr"
      },
      "source": [
        "We can also access the vocabulary size and our embeddings, useful when training our models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zL_IH7r0R6aq"
      },
      "source": [
        "voc_size = len(text_field.vocab) \r\n",
        "pretrained_embeddings = TEXT.vocab.vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvZHuu9WSAQk"
      },
      "source": [
        "Here is an example on how it can be used to initialise a model with our Glove embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8xV5sdzNfKC"
      },
      "source": [
        "\r\n",
        "# Build an FFNN model with an Embedding layer.\r\n",
        "class FFNN(nn.Module):\r\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, num_classes):\r\n",
        "        super().__init__()\r\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\r\n",
        "        # hidden layer\r\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)   \r\n",
        "        # activation\r\n",
        "        self.relu1 = nn.ReLU()       \r\n",
        "        # output layer\r\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)  \r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        # x has shape (batch_size, max_sent_len)\r\n",
        "        embedded = self.embedding(x)        \r\n",
        "        sent_lens = x.ne(0).sum(1, keepdims=True)\r\n",
        "        averaged = embedded.sum(1) / sent_lens\r\n",
        "        out = self.fc1(averaged)\r\n",
        "        out = self.relu1(out)\r\n",
        "        out = self.fc2(out)\r\n",
        "        return out\r\n",
        "\r\n",
        "# Get vocabulary size for the input dimension of the first layer\r\n",
        "INPUT_DIM = len(text_field.vocab) \r\n",
        "\r\n",
        "EPOCHS = 10\r\n",
        "LRATE = 0.5\r\n",
        "\r\n",
        "# we define our embedding dimension (dimensionality of the output of the first layer)\r\n",
        "EMBEDDING_DIM = 50\r\n",
        "# dimensionality of the output of the second hidden layer\r\n",
        "HIDDEN_DIM = 50\r\n",
        "# the output dimension is the number of classes, 1 for binary classification\r\n",
        "OUTPUT_DIM = 1\r\n",
        "\r\n",
        "# Construct the model\r\n",
        "model = FFNN(EMBEDDING_DIM, HIDDEN_DIM, INPUT_DIM, OUTPUT_DIM)\r\n",
        "\r\n",
        "# Initialize the embedding layer with the Glove embeddings from the\r\n",
        "# vocabulary\r\n",
        "pretrained_embeddings = TEXT.vocab.vectors\r\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnVlM9o_SZAn"
      },
      "source": [
        "### 2.3 Batch iterator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdwB790VhGk2"
      },
      "source": [
        "Finally, we build our iterator object. The iterator splits our sets into batches for training and for validation and testing if necessary (not enough memory to hold all samples at once). We then iterate over those batches during our training/validation loop.\r\n",
        "\r\n",
        "In the field of computer vision, we often use `DataLoader` to iterate over batches, but for text we'll use a `BucketIterator`. It is a special type of iterator that will return a batch of examples where each example is of a similar length, minimizing the amount of padding per example. Torchtext will pad for us automatically (handled by the `Field` object).\r\n",
        "\r\n",
        "\r\n",
        "We also want to place the tensors returned by the iterator on the GPU (if you're using one). PyTorch handles this using `torch.device`, we then pass this device to the iterator.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udx8rc4-hF9d"
      },
      "source": [
        "# get iterators over the data\r\n",
        "# place iterators on the GPU if possible\r\n",
        "\r\n",
        "# define our batch size\r\n",
        "BATCH_SIZE = 64\r\n",
        "\r\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\r\n",
        "  (train, valid, test),\r\n",
        "  batch_sizes=(BATCH_SIZE, BATCH_SIZE, BATCH_SIZE), device=DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybmJKhC9-gFi"
      },
      "source": [
        "Batch object is not iterable like pytorch Dataloader. A single Batch object contains the data of one batch .The text and labels can be accessed via column names.\r\n",
        "\r\n",
        "Here we will check the first batch of th iterator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNmNEKIWEd0W",
        "outputId": "60071e35-db6c-4d65-a024-7743bbaf2db7"
      },
      "source": [
        "print(next(iter(train_iter)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[torchtext.data.batch.Batch of size 64]\n",
            "\t[.Text]:[torch.LongTensor of size 202x64]\n",
            "\t[.Label]:[torch.LongTensor of size 64]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnzhOleUH8tr"
      },
      "source": [
        "We can also iterate over all batches.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqX8IjHvo29P"
      },
      "source": [
        "# will output all elements\r\n",
        "for batch in train_iter:\r\n",
        "    print(batch.Text)\r\n",
        "    print(batch.Label)\r\n",
        "    # Training/evaluation code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0KQAQlRLVc5"
      },
      "source": [
        "## 3. Some more examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONpp_sIIjeSD"
      },
      "source": [
        "### 3.1 Using spacy tokenizer and stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoghKeDeKd0h"
      },
      "source": [
        "spacy_nlp = spacy.load('en_core_web_sm')\r\n",
        "spacy_stop_words = spacy.lang.en.stop_words.STOP_WORDS\r\n",
        "print(spacy_stop_words)\r\n",
        "\r\n",
        "text_field = data.Field(tokenize='spacy', lower=True, stop_words=spacy_stop_words)\r\n",
        "label_field = data.Field(sequential=False, use_vocab=False) # we set sequential to false as we don't tokenise our labels\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gs8EMXaug03"
      },
      "source": [
        "### 3.2 LabelField\r\n",
        "We can use the normal Field() object for our labels or we can also use the specialised object LabelField(). Here we are forcing our labels to be of float type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guZKt4ygvQfJ"
      },
      "source": [
        "label_field = data.LabelField(dtype=torch.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA0v7aJSj0JN"
      },
      "source": [
        "\r\n",
        "### 3.3 Using our own tokenizer and dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG_1vh0FDpKk"
      },
      "source": [
        "In this example, we will import our own dataset and process it with torchtext."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sganV7GJHrk",
        "outputId": "bff68b44-cfee-42b6-db0d-010a628d3f12"
      },
      "source": [
        "# We create a new folder where we will put our downloaded dataset - in this case a text file\r\n",
        "!mkdir dataset\r\n",
        "!wget -O dataset/corpus.txt https://gist.githubusercontent.com/kunalj101/ad1d9c58d338e20d09ff26bcc06c4235/raw/1d2261e2276cbb0257a2ed6e2f1f4320464c7c07/corpus"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-16 12:57:49--  https://gist.githubusercontent.com/kunalj101/ad1d9c58d338e20d09ff26bcc06c4235/raw/1d2261e2276cbb0257a2ed6e2f1f4320464c7c07/corpus\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4507148 (4.3M) [text/plain]\n",
            "Saving to: ‘dataset/corpus.txt’\n",
            "\n",
            "dataset/corpus.txt  100%[===================>]   4.30M  22.8MB/s    in 0.2s    \n",
            "\n",
            "2021-02-16 12:57:51 (22.8 MB/s) - ‘dataset/corpus.txt’ saved [4507148/4507148]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cwrka0VHI2IS"
      },
      "source": [
        "\r\n",
        "To use batch iterators over our dataset such as ``BucketIterator``, we need to load our data in a ``Dataset`` class. With torchext we commonly use ``TabularDataset``, which a wrapper around classical ``Dataset``. It is specifically designed to load csv, tsv or json files and process them using the Field objects.  \r\n",
        "\r\n",
        "Our dataset is a .txt file so we will load its content and put our data and labels into a dataframe. We then divide it into train, validation and test sets and save the results into csv files.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "903Gl5bNEpdU"
      },
      "source": [
        "import os\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "\r\n",
        "def load_data(filename):\r\n",
        "  data = open(filename).read()\r\n",
        "  labels, texts = [], []\r\n",
        "\r\n",
        "  for line in data.split(\"\\n\"):\r\n",
        "      content = line.split(' ', 1)\r\n",
        "      labels.append(content[0])\r\n",
        "      texts.append(content[1])\r\n",
        "  \r\n",
        "  return texts, labels\r\n",
        "  \r\n",
        "dataset_dir = './dataset'\r\n",
        "data_file = os.path.join(dataset_dir,'corpus.txt')\r\n",
        "\r\n",
        "text_data, labels = load_data(data_file)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "ldwd3CeY64mg",
        "outputId": "92e46288-1781-4cdf-a1f8-55a6dcd3fb82"
      },
      "source": [
        "# building our dataframe\r\n",
        "\r\n",
        "raw_data = {'Text' : text_data, 'Label': labels}\r\n",
        "df = pd.DataFrame(raw_data, columns=[\"Text\", \"Label\"])\r\n",
        "\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
              "      <td>__label__2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
              "      <td>__label__2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
              "      <td>__label__2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
              "      <td>__label__2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
              "      <td>__label__2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text       Label\n",
              "0  Stuning even for the non-gamer: This sound tra...  __label__2\n",
              "1  The best soundtrack ever to anything.: I'm rea...  __label__2\n",
              "2  Amazing!: This soundtrack is my favorite music...  __label__2\n",
              "3  Excellent Soundtrack: I truly like this soundt...  __label__2\n",
              "4  Remember, Pull Your Jaw Off The Floor After He...  __label__2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ANICerIrsWg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49344987-c8d9-46d5-f180-e24c7fb10b20"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "#splitting into  train,val,test sets\r\n",
        "\r\n",
        "train,test = train_test_split(df, test_size = 0.33, random_state = random.seed(SEED))\r\n",
        "train, val = train_test_split(train, test_size = 0.10, random_state = random.seed(SEED))\r\n",
        "\r\n",
        "print(f'Train size: {len(train)}')\r\n",
        "print(f'Validation size: {len(val)}')\r\n",
        "print(f'Test size: {len(test)}')\r\n",
        "\r\n",
        "\r\n",
        "# save it to csv files \r\n",
        "train.to_csv(\"train.csv\", index=False)\r\n",
        "test.to_csv(\"test.csv\", index=False)\r\n",
        "val.to_csv(\"val.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: 6030\n",
            "Validation size: 670\n",
            "Test size: 3300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfUeakhNKyXv"
      },
      "source": [
        "from torchtext.data import Field, BucketIterator, TabularDataset\r\n",
        "\r\n",
        "# create a custom tokenizer function\r\n",
        "def tokenizer(text): \r\n",
        "  doc = nlp(text)\r\n",
        "  # Remove stop words, punctuation symbols and non alphabetic characters\r\n",
        "  tokens = [token.text.lower() for token in doc if not token.is_stop \r\n",
        "            and not token.is_punct\r\n",
        "            and token.is_alpha] #keep only alphabetic characters\r\n",
        "  return tokens\r\n",
        "\r\n",
        "TEXT = data.Field(tokenize='spacy')\r\n",
        "LABEL = data.LabelField(sequential=False)\r\n",
        "\r\n",
        "# order should match the columns order in our csv/tsv file\r\n",
        "# if no processing was required, we set None\r\n",
        "data_fields = [('Text', TEXT), ('Label', LABEL)]\r\n",
        "\r\n",
        "# We will load our csv files into Dataset objects \r\n",
        "train, val, test = data.TabularDataset.splits(\r\n",
        "                                        path = './',\r\n",
        "                                        train = 'train.csv',\r\n",
        "                                        validation = 'val.csv',\r\n",
        "                                        test = 'test.csv',\r\n",
        "                                        format = 'csv',\r\n",
        "                                        fields = data_fields,\r\n",
        "                                        skip_header = True)\r\n",
        "\r\n",
        "# possible dimensions for glove embeddings\r\n",
        "EMBEDDING_DIM = [25, 50, 100, 200, 300]\r\n",
        "\r\n",
        "TEXT.build_vocab(train,max_size=25000, vectors=f\"glove.6B.{EMBEDDING_DIM[1]}d\")\r\n",
        "LABEL.build_vocab(train) \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUUAWNbheJP-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b434d4b-e91f-4ce9-8b43-7be5c3862df2"
      },
      "source": [
        "print(train[0].Text)\r\n",
        "print(train[0].Label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'was', 'extremely', 'disappointed', '!', ':', 'The', 'book', 'just', 'did', 'not', 'hold', 'my', 'interest', 'very', 'well', '.', 'It', 'was', 'long', 'and', 'tedious', ',', 'and', 'I', 'could', \"n't\", 'wait', 'for', 'it', 'to', 'be', 'over', '.', 'The', 'characters', 'were', 'not', 'believable', 'nor', 'were', 'they', 'erotic', '.', 'They', 'came', 'across', 'quite', 'clown', '-', 'ish', '.', 'Anne', 'Rice', 'is', 'an', 'excellent', 'writer', ',', 'but', 'from', 'this', 'novel', 'you', 'would', 'never', 'believe', 'it', '.']\n",
            "__label__1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCdmgovvClHY",
        "outputId": "d3c26878-96c8-4a85-db39-6357eb22dbca"
      },
      "source": [
        "print(LABEL.vocab.stoi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<function _default_unk_index at 0x7f9937224488>, {'__label__1': 0, '__label__2': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMu71YunC5nW"
      },
      "source": [
        "train_iter, val_iter,_test_iter = data.BucketIterator.splits(\r\n",
        "                                    (train, val, test), batch_sizes= (BATCH_SIZE, BATCH_SIZE, BATCH_SIZE),\r\n",
        "                                    sort_key=lambda x: len(x.Text), device=DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z68bO3Ur_Wnr"
      },
      "source": [
        "### 3.4 Using a Pipeline\r\n",
        "\r\n",
        "We can define a pipeline that will be applied after we tokenised our documents. This can be useful if we want to clearly separate tokenisation from cleaning our tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riHj8hGSLjlp"
      },
      "source": [
        "# defining our pipelines\r\n",
        "\r\n",
        "def clean_string(tokens):\r\n",
        "  tokens = [t.replace(\">\",\"\") for t in tokens]\r\n",
        "  return tokens\r\n",
        "\r\n",
        "def convert_to_int(l):\r\n",
        "  return [int(y) for y in l]\r\n",
        "\r\n",
        "preprocess_pipeline = data.Pipeline(clean_string)\r\n",
        "preprocess_pipeline_label = data.Pipeline(convert_to_int)\r\n",
        "\r\n",
        "text_field = data.Field(tokenize='spacy', lower=True, preprocessing=preprocess_pipeline)\r\n",
        "label_field = data.Field(sequential=False, use_vocab=False, postprocessing = preprocess_pipeline_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MnQDWg89aRw"
      },
      "source": [
        "## 4. More Tutorials\r\n",
        "\r\n",
        "* A tutorial for using torchtext for preprocessing can be found here: [Part 1](https://towardsdatascience.com/use-torchtext-to-load-nlp-datasets-part-i-5da6f1c89d84) and [Part 2](https://towardsdatascience.com/use-torchtext-to-load-nlp-datasets-part-ii-f146c8b9a496). You can find a deeper tutorial [here](http://anie.me/On-Torchtext/).\r\n",
        "\r\n",
        "* Torchtext for machine translation [here](https://towardsdatascience.com/how-to-use-torchtext-for-neural-machine-translation-plus-hack-to-make-it-5x-faster-77f3884d95)\r\n",
        "\r\n",
        "* Pytorch example for using torchtext for BERT [here](https://github.com/pytorch/text/tree/master/examples/BERT)\r\n",
        "\r\n",
        "* Other examples of using torchtext for Transformers: \r\n",
        "  * [Language model](https://ryanong.co.uk/2020/06/28/day-180-learning-pytorch-language-model-with-nn-transformer-and-torchtext-part-1/)\r\n",
        "  * [Ben Trevett - Sentiment analysis](https://colab.research.google.com/github/bentrevett/pytorch-sentiment-analysis/blob/master/6%20-%20Transformers%20for%20Sentiment%20Analysis.ipynb)"
      ]
    }
  ]
}