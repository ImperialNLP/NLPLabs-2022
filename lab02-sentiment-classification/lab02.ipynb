{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab02_SentimentClassification_V2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qS8y5_Ewv6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Uncomment below if you want to install packages into your local environment\n",
        "# Colab already provides the required packages for this lab\n",
        "\n",
        "#! pip install torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEJM0h_uctzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# we fix the seeds to get consistent results before every training\n",
        "# loop in what follows\n",
        "def fix_seed(seed=234):\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE8B9-L8U0aZ",
        "colab_type": "text"
      },
      "source": [
        "# Text classification: Sentiment analysis\n",
        "\n",
        "In this notebook we are going to build state-of-the art models for text classification using the example of sentiment analysis. To be more precise, we will build a feed-forward neural network (FFNN) and a convolutional neural network (CNN). We will look into the details of data preparation, functioning of each model and how the performance of those NNs could be measured efficiently. We will start our work using a toy corpus. Further you can extend your knowledge and use a larger dataset.\n",
        "\n",
        "Again we are using [pytorch](https://www.pytorch.org), an open source deep learning platform, as our backbone library in the course.\n",
        "\n",
        "Here are the toy training and validation sets. It is good practise to use the validation set (a representative set of the test data). This set is used to tune hyperparameters and choose a configuration for your model to ensure the best performance. Our toy sets are already tokenized and lowercased."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abMSnMwJx8bu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # Our toy sentiment analysis corpus\n",
        "train = ['i like his paper !',\n",
        "         'what a well-written essay !',\n",
        "         'i do not agree with the criticism on this paper',\n",
        "         'well done ! it was an enjoyable reading',\n",
        "         'it was very good . send me a copy please .',\n",
        "         'the argumentation in the paper is very weak',\n",
        "         'poor effort !',\n",
        "         'the methodology could have been more detailed',\n",
        "         'i am not impressed',\n",
        "         'could have done better .',\n",
        "]\n",
        "\n",
        "train_labels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
        "\n",
        "# Validation set\n",
        "valid = ['i like your paper', \n",
        "         'i agree with your results', \n",
        "         'what a success ! a well-written paper', \n",
        "         'not enough details . very poor', \n",
        "         'i support the criticism',\n",
        "         'could be better',\n",
        "]\n",
        "\n",
        "valid_labels = [1, 1, 1, 0, 0, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hPlpqVFYpfL",
        "colab_type": "text"
      },
      "source": [
        "## Pre-processing\n",
        "\n",
        "Using the material from the previous lab session, fill in the below function to tokenize the corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ5IL1e1GVn6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tokenized_corpus(corpus):\n",
        "  tokenized_corpus = []\n",
        "\n",
        "  #######################\n",
        "  # Q: Process the corpus\n",
        "  #######################\n",
        " \n",
        "  return tokenized_corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyKCfY6QbO0r",
        "colab_type": "text"
      },
      "source": [
        "## Word2index dictionary\n",
        "\n",
        "Similar to the way it was done in the previous lab, we define here a method that returns a word to index dictionary. Note that we reserve the 0 index for the padding token `<pad>`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bWQ3zKYKlQU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_word2idx(tokenized_corpus):\n",
        "  vocabulary = []\n",
        "  for sentence in tokenized_corpus:\n",
        "    for token in sentence:\n",
        "        if token not in vocabulary:\n",
        "            vocabulary.append(token)\n",
        "  \n",
        "  word2idx = {w: idx+1 for (idx, w) in enumerate(vocabulary)}\n",
        "  # we reserve the 0 index for the padding token\n",
        "  word2idx['<pad>'] = 0\n",
        "  \n",
        " \n",
        "  return word2idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hve1MProNBvp",
        "colab_type": "text"
      },
      "source": [
        "## Preparation of inputs\n",
        "\n",
        "The first layer of our FFNN will be an embedding (look-up) layer which takes as input indexes of tokens (we do not need to one-hot encode our vectors).\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Why do we need to fix the length of our input vectors (we take the maximum sentence length here) ? This process is referred to as padding. Print the padded training corpus.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-xp_gKvObfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model_inputs(tokenized_corpus, word2idx, labels):\n",
        "  # we index our sentences\n",
        "  vectorized_sents = [[word2idx[tok] for tok in sent if tok in word2idx] for sent in tokenized_corpus]\n",
        "\n",
        "  # Sentence lengths\n",
        "  sent_lengths = [len(sent) for sent in vectorized_sents]\n",
        "\n",
        "  # Get maximum length\n",
        "  max_len = max(sent_lengths)\n",
        "  \n",
        "  # we create a tensor of a fixed size filled with zeroes for padding\n",
        "  sent_tensor = torch.zeros((len(vectorized_sents), max_len)).long()\n",
        "\n",
        "  # we fill it with our vectorized sentences \n",
        "  for idx, (sent, sentlen) in enumerate(zip(vectorized_sents, sent_lengths)):\n",
        "    sent_tensor[idx, :sentlen] = torch.LongTensor(sent)\n",
        "\n",
        "  # Label tensor\n",
        "  label_tensor = torch.FloatTensor(labels)\n",
        "  \n",
        "  return sent_tensor, label_tensor\n",
        "\n",
        "###\n",
        "\n",
        "tokenized_corpus = get_tokenized_corpus(train)\n",
        "word2idx = get_word2idx(tokenized_corpus)\n",
        "train_sent_tensor, train_label_tensor = get_model_inputs(tokenized_corpus, word2idx, train_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGg6CtALe8Va",
        "colab_type": "text"
      },
      "source": [
        "# Building the Feed-Forward Neural Network\n",
        "\n",
        "We will start by building a very simple feed-forward neural network (FFNN).\n",
        "Our FFNN class is a sub-class of `nn.Module`. Within the `__init__` method, we define the layers of the module:\n",
        "\n",
        "- Our first layer is an embedding layer (look-up layer). This layer could be initialized with pre-trained embeddings (as we will see at the end of this lab) or could be trained together with other layers.\n",
        " \n",
        "- The next layer is a fully connected layer followed by a ReLU activation.\n",
        "\n",
        "- Finally, the last linear layer is the output layer for the classification task.\n",
        "\n",
        "The `forward()` method is called when we feed data into our model. Please note that the output dimension of each layer is the input dimension for the next one.\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Recall from the previous lab the functioning of a lookup layer. How does the mapping to the dense representation happen?**\n",
        "\n",
        "**Q: Implement the averaging of embeddings in the `forward()` method of the class below.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LWut1gtXGQN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FFNN(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, num_classes):  \n",
        "        super(FFNN, self).__init__()\n",
        "        \n",
        "        # embedding (lookup layer) layer\n",
        "        # padding_idx argument makes sure that the 0-th token in the vocabulary\n",
        "        # is used for padding purposes i.e. its embedding will be a 0-vector\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        \n",
        "        # hidden layer\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
        "        \n",
        "        # activation\n",
        "        self.relu1 = nn.ReLU()\n",
        "        \n",
        "        # output layer\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x has shape (batch_size, max_sent_len)\n",
        "\n",
        "        embedded = self.embedding(x)\n",
        "        # `embedding` has shape (batch size, max_sent_len, embedding dim)\n",
        "\n",
        "        ########################################################################\n",
        "        # Q: Compute the average embeddings of shape (batch_size, embedding_dim)\n",
        "        ########################################################################\n",
        "        # Implement averaging that ignores padding (average using actual sentence lengths).\n",
        "        # How this effect the result?\n",
        "        \n",
        "        averaged = \"<TODO>\"\n",
        "\n",
        "        out = self.fc1(averaged)\n",
        "        out = self.relu1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcAI5BtaQMFh",
        "colab_type": "text"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "In this section we will define the hyperparameters of our model, the loss function, the optimizer and perform a number of training epochs over our toy training corpus.\n",
        "\n",
        "We will use the **Stochastic gradient descent (SGD)** optimizer. The learning rate hyperparameter of the optimizer controls how the weights are adjusted with respect to the loss gradient. The lower the value, the more fine-grained are weight updates.\n",
        "\n",
        "**Note that** it is a common practise to perform training using mini-batches (sets of training instances seen by the model during weight update step). In this case, the epoch loss is defined as the loss averaged across the mini-batches. Since our corpus is very small, we train on the whole training set without batching.\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Why is the number of output classes is equal to 1 for binary classification?**\n",
        "\n",
        "\n",
        "**Q: Try to modify the learning rate (which is initially set to 0.5 below) in the range $[0.0001, 0.5]$. How does the loss react to these changes?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrtOwTUsyArb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reset the seed before every model construction for reproducible results\n",
        "fix_seed()\n",
        "\n",
        "# we will train for N epochs (The model will see the corpus N times)\n",
        "EPOCHS = 10\n",
        "\n",
        "# Learning rate is initially set to 0.5\n",
        "LRATE = 0.5\n",
        "\n",
        "# we define our embedding dimension (dimensionality of the output of the first layer)\n",
        "EMBEDDING_DIM = 50\n",
        "\n",
        "# dimensionality of the output of the second hidden layer\n",
        "HIDDEN_DIM = 50\n",
        "\n",
        "# the output dimension is the number of classes, 1 for binary classification\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "# Construct the model\n",
        "model = FFNN(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx), OUTPUT_DIM)\n",
        "\n",
        "# Print the model\n",
        "print(model)\n",
        "\n",
        "# we use the stochastic gradient descent (SGD) optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=LRATE)\n",
        "\n",
        "# we use the binary cross-entropy loss with sigmoid (applied to logits) \n",
        "# Recall that we did not apply any activation to our output layer, hence we need\n",
        "# to make our outputs look like probabilities.\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Input and label tensors\n",
        "feature = train_sent_tensor\n",
        "target = train_label_tensor\n",
        "\n",
        "################\n",
        "# Start training\n",
        "################\n",
        "print(f'Will train for {EPOCHS} epochs')\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  # to ensure the dropout (explained later) is \"turned on\" while training\n",
        "  # good practice to include even if do not use here\n",
        "  model.train()\n",
        "  \n",
        "  # we zero the gradients as they are not removed automatically\n",
        "  optimizer.zero_grad()\n",
        "  \n",
        "  # squeeze is needed as the predictions will have the shape (batch size, 1)\n",
        "  # and we need to remove the dimension of size 1\n",
        "  predictions = model(feature).squeeze(1)\n",
        "\n",
        "  # Compute the loss\n",
        "  loss = loss_fn(predictions, target)\n",
        "  train_loss = loss.item()\n",
        "\n",
        "  # calculate the gradient of each parameter\n",
        "  loss.backward()\n",
        "\n",
        "  # update the parameters using the gradients and optimizer algorithm \n",
        "  optimizer.step()\n",
        "  \n",
        "  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwY-k6x0xIkl",
        "colab_type": "text"
      },
      "source": [
        "## Measuring the accuracy\n",
        "\n",
        "In addition to measuring the loss, we can also evaluate the actual classification performance of our model. (In the case of training with mini-batches, the epoch accuracy is defined as the accuracy averaged across the mini-batches.)\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Fill in the below function so that it computes the accuracy of the model. Once you are done, improve the loop so that it also prints the training accuracy after each epoch.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R23R0XjuPTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(output, target):\n",
        "  #####################################\n",
        "  # Q: Return the accuracy of the model\n",
        "  #####################################\n",
        "  acc = \"<TODO>\"\n",
        "  return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eP2s5MyyrNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reset the seed for consistent results\n",
        "fix_seed()\n",
        "\n",
        "# we will train for N epochs (The model will see the corpus N times)\n",
        "EPOCHS = 10\n",
        "\n",
        "# Learning rate is initially set to 0.5\n",
        "LRATE = 0.5\n",
        "\n",
        "# we define our embedding dimension (dimensionality of the output of the first layer)\n",
        "EMBEDDING_DIM = 50\n",
        "\n",
        "# dimensionality of the output of the second hidden layer\n",
        "HIDDEN_DIM = 50\n",
        "\n",
        "# the output dimension is the number of classes, 1 for binary classification\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "# Construct the model\n",
        "model = FFNN(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx), OUTPUT_DIM)\n",
        "\n",
        "# Print the model\n",
        "print(model)\n",
        "\n",
        "# we use the stochastic gradient descent (SGD) optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=LRATE)\n",
        "\n",
        "# we use the binary cross-entropy loss with sigmoid (applied to logits) \n",
        "# Recall that we did not apply any activation to our output layer, hence we need\n",
        "# to make our outputs look like probabilities.\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Input and label tensors\n",
        "feature = train_sent_tensor\n",
        "target = train_label_tensor\n",
        "\n",
        "################\n",
        "# Start training\n",
        "################\n",
        "print(f'Will train for {EPOCHS} epochs')\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  # to ensure the dropout (explained later) is \"turned on\" while training\n",
        "  # good practice to include even if do not use here\n",
        "  model.train()\n",
        "  \n",
        "  # we zero the gradients as they are not removed automatically\n",
        "  optimizer.zero_grad()\n",
        "  \n",
        "  # squeeze is needed as the predictions will have the shape (batch size, 1)\n",
        "  # and we need to remove the dimension of size 1\n",
        "  predictions = model(feature).squeeze(1)\n",
        "\n",
        "  # Compute the loss\n",
        "  loss = loss_fn(predictions, target)\n",
        "  train_loss = loss.item()\n",
        "  \n",
        "  #####################\n",
        "  # Q: Compute accuracy\n",
        "  #####################\n",
        "  train_acc = \"<TODO>\"\n",
        "\n",
        "  # calculate the gradient of each parameter\n",
        "  loss.backward()\n",
        "\n",
        "  # update the parameters using the gradients and optimizer algorithm \n",
        "  optimizer.step()\n",
        "\n",
        "  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2UhMGlMq3Re",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameter tuning on the validation set\n",
        "\n",
        "You should now apply the previous pre-processing and input preparation procedures to the validation set as well.\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Should we re-use the word to index dictionary we created before? Why?**\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9_FERooxUQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###############################################\n",
        "# Q: Prepare the validation corpus and labels #\n",
        "###############################################\n",
        "print(valid_sent_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUlXfydh1Uxv",
        "colab_type": "text"
      },
      "source": [
        "**Q: Try to modify the learning rate and the number of epochs now. How will the validation loss and accuracy react to those changes?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rcKPET9BHb-7",
        "colab": {}
      },
      "source": [
        "# Reset the seed for consistent results\n",
        "fix_seed()\n",
        "\n",
        "# we will train for N epochs (The model will see the corpus N times)\n",
        "EPOCHS = 10\n",
        "\n",
        "# Learning rate is initially set to 0.5\n",
        "LRATE = 0.5\n",
        "\n",
        "# we define our embedding dimension (dimensionality of the output of the first layer)\n",
        "EMBEDDING_DIM = 50\n",
        "\n",
        "# dimensionality of the output of the second hidden layer\n",
        "HIDDEN_DIM = 50\n",
        "\n",
        "# the output dimension is the number of classes, 1 for binary classification\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "# Construct the model\n",
        "model = FFNN(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx), OUTPUT_DIM)\n",
        "\n",
        "# we use the stochastic gradient descent (SGD) optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=LRATE)\n",
        "\n",
        "# we use the binary cross-entropy loss with sigmoid (applied to logits) \n",
        "# Recall that we did not apply any activation to our output layer, hence we need\n",
        "# to make our outputs look like probabilities.\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Input and label tensors for training\n",
        "feature_train = train_sent_tensor\n",
        "target_train = train_label_tensor\n",
        "\n",
        "# Input and label tensors for validation\n",
        "feature_valid = valid_sent_tensor\n",
        "target_valid = valid_label_tensor\n",
        "\n",
        "################\n",
        "# Start training\n",
        "################\n",
        "print(f'Will train for {EPOCHS} epochs')\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  # to ensure the dropout (explained later) is \"turned on\" while training\n",
        "  # good practice to include even if do not use here\n",
        "  model.train()\n",
        "  \n",
        "  # we zero the gradients as they are not removed automatically\n",
        "  optimizer.zero_grad()\n",
        "  \n",
        "  # squeeze is needed as the predictions will have the shape (batch size, 1)\n",
        "  # and we need to remove the dimension of size 1\n",
        "  predictions = model(feature_train).squeeze(1)\n",
        "\n",
        "  # Compute the loss\n",
        "  loss = loss_fn(predictions, target_train)\n",
        "  train_loss = loss.item()\n",
        "\n",
        "  # Compute training accuracy\n",
        "  train_acc = accuracy(predictions, target_train)\n",
        "\n",
        "  # calculate the gradient of each parameter\n",
        "  loss.backward()\n",
        "\n",
        "  # update the parameters using the gradients and optimizer algorithm \n",
        "  optimizer.step()\n",
        "  \n",
        "  # this puts the model in \"evaluation mode\" (turns off dropout and batch normalization)\n",
        "  # good practise to include even if we do not use them right now\n",
        "  model.eval()\n",
        "\n",
        "  # we do not compute gradients within this block, i.e. no training\n",
        "  with torch.no_grad():\n",
        "    predictions_valid = model(feature_valid).squeeze(1)\n",
        "    valid_loss = loss_fn(predictions_valid, target_valid).item()\n",
        "    valid_acc = accuracy(predictions_valid, target_valid)\n",
        "  \n",
        "  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:6.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:6.2f}% |')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npzWXeQATPs4",
        "colab_type": "text"
      },
      "source": [
        "## Testing the model\n",
        "\n",
        "Now let us test our trained model. We define a small test set below. First, apply the data preparation procedures to this test set as you did for the validation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Cx2eJi1a8R6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = ['i really do not like your paper', \n",
        "        'well done', \n",
        "        'good results for a paper !',\n",
        "        'amazing effort', \n",
        "        'your effort is poor !', \n",
        "        'not impressed'   \n",
        "]\n",
        "\n",
        "test_labels = [0, 1, 1, 1, 0, 0]\n",
        "\n",
        "#########################################\n",
        "# Q: Prepare the test corpus and labels #\n",
        "\n",
        "print(test_sent_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0A1Iu-p4FUL",
        "colab_type": "text"
      },
      "source": [
        "**Q: Fill in the below function for the computation of F-measure. Once done, complete the missing lines in the final evaluation part.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixhvaYkW_SfX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f_measure(output, gold):\n",
        "  ############################################\n",
        "  # Q: Compute precision, recall and f-measure \n",
        "  ############################################\n",
        "  precision = \"<TODO>\"\n",
        "  recall = \"<TODO>\"\n",
        "  fscore = \"<TODO>\"\n",
        "\n",
        "  # Print them\n",
        "  print(f\"     Recall: {recall:.2f}, Precision: {precision:.2f}, F-measure: {fscore:.2f}\")\n",
        "  \n",
        "\n",
        "####\n",
        "\n",
        "model.eval()\n",
        "\n",
        "feature_test = test_sent_tensor\n",
        "target_test = test_label_tensor\n",
        "\n",
        "with torch.no_grad():\n",
        "  ####################################################################\n",
        "  # Q: Get predictions for the test set, compute the loss and accuracy\n",
        "  ####################################################################\n",
        "  predictions = \"<TODO>\"\n",
        "  test_loss = \"<TODO>\"\n",
        "  test_acc = \"<TODO>\"\n",
        "\n",
        "  # Print\n",
        "  print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
        "  f_measure(predictions, test_labels)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofMn0IjZ3lUr",
        "colab_type": "text"
      },
      "source": [
        "**Q:  Are the resulting evaluations different ? How do you interpret those differences? Print the predictions.**\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2znwTJeU3UT",
        "colab_type": "text"
      },
      "source": [
        "# Building the Convolutional Neural Network (CNN)\n",
        "\n",
        "We will implement a model inspired by the state-of-art CNN model as described in [Convolutional Neural Networks for Sentence Classification (Kim, 2014)](https://arxiv.org/abs/1408.5882).\n",
        "\n",
        "Similar to the FFNN model, we start with an embedding layer. We implement the convolutional layer with the help of `nn.Conv2d` and use the ReLU activation after it. The above-mentioned paper, being inspired by the convolution for images, applies a 2-dimensional convolution: a (window size, embedding dimension) filter. It covers `n` sequential words, taking embedding dimensions as the width. We then pass the tensors through a **max pooling layer**.\n",
        "\n",
        "The **max pooling layer** is typically followed by a **dropout** layer. The latter sets a random set of activations in the max-pooling layer to zero. This prevents the network from learning to rely on specific weights and helps to prevent overfitting. Note that the dropout layer is only used during training, and not during test time.\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Study the shapes of outputs coming from convolution and max pooling layers. What is the shape of the max pooling layer output?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUPETZaOgvgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, out_channels, window_size, output_dim, dropout):\n",
        "    super(CNN, self).__init__()\n",
        "    \n",
        "    # Create the embedding layer as usual\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "    \n",
        "    # in_channels -- 1 text channel\n",
        "    # out_channels -- the number of output channels\n",
        "    # kernel_size is (window size x embedding dim)\n",
        "    self.conv = nn.Conv2d(\n",
        "      in_channels=1, out_channels=out_channels,\n",
        "      kernel_size=(window_size, embedding_dim))\n",
        "    \n",
        "    # the dropout layer\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # the output layer\n",
        "    self.fc = nn.Linear(out_channels, output_dim)\n",
        "        \n",
        "  def forward(self, x):\n",
        "    # x -> (batch size, max_sent_length)\n",
        "    \n",
        "    embedded = self.embedding(x)\n",
        "    # embedded -> (batch size, max_sent_length, embedding_dim)\n",
        "    \n",
        "    # images have 3 RGB channels \n",
        "    # for the text we add 1 channel\n",
        "    embedded = embedded.unsqueeze(1)\n",
        "    # embedded -> (batch size, 1, max_sent_length, embedding dim)\n",
        "\n",
        "    # Compute the feature maps      \n",
        "    feature_maps = self.conv(embedded)\n",
        "\n",
        "    ##########################################\n",
        "    # Q: What is the shape of `feature_maps` ?\n",
        "    ##########################################\n",
        "    \n",
        "    feature_maps = feature_maps.squeeze(3)\n",
        "    \n",
        "    ##########################################\n",
        "    # Q: why do we remove 1 dimension here?\n",
        "    ##########################################\n",
        "    \n",
        "    # Apply ReLU\n",
        "    feature_maps = F.relu(feature_maps)\n",
        "    \n",
        "    # Apply the max pooling layer\n",
        "    pooled = F.max_pool1d(feature_maps, feature_maps.shape[2])\n",
        "    \n",
        "    pooled = pooled.squeeze(2)\n",
        "\n",
        "    ####################################\n",
        "    # Q: What is the shape of `pooled` ?\n",
        "    ####################################\n",
        "    \n",
        "    dropped = self.dropout(pooled)\n",
        "    preds = self.fc(dropped)\n",
        "    \n",
        "    return preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UZroMvLtFVv",
        "colab_type": "text"
      },
      "source": [
        "## Training and testing the CNN\n",
        "\n",
        "Here we will define the CNN-specific hyperparameters and perform the network training and testing. **Note that** the learning rate is initially set to 0.1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lq1lcWbqRwNY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fix_seed()\n",
        "\n",
        "EPOCHS = 10\n",
        "LRATE = 0.1\n",
        "\n",
        "EMBEDDING_DIM = 50\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "# the hyperparameters specific to CNN\n",
        "# we define the number of filters\n",
        "N_OUT_CHANNELS = 100\n",
        "\n",
        "# we define the window size\n",
        "WINDOW_SIZE = 1\n",
        "\n",
        "# we apply the dropout with the probability 0.2\n",
        "DROPOUT = 0.2\n",
        "\n",
        "# Construct the model\n",
        "model = CNN(len(word2idx), EMBEDDING_DIM, N_OUT_CHANNELS, WINDOW_SIZE, OUTPUT_DIM, DROPOUT)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=LRATE)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "feature_train = train_sent_tensor\n",
        "target_train = train_label_tensor\n",
        "\n",
        "feature_valid = valid_sent_tensor\n",
        "target_valid = valid_label_tensor\n",
        "\n",
        "feature_test = test_sent_tensor\n",
        "target_test = test_label_tensor\n",
        "\n",
        "################\n",
        "# Start training\n",
        "################\n",
        "print(f'Will train for {EPOCHS} epochs')\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  model.train()\n",
        "  \n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  predictions = model(feature_train).squeeze(1)\n",
        "  loss = loss_fn(predictions, target_train)\n",
        "  train_loss = loss.item()\n",
        "  train_acc = accuracy(predictions, target_train)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    predictions_valid = model(feature_valid).squeeze(1)\n",
        "    valid_loss = loss_fn(predictions_valid, target_valid).item()\n",
        "    valid_acc = accuracy(predictions_valid, target_valid)\n",
        "  \n",
        "  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:6.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:6.2f}% |')\n",
        "\n",
        "\n",
        "## Finally, test on the test set\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    predictions = model(feature_test).squeeze(1)\n",
        "    loss = loss_fn(predictions, target_test)\n",
        "    acc = accuracy(predictions, target_test)\n",
        "    print(f'Test Loss: {loss:.3f} | Test Acc: {acc*100:.2f}%')\n",
        "    f_measure(predictions, test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5si3n_wf-ImU",
        "colab_type": "text"
      },
      "source": [
        " **Q: Is the performance of CNN different from the performance of FFNN? Output predictions.**\n",
        " \n",
        "**Q: Is padding necessary for CNN inputs? What is the role of the window size?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4VCX4XSqvlu",
        "colab_type": "text"
      },
      "source": [
        "## Initializing CNN with pre-trained representations\n",
        "\n",
        "The work [Convolutional Neural Networks for Sentence Classification (Kim, 2014)](https://arxiv.org/abs/1408.5882) also investigates the exploitation of pre-trained embeddings and demonstrates the efficiency of using them.\n",
        "\n",
        "First, download the embeddings and unzip them below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgvTCi68lOGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kq8ou3Y0CSQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Unzip the file: 4 different embedding sizes are provided\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afclMKCYLYT9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check the file format\n",
        "!head -n10 glove.6B.50d.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMwxUcgDHzGu",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Try and initialize the CNN embedding layer with the `50D` pre-trained GloVe embeddings. Pay particular attention to keeping the correct indices from the `word2idx` for the lookup table! Once you fill the below `wvecs` matrix, copy the previous training loop and initialize its embedding layer with the pre-trained ones as follows:\n",
        "\n",
        "```python\n",
        "model.embedding.weight.data.copy_(torch.from_numpy(wvecs))\n",
        "```\n",
        "\n",
        "**Note:** The learning rate is initially set to 0.5.\n",
        "\n",
        "---\n",
        "\n",
        "**Q: What should the embedding for the padding token `<pad>` be?**\n",
        " \n",
        "**Q: What is the impact of using those pre-trained embeddings on the model performance?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCsV8mtBg4-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "EMBEDDING_DIM = 50\n",
        "\n",
        "# Yet another hyperparameter: since the pre-trained embeddings are coming\n",
        "# from a different network, their magnitudes could differ from the parameters\n",
        "# of this network. So scaling may be necessary.\n",
        "SCALE_EMBS = 0.65\n",
        "\n",
        "# Creates the empty numpy array that you should fill below\n",
        "wvecs = np.zeros((len(word2idx), EMBEDDING_DIM), dtype='float32')\n",
        "\n",
        "#####################################################################\n",
        "# Q: Read line by line, find the corresponding word and\n",
        "# insert its embedding to the correct position in the `wvecs` matrix.\n",
        "# Once done, apply the SCALE_EMBS factor to scale the vectors\n",
        "#####################################################################\n",
        "with open(f'glove.6B.{EMBEDDING_DIM}d.txt', 'r') as f:\n",
        "  \"<TODO>\"\n",
        "        \n",
        "print(wvecs)\n",
        "\n",
        "#####################\n",
        "# Re-create the model\n",
        "#####################\n",
        "fix_seed()\n",
        "\n",
        "EPOCHS = 10\n",
        "LRATE = 0.5\n",
        "\n",
        "# the hyperparameters specific to CNN\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "# we define the number of filters\n",
        "N_OUT_CHANNELS = 100\n",
        "\n",
        "# we define the window size\n",
        "WINDOW_SIZE = 1\n",
        "\n",
        "# we apply the dropout with the probability 0.1\n",
        "DROPOUT = 0.1\n",
        "\n",
        "# Construct the model\n",
        "model = CNN(len(word2idx), EMBEDDING_DIM, N_OUT_CHANNELS, WINDOW_SIZE, OUTPUT_DIM, DROPOUT)\n",
        "\n",
        "#################################################################\n",
        "### Q: Initialize the embeddings with the loaded pre-trained ones\n",
        "#################################################################\n",
        "\"<TODO>\"\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=LRATE)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "feature_train = train_sent_tensor\n",
        "target_train = train_label_tensor\n",
        "\n",
        "feature_valid = valid_sent_tensor\n",
        "target_valid = valid_label_tensor\n",
        "\n",
        "feature_test = test_sent_tensor\n",
        "target_test = test_label_tensor\n",
        "\n",
        "################\n",
        "# Start training\n",
        "################\n",
        "print(f'Will train for {EPOCHS} epochs')\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  model.train()\n",
        "  \n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  predictions = model(feature_train).squeeze(1)\n",
        "  loss = loss_fn(predictions, target_train)\n",
        "  train_loss = loss.item()\n",
        "  train_acc = accuracy(predictions, target_train)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    predictions_valid = model(feature_valid).squeeze(1)\n",
        "    valid_loss = loss_fn(predictions_valid, target_valid).item()\n",
        "    valid_acc = accuracy(predictions_valid, target_valid)\n",
        "  \n",
        "  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:6.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:6.2f}% |')\n",
        "\n",
        "\n",
        "## Finally, test on the test set\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    predictions = model(feature_test).squeeze(1)\n",
        "    loss = loss_fn(predictions, target_test)\n",
        "    acc = accuracy(predictions, target_test)\n",
        "    print(f'Test Loss: {loss:.3f} | Test Acc: {acc*100:.2f}%')\n",
        "    f_measure(predictions, test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKsR86iBOkgP",
        "colab_type": "text"
      },
      "source": [
        "# Advanced: Experimenting with larger corpora\n",
        "\n",
        "For advanced experiments with a larger dataset, we suggest to use the [IMBD dataset](http://ai.stanford.edu/~amaas/data/sentiment/) of movie reviews available from [`torchtext.datasets`](https://torchtext.readthedocs.io/en/latest/data.html). This module also provides a range of useful functionalities for data preparation: defining a preprocessing pipeline, splitting, batching, padding, iterating through data, loading pre-trained embeddings, building vocabulary, etc. Below we provide an example using the tokenizer as provided by the [spaCy](https://spacy.io) toolkit.\n",
        "\n",
        "With the batch size provided, `BucketIterator` defines mini-batches by grouping sequences with similar original lengths, so that there is minimal need for padding.  For this bigger dataset, use `.cuda()` on any input batches/tensors, network modules and loss functions to place computations on the GPU. When working on **Google Colab**, make sure that you changed your runtime to GPU from the above menu.\n",
        "\n",
        "You can start by applying the provided CNN model to this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71Dq7amIOnUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.legacy import data, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import spacy\n",
        "\n",
        "# Fix GPU seeds\n",
        "SEED = 9320\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  DEVICE='cuda:0'\n",
        "else:\n",
        "  DEVICE='cpu'\n",
        "\n",
        "print('Device is', DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY11cw5-PJ2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NOTE: Execution of this cell takes a couple of minutes\n",
        "##\n",
        "\n",
        "# define types of data and their preprocessing\n",
        "text_field = data.Field(tokenize='spacy', lower=True)\n",
        "label_field = data.LabelField(dtype=torch.float)\n",
        "\n",
        "# get pre-defined split\n",
        "train, test_init = datasets.IMDB.splits(text_field, label_field)\n",
        "\n",
        "# define our own validation and test set (initial test set is too large)\n",
        "train, valid_test = train.split(split_ratio=0.9, random_state=random.seed(SEED))\n",
        "valid, test = valid_test.split(split_ratio=0.5, random_state=random.seed(SEED))\n",
        "\n",
        "print(f'Train size: {len(train)}')\n",
        "print(f'Validation size: {len(valid)}')\n",
        "print(f'Test size: {len(test)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udpDm9lsQi2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build vocabulary with maximum size (less frequent words are not considered)\n",
        "# load the pre-trained word embeddings.\n",
        "EMBEDDING_DIM = 50\n",
        "\n",
        "text_field.build_vocab(train, max_size=25000, vectors=f\"glove.6B.{EMBEDDING_DIM}d\")\n",
        "label_field.build_vocab(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYWVMjoGO9jE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get iterators over the data\n",
        "# place iterators on the GPU if possible\n",
        "\n",
        "# define our batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "  (train, valid, test),\n",
        "  batch_sizes=(BATCH_SIZE, BATCH_SIZE, BATCH_SIZE), device=DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1Gr756JtRcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_data(data_iter, model, loss_fn):\n",
        "  model.eval()\n",
        "  loss = 0\n",
        "  acc = 0\n",
        "  denom = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in data_iter:\n",
        "      # place on the GPU          \n",
        "      feature, target = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
        "      predictions = model(feature.t()).squeeze(1)\n",
        "      \n",
        "      _loss = loss_fn(predictions, target)\n",
        "      loss += (_loss.item() * predictions.shape[0])\n",
        "      acc += (accuracy(predictions, target) * predictions.shape[0])\n",
        "      denom += predictions.shape[0]\n",
        "\n",
        "  model.train()\n",
        "  return loss / denom, acc / denom"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP-EZueBPo1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(train_iter, dev_iter, model, loss_fn, n_epochs):\n",
        "  for epoch in range(1, n_epochs + 1): \n",
        "    print(f'Starting epoch {epoch}')\n",
        "    train_loss = 0\n",
        "    train_loss_denom = 0\n",
        "    train_acc = 0\n",
        "    model.train()\n",
        "    \n",
        "    # iterate over batches\n",
        "    for batch in train_iter:\n",
        "        # place on the GPU          \n",
        "        feature, target = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(feature.t()).squeeze(1)\n",
        "          \n",
        "        loss = loss_fn(predictions, target)\n",
        "        acc = accuracy(predictions, target)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += (loss.item() * predictions.shape[0])\n",
        "        train_loss_denom += predictions.shape[0]\n",
        "        train_acc += (acc * predictions.shape[0])\n",
        "        \n",
        "    valid_loss, valid_acc = eval_data(dev_iter, model, loss_fn)\n",
        "\n",
        "    # Normalize everything\n",
        "    train_loss /= train_loss_denom\n",
        "    train_acc /= train_loss_denom\n",
        "\n",
        "    print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:6.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:6.2f}%')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_g02vvYMSVZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_OUT_CHANNELS = 100\n",
        "LRATE = 0.5\n",
        "DROPOUT = 0.4\n",
        "WINDOW_SIZE = 1\n",
        "\n",
        "# Construct the model\n",
        "model = CNN(len(text_field.vocab), EMBEDDING_DIM, N_OUT_CHANNELS, WINDOW_SIZE, OUTPUT_DIM, DROPOUT)\n",
        "print(model)\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=LRATE)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Start training\n",
        "train_model(train_iterator, valid_iterator, model, loss_fn, n_epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0DSqbKBRCKy",
        "colab_type": "text"
      },
      "source": [
        "**Q: The paper Convolutional Neural Networks for Sentence Classification (Kim, 2014) applies 3 convolutional layers in parallel with window sizes [3, 4, 5]. Try to extend our CNN model with 2 more convolution layers and apply these window sizes. Outputs of the pooling layers are concatenated. What will be the effect on the model performance?**\n",
        "\n",
        "**Hint:** you can use the `nn.ModuleList function`.\n",
        "\n",
        "**Q: Pre-processing: experiment with filtering out stop words from input data. What will be the effect on the performance? You may choose to use spaCy to get a list of stop words. Here's an example:**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQpqpHFKRYOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example\n",
        "spacy_nlp = spacy.load('en_core_web_sm')\n",
        "spacy_stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "print(spacy_stop_words)\n",
        "text_field = data.Field(tokenize='spacy', lower=True, stop_words=spacy_stop_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ely0azrRsW5",
        "colab_type": "text"
      },
      "source": [
        "**Q: Apply a Naive Bayes classifier to the problem. How would it perform for this task? You can use the `sklearn.naive_bayes.MultinomialNB` implementation from the popular `scikit-learn` toolkit. Extraction of the data for this purpose could be performed as follows:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ1EoMnuXgCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for example in train:\n",
        "  if example.label == 'pos':\n",
        "      label = 1\n",
        "  else:\n",
        "      label = 0"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}