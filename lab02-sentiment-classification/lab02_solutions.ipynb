{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab02_SentimentClassification_Solutions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qS8y5_Ewv6n"
      },
      "source": [
        "# Uncomment below if you want to install packages into your local environment\n",
        "# Colab already provides the required packages for this lab\n",
        "\n",
        "#! pip install torch"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEJM0h_uctzV"
      },
      "source": [
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# we fix the seeds to get consistent results before every training\n",
        "# loop in what follows\n",
        "def fix_seed(seed=234):\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE8B9-L8U0aZ"
      },
      "source": [
        "# Text classification: Sentiment analysis\n",
        "\n",
        "In this notebook we are going to build state-of-the art models for text classification using the example of sentiment analysis. To be more precise, we will build a feed-forward neural network (FFNN) and a convolutional neural network (CNN). We will look into the details of data preparation, functioning of each model and how the performance of those NNs could be measured efficiently. We will start our work using a toy corpus. Further you can extend your knowledge and use a larger dataset.\n",
        "\n",
        "Again we are using [pytorch](https://www.pytorch.org), an open source deep learning platform, as our backbone library in the course.\n",
        "\n",
        "Here are the toy training and validation sets. It is good practise to use the validation set (a representative set of the test data). This set is used to tune hyperparameters and choose a configuration for your model to ensure the best performance. Our toy sets are already tokenized and lowercased."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abMSnMwJx8bu"
      },
      "source": [
        " # Our toy sentiment analysis corpus\n",
        "train = ['i like his paper !',\n",
        "         'what a well-written essay !',\n",
        "         'i do not agree with the criticism on this paper',\n",
        "         'well done ! it was an enjoyable reading',\n",
        "         'it was very good . send me a copy please .',\n",
        "         'the argumentation in the paper is very weak',\n",
        "         'poor effort !',\n",
        "         'the methodology could have been more detailed',\n",
        "         'i am not impressed',\n",
        "         'could have done better .',\n",
        "]\n",
        "\n",
        "train_labels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
        "\n",
        "# Validation set\n",
        "valid = ['i like your paper', \n",
        "         'i agree with your results', \n",
        "         'what a success ! a well-written paper', \n",
        "         'not enough details . very poor', \n",
        "         'i support the criticism',\n",
        "         'could be better',\n",
        "]\n",
        "\n",
        "valid_labels = [1, 1, 1, 0, 0, 0]"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hPlpqVFYpfL"
      },
      "source": [
        "## Pre-processing\n",
        "\n",
        "Using the material from the previous lab session, fill in the below function to tokenize the corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ5IL1e1GVn6"
      },
      "source": [
        "def get_tokenized_corpus(corpus):\n",
        "  tokenized_corpus = []\n",
        "\n",
        "  #######################\n",
        "  # Q: Process the corpus\n",
        "  #######################\n",
        "  for sentence in corpus:\n",
        "    tokenized_sentence = []\n",
        "    for token in sentence.split(' '): \n",
        "      tokenized_sentence.append(token)\n",
        "    tokenized_corpus.append(tokenized_sentence)\n",
        " \n",
        "  return tokenized_corpus"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyKCfY6QbO0r"
      },
      "source": [
        "## Word2index dictionary\n",
        "\n",
        "Similar to the way it was done in the previous lab, we define here a method that returns a word to index dictionary. Note that we reserve the 0 index for the padding token `<pad>`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bWQ3zKYKlQU"
      },
      "source": [
        "def get_word2idx(tokenized_corpus):\n",
        "  vocabulary = []\n",
        "  for sentence in tokenized_corpus:\n",
        "    for token in sentence:\n",
        "        if token not in vocabulary:\n",
        "            vocabulary.append(token)\n",
        "  \n",
        "  word2idx = {w: idx+1 for (idx, w) in enumerate(vocabulary)}\n",
        "  # we reserve the 0 index for the padding token\n",
        "  word2idx['<pad>'] = 0\n",
        "  \n",
        " \n",
        "  return word2idx"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hve1MProNBvp"
      },
      "source": [
        "## Preparation of inputs\n",
        "\n",
        "The first layer of our FFNN will be an embedding (look-up) layer which takes as input indexes of tokens (we do not need to one-hot encode our vectors).\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Why do we need to fix the length of our input vectors (we take the maximum sentence length here) ? This process is referred to as padding. Print the padded training corpus.**\n",
        "\n",
        "*A: We are preparing inputs to FFNN which takes fized-size vectors as inputs. Padding fixes input sentence size.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-xp_gKvObfw",
        "outputId": "dda6db3c-9bbb-4408-8ff5-593af3f78def",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def get_model_inputs(tokenized_corpus, word2idx, labels):\n",
        "  # we index our sentences\n",
        "  vectorized_sents = [[word2idx[tok] for tok in sent if tok in word2idx] for sent in tokenized_corpus]\n",
        "\n",
        "  # Sentence lengths\n",
        "  sent_lengths = [len(sent) for sent in vectorized_sents]\n",
        "\n",
        "  # Get maximum length\n",
        "  max_len = max(sent_lengths)\n",
        "  \n",
        "  # we create a tensor of a fixed size filled with zeroes for padding\n",
        "  sent_tensor = torch.zeros((len(vectorized_sents), max_len)).long()\n",
        "\n",
        "  # we fill it with our vectorized sentences \n",
        "  for idx, (sent, sentlen) in enumerate(zip(vectorized_sents, sent_lengths)):\n",
        "    sent_tensor[idx, :sentlen] = torch.LongTensor(sent)\n",
        "\n",
        "  # Label tensor\n",
        "  label_tensor = torch.FloatTensor(labels)\n",
        "  \n",
        "  return sent_tensor, label_tensor\n",
        "\n",
        "###\n",
        "\n",
        "tokenized_corpus = get_tokenized_corpus(train)\n",
        "word2idx = get_word2idx(tokenized_corpus)\n",
        "train_sent_tensor, train_label_tensor = get_model_inputs(tokenized_corpus, word2idx, train_labels)\n",
        "\n",
        "print(f'Vocabulary size: {len(word2idx)}')\n",
        "print('Training set tensor:')\n",
        "print(train_sent_tensor)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 47\n",
            "Training set tensor:\n",
            "tensor([[ 1,  2,  3,  4,  5,  0,  0,  0,  0,  0,  0],\n",
            "        [ 6,  7,  8,  9,  5,  0,  0,  0,  0,  0,  0],\n",
            "        [ 1, 10, 11, 12, 13, 14, 15, 16, 17,  4,  0],\n",
            "        [18, 19,  5, 20, 21, 22, 23, 24,  0,  0,  0],\n",
            "        [20, 21, 25, 26, 27, 28, 29,  7, 30, 31, 27],\n",
            "        [14, 32, 33, 14,  4, 34, 25, 35,  0,  0,  0],\n",
            "        [36, 37,  5,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [14, 38, 39, 40, 41, 42, 43,  0,  0,  0,  0],\n",
            "        [ 1, 44, 11, 45,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [39, 40, 19, 46, 27,  0,  0,  0,  0,  0,  0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGg6CtALe8Va"
      },
      "source": [
        "# Building the Feed-Forward Neural Network\n",
        "\n",
        "We will start by building a very simple feed-forward neural network (FFNN).\n",
        "Our FFNN class is a sub-class of `nn.Module`. Within the `__init__` method, we define the layers of the module:\n",
        "\n",
        "- Our first layer is an embedding layer (look-up layer). This layer could be initialized with pre-trained embeddings (as we will see at the end of this lab) or could be trained together with other layers.\n",
        " \n",
        "- The next layer is a fully connected layer followed by a ReLU activation.\n",
        "\n",
        "- Finally, the last linear layer is the output layer for the classification task.\n",
        "\n",
        "The `forward()` method is called when we feed data into our model. Please note that the output dimension of each layer is the input dimension for the next one.\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Recall from the previous lab the functioning of a lookup layer. How does the mapping to the dense representation happen?**\n",
        "\n",
        "*A: We multiply the one-hot input vector of the size of the vocabulary by a matrix of the shape `vocabulary size X embedding size`.*\n",
        "\n",
        "**Q: Implement the averaging of embeddings in the `forward()` method of the class below.**\n",
        "\n",
        "*A: If all the input sentences had the same lengths, i.e. there were no 0-padded positions, the solution would simply be* `embedded.mean(1)`*. But in this case, we have to take care of independent sentence lengths when averaging, by first summing the embeddings and then normalizing the sum by the corresponding length:*\n",
        "\n",
        "```python\n",
        "sent_lens = x.ne(0).sum(1, keepdims=True)\n",
        "averaged = embeddings.sum(1) / sent_lens\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LWut1gtXGQN"
      },
      "source": [
        "class FFNN(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, num_classes):  \n",
        "        super(FFNN, self).__init__()\n",
        "        \n",
        "        # embedding (lookup layer) layer\n",
        "        # padding_idx argument makes sure that the 0-th token in the vocabulary\n",
        "        # is used for padding purposes i.e. its embedding will be a 0-vector\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        \n",
        "        # hidden layer\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
        "        \n",
        "        # activation\n",
        "        self.relu1 = nn.ReLU()\n",
        "        \n",
        "        # output layer\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x has shape (batch_size, max_sent_len)\n",
        "\n",
        "        embedded = self.embedding(x)\n",
        "        # `embedding` has shape (batch size, max_sent_len, embedding dim)\n",
        "\n",
        "        ########################################################################\n",
        "        # Q: Compute the average embeddings of shape (batch_size, embedding_dim)\n",
        "        ########################################################################\n",
        "        # Implement averaging that ignores padding (average using actual sentence lengths).\n",
        "        # How this effect the result?\n",
        "        \n",
        "        sent_lens = x.ne(0).sum(1, keepdims=True)\n",
        "        averaged = embedded.sum(1) / sent_lens\n",
        "\n",
        "        out = self.fc1(averaged)\n",
        "        out = self.relu1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcAI5BtaQMFh"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "In this section we will define the hyperparameters of our model, the loss function, the optimizer and perform a number of training epochs over our toy training corpus.\n",
        "\n",
        "We will use the **Stochastic gradient descent (SGD)** optimizer. The learning rate hyperparameter of the optimizer controls how the weights are adjusted with respect to the loss gradient. The lower the value, the more fine-grained are weight updates.\n",
        "\n",
        "**Note that** it is a common practise to perform training using mini-batches (sets of training instances seen by the model during weight update step). In this case, the epoch loss is defined as the loss averaged across the mini-batches. Since our corpus is very small, we train on the whole training set without batching.\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Why is the number of output classes is equal to 1 for binary classification?**\n",
        "\n",
        "*A: The output in the case of the sigmoid transformation is considered as a probability of the positive class. If it is $>=0.5$ the output class is $1$, $0$ otherwise.* \n",
        "\n",
        "**Q: Try to modify the learning rate (which is initially set to 0.5 below) in the range $[0.0001, 0.5]$. How does the loss react to these changes?**\n",
        "\n",
        "*A: The loss will typically change slower for a lower learning rate.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrtOwTUsyArb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df5e711d-5e75-4cd2-e07c-53aed1cbf212"
      },
      "source": [
        "# Reset the seed before every model construction for reproducible results\n",
        "fix_seed()\n",
        "\n",
        "# we will train for N epochs (The model will see the corpus N times)\n",
        "EPOCHS = 10\n",
        "\n",
        "# Learning rate is initially set to 0.5\n",
        "LRATE = 0.5\n",
        "\n",
        "# we define our embedding dimension (dimensionality of the output of the first layer)\n",
        "EMBEDDING_DIM = 50\n",
        "\n",
        "# dimensionality of the output of the second hidden layer\n",
        "HIDDEN_DIM = 50\n",
        "\n",
        "# the output dimension is the number of classes, 1 for binary classification\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "# Construct the model\n",
        "model = FFNN(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx), OUTPUT_DIM)\n",
        "\n",
        "# Print the model\n",
        "print(model)\n",
        "\n",
        "# we use the stochastic gradient descent (SGD) optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=LRATE)\n",
        "\n",
        "# we use the binary cross-entropy loss with sigmoid (applied to logits) \n",
        "# Recall that we did not apply any activation to our output layer, hence we need\n",
        "# to make our outputs look like probabilities.\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Input and label tensors\n",
        "feature = train_sent_tensor\n",
        "target = train_label_tensor\n",
        "\n",
        "################\n",
        "# Start training\n",
        "################\n",
        "print(f'Will train for {EPOCHS} epochs')\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  # to ensure the dropout (explained later) is \"turned on\" while training\n",
        "  # good practice to include even if do not use here\n",
        "  model.train()\n",
        "  \n",
        "  # we zero the gradients as they are not removed automatically\n",
        "  optimizer.zero_grad()\n",
        "  \n",
        "  # squeeze is needed as the predictions will have the shape (batch size, 1)\n",
        "  # and we need to remove the dimension of size 1\n",
        "  predictions = model(feature).squeeze(1)\n",
        "\n",
        "  # Compute the loss\n",
        "  loss = loss_fn(predictions, target)\n",
        "  train_loss = loss.item()\n",
        "\n",
        "  # calculate the gradient of each parameter\n",
        "  loss.backward()\n",
        "\n",
        "  # update the parameters using the gradients and optimizer algorithm \n",
        "  optimizer.step()\n",
        "  \n",
        "  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f}')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FFNN(\n",
            "  (embedding): Embedding(47, 50, padding_idx=0)\n",
            "  (fc1): Linear(in_features=50, out_features=50, bias=True)\n",
            "  (relu1): ReLU()\n",
            "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
            ")\n",
            "Will train for 10 epochs\n",
            "| Epoch: 01 | Train Loss: 0.699\n",
            "| Epoch: 02 | Train Loss: 0.663\n",
            "| Epoch: 03 | Train Loss: 0.631\n",
            "| Epoch: 04 | Train Loss: 0.599\n",
            "| Epoch: 05 | Train Loss: 0.564\n",
            "| Epoch: 06 | Train Loss: 0.525\n",
            "| Epoch: 07 | Train Loss: 0.483\n",
            "| Epoch: 08 | Train Loss: 0.437\n",
            "| Epoch: 09 | Train Loss: 0.391\n",
            "| Epoch: 10 | Train Loss: 0.347\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwY-k6x0xIkl"
      },
      "source": [
        "## Measuring the accuracy\n",
        "\n",
        "In addition to measuring the loss, we can also evaluate the actual classification performance of our model. (In the case of training with mini-batches, the epoch accuracy is defined as the accuracy averaged across the mini-batches.)\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Fill in the below function so that it computes the accuracy of the model. Once you are done, improve the previous loop so that it also prints the training accuracy after each epoch.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R23R0XjuPTn"
      },
      "source": [
        "def accuracy(output, target):\n",
        "  #####################################\n",
        "  # Q: Return the accuracy of the model\n",
        "  #####################################\n",
        "  # Pass through the sigmoid and round the values to 0 or 1\n",
        "  output = torch.round(torch.sigmoid(output))\n",
        "  correct = (output == target).float()\n",
        "  acc = correct.mean()\n",
        "\n",
        "  return acc"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eP2s5MyyrNO",
        "outputId": "b1aab344-a84e-4579-b6aa-1917bdca0abc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Reset the seed for consistent results\n",
        "fix_seed()\n",
        "\n",
        "# we will train for N epochs (The model will see the corpus N times)\n",
        "EPOCHS = 10\n",
        "\n",
        "# Learning rate is initially set to 0.5\n",
        "LRATE = 0.5\n",
        "\n",
        "# we define our embedding dimension (dimensionality of the output of the first layer)\n",
        "EMBEDDING_DIM = 50\n",
        "\n",
        "# dimensionality of the output of the second hidden layer\n",
        "HIDDEN_DIM = 50\n",
        "\n",
        "# the output dimension is the number of classes, 1 for binary classification\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "# Construct the model\n",
        "model = FFNN(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx), OUTPUT_DIM)\n",
        "\n",
        "# Print the model\n",
        "print(model)\n",
        "\n",
        "# we use the stochastic gradient descent (SGD) optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=LRATE)\n",
        "\n",
        "# we use the binary cross-entropy loss with sigmoid (applied to logits) \n",
        "# Recall that we did not apply any activation to our output layer, hence we need\n",
        "# to make our outputs look like probabilities.\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Input and label tensors\n",
        "feature = train_sent_tensor\n",
        "target = train_label_tensor\n",
        "\n",
        "################\n",
        "# Start training\n",
        "################\n",
        "print(f'Will train for {EPOCHS} epochs')\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  # to ensure the dropout (explained later) is \"turned on\" while training\n",
        "  # good practice to include even if do not use here\n",
        "  model.train()\n",
        "  \n",
        "  # we zero the gradients as they are not removed automatically\n",
        "  optimizer.zero_grad()\n",
        "  \n",
        "  # squeeze is needed as the predictions will have the shape (batch size, 1)\n",
        "  # and we need to remove the dimension of size 1\n",
        "  predictions = model(feature).squeeze(1)\n",
        "\n",
        "  # Compute the loss\n",
        "  loss = loss_fn(predictions, target)\n",
        "  train_loss = loss.item()\n",
        "  \n",
        "  #####################\n",
        "  # Q: Compute accuracy\n",
        "  #####################\n",
        "  train_acc = accuracy(predictions, target)\n",
        "\n",
        "  # calculate the gradient of each parameter\n",
        "  loss.backward()\n",
        "\n",
        "  # update the parameters using the gradients and optimizer algorithm \n",
        "  optimizer.step()\n",
        "\n",
        "  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FFNN(\n",
            "  (embedding): Embedding(47, 50, padding_idx=0)\n",
            "  (fc1): Linear(in_features=50, out_features=50, bias=True)\n",
            "  (relu1): ReLU()\n",
            "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
            ")\n",
            "Will train for 10 epochs\n",
            "| Epoch: 01 | Train Loss: 0.699 | Train Acc: 50.00%\n",
            "| Epoch: 02 | Train Loss: 0.663 | Train Acc: 70.00%\n",
            "| Epoch: 03 | Train Loss: 0.631 | Train Acc: 90.00%\n",
            "| Epoch: 04 | Train Loss: 0.599 | Train Acc: 90.00%\n",
            "| Epoch: 05 | Train Loss: 0.564 | Train Acc: 100.00%\n",
            "| Epoch: 06 | Train Loss: 0.525 | Train Acc: 100.00%\n",
            "| Epoch: 07 | Train Loss: 0.483 | Train Acc: 100.00%\n",
            "| Epoch: 08 | Train Loss: 0.437 | Train Acc: 100.00%\n",
            "| Epoch: 09 | Train Loss: 0.391 | Train Acc: 100.00%\n",
            "| Epoch: 10 | Train Loss: 0.347 | Train Acc: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2UhMGlMq3Re"
      },
      "source": [
        "## Hyperparameter tuning on the validation set\n",
        "\n",
        "You should now apply the previous pre-processing and input preparation procedures to the validation set as well.\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Should we re-use the word to index dictionary we created before? Why?**\n",
        "\n",
        "*A: Yes because the training vocabulary is the one known to the model, we have to use that one to map our tokens.*\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9_FERooxUQR",
        "outputId": "fc770731-d3d7-42a7-cb3e-a596eb30dad2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "###############################################\n",
        "# Q: Prepare the validation corpus and labels #\n",
        "###############################################\n",
        "tokenized_valid_corpus = get_tokenized_corpus(valid)\n",
        "valid_sent_tensor, valid_label_tensor = get_model_inputs(tokenized_valid_corpus, word2idx, valid_labels)\n",
        "print(valid_sent_tensor)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1,  2,  4,  0,  0,  0],\n",
            "        [ 1, 12, 13,  0,  0,  0],\n",
            "        [ 6,  7,  5,  7,  8,  4],\n",
            "        [11, 27, 25, 36,  0,  0],\n",
            "        [ 1, 14, 15,  0,  0,  0],\n",
            "        [39, 46,  0,  0,  0,  0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUlXfydh1Uxv"
      },
      "source": [
        "**Q: Try to modify the learning rate and the number of epochs now. How will the validation loss and accuracy react to those changes?**\n",
        "\n",
        "*A: Typically the validation loss and accuracy will change slower with a lower learning rate. This potentially increases the chance of an optimal training result. However, the optimization will take longer time because steps towards the minimum of the loss function are smaller. Hence, we increase the number of epochs. A very high learning rate risks to cause the loss to \"bounce around\" and even overshoot the optimum, preventing convergence.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcKPET9BHb-7",
        "outputId": "563458f0-2b70-4b9e-8c35-781c5a4832c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Reset the seed for consistent results\n",
        "fix_seed()\n",
        "\n",
        "# we will train for N epochs (The model will see the corpus N times)\n",
        "EPOCHS = 10\n",
        "\n",
        "# Learning rate is initially set to 0.5\n",
        "LRATE = 0.5\n",
        "\n",
        "# we define our embedding dimension (dimensionality of the output of the first layer)\n",
        "EMBEDDING_DIM = 50\n",
        "\n",
        "# dimensionality of the output of the second hidden layer\n",
        "HIDDEN_DIM = 50\n",
        "\n",
        "# the output dimension is the number of classes, 1 for binary classification\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "# Construct the model\n",
        "model = FFNN(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx), OUTPUT_DIM)\n",
        "\n",
        "# we use the stochastic gradient descent (SGD) optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=LRATE)\n",
        "\n",
        "# we use the binary cross-entropy loss with sigmoid (applied to logits) \n",
        "# Recall that we did not apply any activation to our output layer, hence we need\n",
        "# to make our outputs look like probabilities.\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Input and label tensors for training\n",
        "feature_train = train_sent_tensor\n",
        "target_train = train_label_tensor\n",
        "\n",
        "# Input and label tensors for validation\n",
        "feature_valid = valid_sent_tensor\n",
        "target_valid = valid_label_tensor\n",
        "\n",
        "################\n",
        "# Start training\n",
        "################\n",
        "print(f'Will train for {EPOCHS} epochs')\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  # to ensure the dropout (explained later) is \"turned on\" while training\n",
        "  # good practice to include even if do not use here\n",
        "  model.train()\n",
        "  \n",
        "  # we zero the gradients as they are not removed automatically\n",
        "  optimizer.zero_grad()\n",
        "  \n",
        "  # squeeze is needed as the predictions will have the shape (batch size, 1)\n",
        "  # and we need to remove the dimension of size 1\n",
        "  predictions = model(feature_train).squeeze(1)\n",
        "\n",
        "  # Compute the loss\n",
        "  loss = loss_fn(predictions, target_train)\n",
        "  train_loss = loss.item()\n",
        "\n",
        "  # Compute training accuracy\n",
        "  train_acc = accuracy(predictions, target_train)\n",
        "\n",
        "  # calculate the gradient of each parameter\n",
        "  loss.backward()\n",
        "\n",
        "  # update the parameters using the gradients and optimizer algorithm \n",
        "  optimizer.step()\n",
        "  \n",
        "  # this puts the model in \"evaluation mode\" (turns off dropout and batch normalization)\n",
        "  # good practise to include even if we do not use them right now\n",
        "  model.eval()\n",
        "\n",
        "  # we do not compute gradients within this block, i.e. no training\n",
        "  with torch.no_grad():\n",
        "    predictions_valid = model(feature_valid).squeeze(1)\n",
        "    valid_loss = loss_fn(predictions_valid, target_valid).item()\n",
        "    valid_acc = accuracy(predictions_valid, target_valid)\n",
        "  \n",
        "  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:6.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:6.2f}% |')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Will train for 10 epochs\n",
            "| Epoch: 01 | Train Loss: 0.699 | Train Acc:  50.00% | Val. Loss: 0.654 | Val. Acc:  66.67% |\n",
            "| Epoch: 02 | Train Loss: 0.663 | Train Acc:  70.00% | Val. Loss: 0.632 | Val. Acc: 100.00% |\n",
            "| Epoch: 03 | Train Loss: 0.631 | Train Acc:  90.00% | Val. Loss: 0.607 | Val. Acc: 100.00% |\n",
            "| Epoch: 04 | Train Loss: 0.599 | Train Acc:  90.00% | Val. Loss: 0.579 | Val. Acc: 100.00% |\n",
            "| Epoch: 05 | Train Loss: 0.564 | Train Acc: 100.00% | Val. Loss: 0.545 | Val. Acc: 100.00% |\n",
            "| Epoch: 06 | Train Loss: 0.525 | Train Acc: 100.00% | Val. Loss: 0.506 | Val. Acc: 100.00% |\n",
            "| Epoch: 07 | Train Loss: 0.483 | Train Acc: 100.00% | Val. Loss: 0.463 | Val. Acc: 100.00% |\n",
            "| Epoch: 08 | Train Loss: 0.437 | Train Acc: 100.00% | Val. Loss: 0.420 | Val. Acc: 100.00% |\n",
            "| Epoch: 09 | Train Loss: 0.391 | Train Acc: 100.00% | Val. Loss: 0.375 | Val. Acc: 100.00% |\n",
            "| Epoch: 10 | Train Loss: 0.347 | Train Acc: 100.00% | Val. Loss: 0.335 | Val. Acc: 100.00% |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npzWXeQATPs4"
      },
      "source": [
        "## Testing the model\n",
        "\n",
        "Now let us test our trained model. We define a small test set below. First, apply the data preparation procedures to this test set as you did for the validation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Cx2eJi1a8R6",
        "outputId": "e282fab5-7274-47a6-839d-c4ad57dcb751",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test = ['i really do not like your paper', \n",
        "        'well done', \n",
        "        'good results for a paper !',\n",
        "        'amazing effort', \n",
        "        'your effort is poor !', \n",
        "        'not impressed'   \n",
        "]\n",
        "\n",
        "test_labels = [0, 1, 1, 1, 0, 0]\n",
        "\n",
        "#########################################\n",
        "# Q: Prepare the test corpus and labels #\n",
        "#########################################\n",
        "tokenized_test_corpus = get_tokenized_corpus(test)\n",
        "test_sent_tensor, test_label_tensor = get_model_inputs(tokenized_test_corpus, word2idx, test_labels)\n",
        "print(test_sent_tensor)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1, 10, 11,  2,  4],\n",
            "        [18, 19,  0,  0,  0],\n",
            "        [26,  7,  4,  5,  0],\n",
            "        [37,  0,  0,  0,  0],\n",
            "        [37, 34, 36,  5,  0],\n",
            "        [11, 45,  0,  0,  0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0A1Iu-p4FUL"
      },
      "source": [
        "**Q: Fill in the below function for the computation of F-measure. Once done, complete the missing lines in the final evaluation part.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixhvaYkW_SfX",
        "outputId": "de8f474e-0926-48e4-be3c-de3a2afd5ac6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def f_measure(output, gold):\n",
        "  ############################################\n",
        "  # Q: Compute precision, recall and f-measure \n",
        "  ############################################\n",
        "  pred = torch.round(torch.sigmoid(output))\n",
        "  pred = pred.detach().cpu().numpy()\n",
        "     \n",
        "  test_pos_preds = np.sum(pred)\n",
        "  test_pos_real = np.sum(gold)\n",
        "    \n",
        "  correct = (np.logical_and(pred, gold)).astype(int)\n",
        "  correct = np.sum(correct)\n",
        "  \n",
        "  precision = correct / test_pos_preds\n",
        "  recall = correct / test_pos_real\n",
        "  \n",
        "  fscore = (2.0 * precision * recall) / (precision + recall)\n",
        "\n",
        "  # Print them\n",
        "  print(f\"     Recall: {recall:.2f}, Precision: {precision:.2f}, F-measure: {fscore:.2f}\")\n",
        "  \n",
        "\n",
        "####\n",
        "\n",
        "model.eval()\n",
        "\n",
        "feature_test = test_sent_tensor\n",
        "target_test = test_label_tensor\n",
        "\n",
        "with torch.no_grad():\n",
        "  ####################################################################\n",
        "  # Q: Get predictions for the test set, compute the loss and accuracy\n",
        "  ####################################################################\n",
        "  predictions = model(feature_test).squeeze(1)\n",
        "  test_loss = loss_fn(predictions, target_test).item()\n",
        "  test_acc = accuracy(predictions, target_test)\n",
        "\n",
        "  # Print\n",
        "  print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
        "  f_measure(predictions, test_labels)  "
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.059 | Test Acc: 50.00%\n",
            "     Recall: 0.33, Precision: 0.50, F-measure: 0.40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofMn0IjZ3lUr"
      },
      "source": [
        "**Q:  Are the resulting evaluations different ? How do you interpret those differences? Print the predictions.**\n",
        "\n",
        "*A: Accuracy is higher than F-measure. F-measure focuses on the results for the positive class in terms of their precision and recall (for our example precision is high, recall is low), while accuracy compares all the predictions for both classes to gold labels.*\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2znwTJeU3UT"
      },
      "source": [
        "# Building the Convolutional Neural Network (CNN)\n",
        "\n",
        "We will implement a model inspired by the state-of-art CNN model as described in [Convolutional Neural Networks for Sentence Classification (Kim, 2014)](https://arxiv.org/abs/1408.5882).\n",
        "\n",
        "Similar to the FFNN model, we start with an embedding layer. We implement the convolutional layer with the help of `nn.Conv2d` and use the ReLU activation after it. The above-mentioned paper, being inspired by the convolution for images, applies a 2-dimensional convolution: a (window size, embedding dimension) filter. It covers `n` sequential words, taking embedding dimensions as the width. We then pass the tensors through a **max pooling layer**.\n",
        "\n",
        "The **max pooling layer** is typically followed by a **dropout** layer. The latter sets a random set of activations in the max-pooling layer to zero. This prevents the network from learning to rely on specific weights and helps to prevent overfitting. Note that the dropout layer is only used during training, and not during test time.\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Study the shapes of outputs coming from convolution and max pooling layers. What is the shape of the max pooling layer output?**\n",
        "\n",
        "*A:* `batch_size X n_filters`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUPETZaOgvgB"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, out_channels, window_size, output_dim, dropout):\n",
        "    super(CNN, self).__init__()\n",
        "    \n",
        "    # Create the embedding layer as usual\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "    \n",
        "    # in_channels -- 1 text channel\n",
        "    # out_channels -- the number of output channels\n",
        "    # kernel_size is (window size x embedding dim)\n",
        "    self.conv = nn.Conv2d(\n",
        "      in_channels=1, out_channels=out_channels,\n",
        "      kernel_size=(window_size, embedding_dim))\n",
        "    \n",
        "    # the dropout layer\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # the output layer\n",
        "    self.fc = nn.Linear(out_channels, output_dim)\n",
        "        \n",
        "  def forward(self, x):\n",
        "    # x -> (batch size, max_sent_length)\n",
        "    \n",
        "    embedded = self.embedding(x)\n",
        "    # embedded -> (batch size, max_sent_length, embedding_dim)\n",
        "    \n",
        "    # images have 3 RGB channels \n",
        "    # for the text we add 1 channel\n",
        "    embedded = embedded.unsqueeze(1)\n",
        "    # embedded -> (batch size, 1, max_sent_length, embedding dim)\n",
        "\n",
        "    # Compute the feature maps      \n",
        "    feature_maps = self.conv(embedded)\n",
        "\n",
        "    ##########################################\n",
        "    # Q: What is the shape of `feature_maps` ?\n",
        "    ##########################################\n",
        "    # A: (batch size, n filters, max_sent_length - window size + 1, 1)\n",
        "    \n",
        "    feature_maps = feature_maps.squeeze(3)\n",
        "    \n",
        "    # Q: why do we remove 1 dimension here?\n",
        "    # A: we do need the 1 channel anymore\n",
        "    \n",
        "    # Apply ReLU\n",
        "    feature_maps = F.relu(feature_maps)\n",
        "    \n",
        "    # Apply the max pooling layer\n",
        "    pooled = F.max_pool1d(feature_maps, feature_maps.shape[2])\n",
        "    \n",
        "    pooled = pooled.squeeze(2)\n",
        "\n",
        "    ####################################\n",
        "    # Q: What is the shape of `pooled` ?\n",
        "    ####################################\n",
        "    # A: (batch size, n_filters)\n",
        "    \n",
        "    dropped = self.dropout(pooled)\n",
        "    preds = self.fc(dropped)\n",
        "    \n",
        "    return preds"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UZroMvLtFVv"
      },
      "source": [
        "## Training and testing the CNN\n",
        "\n",
        "Here we will define the CNN-specific hyperparameters and perform the network training and testing. **Note that** the learning rate is initially set to 0.1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lq1lcWbqRwNY",
        "outputId": "4c9206a8-e339-4ca1-8cc9-e0d51f96b927",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "fix_seed()\n",
        "\n",
        "EPOCHS = 10\n",
        "LRATE = 0.1\n",
        "\n",
        "EMBEDDING_DIM = 50\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "# the hyperparameters specific to CNN\n",
        "# we define the number of filters\n",
        "N_OUT_CHANNELS = 100\n",
        "\n",
        "# we define the window size\n",
        "WINDOW_SIZE = 1\n",
        "\n",
        "# we apply the dropout with the probability 0.2\n",
        "DROPOUT = 0.2\n",
        "\n",
        "# Construct the model\n",
        "model = CNN(len(word2idx), EMBEDDING_DIM, N_OUT_CHANNELS, WINDOW_SIZE, OUTPUT_DIM, DROPOUT)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=LRATE)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "feature_train = train_sent_tensor\n",
        "target_train = train_label_tensor\n",
        "\n",
        "feature_valid = valid_sent_tensor\n",
        "target_valid = valid_label_tensor\n",
        "\n",
        "feature_test = test_sent_tensor\n",
        "target_test = test_label_tensor\n",
        "\n",
        "################\n",
        "# Start training\n",
        "################\n",
        "print(f'Will train for {EPOCHS} epochs')\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  model.train()\n",
        "  \n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  predictions = model(feature_train).squeeze(1)\n",
        "  loss = loss_fn(predictions, target_train)\n",
        "  train_loss = loss.item()\n",
        "  train_acc = accuracy(predictions, target_train)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    predictions_valid = model(feature_valid).squeeze(1)\n",
        "    valid_loss = loss_fn(predictions_valid, target_valid).item()\n",
        "    valid_acc = accuracy(predictions_valid, target_valid)\n",
        "  \n",
        "  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:6.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:6.2f}% |')\n",
        "\n",
        "\n",
        "## Finally, test on the test set\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    predictions = model(feature_test).squeeze(1)\n",
        "    loss = loss_fn(predictions, target_test)\n",
        "    acc = accuracy(predictions, target_test)\n",
        "    print(f'Test Loss: {loss:.3f} | Test Acc: {acc*100:.2f}%')\n",
        "    f_measure(predictions, test_labels)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Will train for 10 epochs\n",
            "| Epoch: 01 | Train Loss: 0.659 | Train Acc:  50.00% | Val. Loss: 0.657 | Val. Acc:  50.00% |\n",
            "| Epoch: 02 | Train Loss: 0.610 | Train Acc:  70.00% | Val. Loss: 0.626 | Val. Acc:  83.33% |\n",
            "| Epoch: 03 | Train Loss: 0.557 | Train Acc:  80.00% | Val. Loss: 0.586 | Val. Acc: 100.00% |\n",
            "| Epoch: 04 | Train Loss: 0.474 | Train Acc: 100.00% | Val. Loss: 0.548 | Val. Acc: 100.00% |\n",
            "| Epoch: 05 | Train Loss: 0.467 | Train Acc: 100.00% | Val. Loss: 0.517 | Val. Acc: 100.00% |\n",
            "| Epoch: 06 | Train Loss: 0.427 | Train Acc: 100.00% | Val. Loss: 0.488 | Val. Acc:  83.33% |\n",
            "| Epoch: 07 | Train Loss: 0.381 | Train Acc: 100.00% | Val. Loss: 0.463 | Val. Acc: 100.00% |\n",
            "| Epoch: 08 | Train Loss: 0.360 | Train Acc: 100.00% | Val. Loss: 0.453 | Val. Acc:  83.33% |\n",
            "| Epoch: 09 | Train Loss: 0.308 | Train Acc: 100.00% | Val. Loss: 0.418 | Val. Acc:  83.33% |\n",
            "| Epoch: 10 | Train Loss: 0.303 | Train Acc: 100.00% | Val. Loss: 0.401 | Val. Acc:  83.33% |\n",
            "Test Loss: 0.605 | Test Acc: 66.67%\n",
            "     Recall: 0.67, Precision: 0.67, F-measure: 0.67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5si3n_wf-ImU"
      },
      "source": [
        " **Q: Is the performance of CNN different from the performance of FFNN? Output predictions.**\n",
        " \n",
        " *A: CNN typically performs better on larger datasets. Here, although our toy dataset is for demonstrational purposes, CNN performs better than the FFNN as well.*\n",
        "\n",
        "**Q: Is padding necessary for CNN inputs? What is the role of the window size?**\n",
        "\n",
        "*A: For CNNs, padding is only necessary for the case when an input sentence size is smaller than the longest window size. The window size determines the receptive field (i.e. span) of the convolution operation.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4VCX4XSqvlu"
      },
      "source": [
        "## Initializing CNN with pre-trained representations\n",
        "\n",
        "The work [Convolutional Neural Networks for Sentence Classification (Kim, 2014)](https://arxiv.org/abs/1408.5882) also investigates the exploitation of pre-trained embeddings and demonstrates the efficiency of using them.\n",
        "\n",
        "First, download the embeddings and unzip them below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgvTCi68lOGn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb288204-ae92-437c-8cc4-1d9f3f1662b2"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-03 15:11:17--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-02-03 15:11:17--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-02-03 15:11:17--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1      100%[===================>] 822.24M  5.09MB/s    in 2m 40s  \n",
            "\n",
            "2022-02-03 15:13:58 (5.13 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kq8ou3Y0CSQh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f239fe79-b163-45d1-d10c-136478790b3d"
      },
      "source": [
        "# Unzip the file: 4 different embedding sizes are provided\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afclMKCYLYT9"
      },
      "source": [
        "# Check the file format\n",
        "!head -n10 glove.6B.50d.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMwxUcgDHzGu"
      },
      "source": [
        "\n",
        "Try and initialize the CNN embedding layer with the `50D` pre-trained GloVe embeddings. Pay particular attention to keeping the correct indices from the `word2idx` for the lookup table! Once you fill the below `wvecs` matrix, copy the previous training loop and initialize its embedding layer with the pre-trained ones as follows:\n",
        "\n",
        "```python\n",
        "model.embedding.weight.data.copy_(torch.from_numpy(wvecs))\n",
        "```\n",
        "\n",
        "**Note:** The learning rate is initially set to 0.5.\n",
        "\n",
        "---\n",
        "\n",
        "**Q: What should the embedding for the padding token `<pad>` be?**\n",
        " \n",
        " *A: It should be a constant. We choose a vector with zero values.*\n",
        " \n",
        " **Q: What is the impact of using those pre-trained embeddings on the model performance?**\n",
        " \n",
        " *A: The model typically performs better.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCsV8mtBg4-Y"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "EMBEDDING_DIM = 50\n",
        "\n",
        "# Yet another hyperparameter: since the pre-trained embeddings are coming\n",
        "# from a different network, their magnitudes could differ from the parameters\n",
        "# of this network. So scaling may be necessary.\n",
        "SCALE_EMBS = 0.65\n",
        "\n",
        "# Creates the empty numpy array that you should fill below\n",
        "wvecs = np.zeros((len(word2idx), EMBEDDING_DIM), dtype='float32')\n",
        "\n",
        "#####################################################################\n",
        "# Q: Read line by line, find the corresponding word and\n",
        "# insert its embedding to the correct position in the `wvecs` matrix.\n",
        "# Once done, apply the SCALE_EMBS factor to scale the vectors\n",
        "#####################################################################\n",
        "with open(f'glove.6B.{EMBEDDING_DIM}d.txt', 'r') as f:\n",
        "  for line in tqdm(f):\n",
        "    if len(line.strip().split()) > 3:\n",
        "      word = line.strip().split()[0]\n",
        "      if word in word2idx:\n",
        "        (word, vec) = (word, list(map(float, line.strip().split()[1:])))\n",
        "        idx = word2idx[word]\n",
        "        wvecs[idx] = vec\n",
        "\n",
        "wvecs = wvecs * SCALE_EMBS\n",
        "\n",
        "print()          \n",
        "print(wvecs)\n",
        "\n",
        "#####################\n",
        "# Re-create the model\n",
        "#####################\n",
        "fix_seed()\n",
        "\n",
        "EPOCHS = 10\n",
        "LRATE = 0.5\n",
        "\n",
        "# the hyperparameters specific to CNN\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "# we define the number of filters\n",
        "N_OUT_CHANNELS = 100\n",
        "\n",
        "# we define the window size\n",
        "WINDOW_SIZE = 1\n",
        "\n",
        "# we apply the dropout with the probability 0.1\n",
        "DROPOUT = 0.1\n",
        "\n",
        "# Construct the model\n",
        "model = CNN(len(word2idx), EMBEDDING_DIM, N_OUT_CHANNELS, WINDOW_SIZE, OUTPUT_DIM, DROPOUT)\n",
        "\n",
        "#################################################################\n",
        "### Q: Initialize the embeddings with the loaded pre-trained ones\n",
        "#################################################################\n",
        "model.embedding.weight.data.copy_(torch.from_numpy(wvecs))\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=LRATE)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "feature_train = train_sent_tensor\n",
        "target_train = train_label_tensor\n",
        "\n",
        "feature_valid = valid_sent_tensor\n",
        "target_valid = valid_label_tensor\n",
        "\n",
        "feature_test = test_sent_tensor\n",
        "target_test = test_label_tensor\n",
        "\n",
        "################\n",
        "# Start training\n",
        "################\n",
        "print(f'Will train for {EPOCHS} epochs')\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  model.train()\n",
        "  \n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  predictions = model(feature_train).squeeze(1)\n",
        "  loss = loss_fn(predictions, target_train)\n",
        "  train_loss = loss.item()\n",
        "  train_acc = accuracy(predictions, target_train)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    predictions_valid = model(feature_valid).squeeze(1)\n",
        "    valid_loss = loss_fn(predictions_valid, target_valid).item()\n",
        "    valid_acc = accuracy(predictions_valid, target_valid)\n",
        "  \n",
        "  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:6.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:6.2f}% |')\n",
        "\n",
        "\n",
        "## Finally, test on the test set\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    predictions = model(feature_test).squeeze(1)\n",
        "    loss = loss_fn(predictions, target_test)\n",
        "    acc = accuracy(predictions, target_test)\n",
        "    print(f'Test Loss: {loss:.3f} | Test Acc: {acc*100:.2f}%')\n",
        "    f_measure(predictions, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKsR86iBOkgP"
      },
      "source": [
        "# Advanced: Experimenting with larger corpora\n",
        "\n",
        "For advanced experiments with a larger dataset, we suggest to use the [IMBD dataset](http://ai.stanford.edu/~amaas/data/sentiment/) of movie reviews available from [`torchtext.datasets`](https://torchtext.readthedocs.io/en/latest/data.html). This module also provides a range of useful functionalities for data preparation: defining a preprocessing pipeline, splitting, batching, padding, iterating through data, loading pre-trained embeddings, building vocabulary, etc. Below we provide an example using the tokenizer as provided by the [spaCy](https://spacy.io) toolkit.\n",
        "\n",
        "With the batch size provided, `BucketIterator` defines mini-batches by grouping sequences with similar original lengths, so that there is minimal need for padding.  For this bigger dataset, use `.cuda()` on any input batches/tensors, network modules and loss functions to place computations on the GPU. When working on **Google Colab**, make sure that you changed your runtime to GPU from the above menu.\n",
        "\n",
        "You can start by applying the provided CNN model to this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71Dq7amIOnUP"
      },
      "source": [
        "from torchtext.legacy import data, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import spacy\n",
        "\n",
        "# Fix GPU seeds\n",
        "SEED = 9320\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  DEVICE='cuda:0'\n",
        "else:\n",
        "  DEVICE='cpu'\n",
        "\n",
        "print('Device is', DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY11cw5-PJ2a",
        "outputId": "b2e622ca-1bb9-4627-ec2b-0d8334b4315a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# NOTE: Execution of this cell takes a couple of minutes\n",
        "##\n",
        "\n",
        "# define types of data and their preprocessing\n",
        "text_field = data.Field(tokenize='spacy', lower=True)\n",
        "label_field = data.LabelField(dtype=torch.float)\n",
        "\n",
        "# get pre-defined split\n",
        "train, test_init = datasets.IMDB.splits(text_field, label_field)\n",
        "\n",
        "# define our own validation and test set (initial test set is too large)\n",
        "train, valid_test = train.split(split_ratio=0.9, random_state=random.seed(SEED))\n",
        "valid, test = valid_test.split(split_ratio=0.5, random_state=random.seed(SEED))\n",
        "\n",
        "print(f'Train size: {len(train)}')\n",
        "print(f'Validation size: {len(valid)}')\n",
        "print(f'Test size: {len(test)}')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84.1M/84.1M [00:03<00:00, 22.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 22500\n",
            "Validation size: 1250\n",
            "Test size: 1250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udpDm9lsQi2I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b98c130b-7ad3-4e69-fe8c-9958bd9a4bac"
      },
      "source": [
        "# build vocabulary with maximum size (less frequent words are not considered)\n",
        "# load the pre-trained word embeddings.\n",
        "EMBEDDING_DIM = 50\n",
        "\n",
        "text_field.build_vocab(train, max_size=25000, vectors=f\"glove.6B.{EMBEDDING_DIM}d\")\n",
        "label_field.build_vocab(train)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 399999/400000 [00:10<00:00, 39726.99it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYWVMjoGO9jE"
      },
      "source": [
        "# get iterators over the data\n",
        "# place iterators on the GPU if possible\n",
        "\n",
        "# define our batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "  (train, valid, test),\n",
        "  batch_sizes=(BATCH_SIZE, BATCH_SIZE, BATCH_SIZE), device=DEVICE)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1Gr756JtRcC"
      },
      "source": [
        "def eval_data(data_iter, model, loss_fn):\n",
        "  model.eval()\n",
        "  loss = 0\n",
        "  acc = 0\n",
        "  denom = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in data_iter:\n",
        "      # place on the GPU          \n",
        "      feature, target = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
        "      predictions = model(feature.t()).squeeze(1)\n",
        "      \n",
        "      _loss = loss_fn(predictions, target)\n",
        "      loss += (_loss.item() * predictions.shape[0])\n",
        "      acc += (accuracy(predictions, target) * predictions.shape[0])\n",
        "      denom += predictions.shape[0]\n",
        "\n",
        "  model.train()\n",
        "  return loss / denom, acc / denom"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP-EZueBPo1G"
      },
      "source": [
        "def train_model(train_iter, dev_iter, model, loss_fn, n_epochs):\n",
        "  for epoch in range(1, n_epochs + 1): \n",
        "    print(f'Starting epoch {epoch}')\n",
        "    train_loss = 0\n",
        "    train_loss_denom = 0\n",
        "    train_acc = 0\n",
        "    model.train()\n",
        "    \n",
        "    # iterate over batches\n",
        "    for batch in train_iter:\n",
        "        # place on the GPU          \n",
        "        feature, target = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(feature.t()).squeeze(1)\n",
        "          \n",
        "        loss = loss_fn(predictions, target)\n",
        "        acc = accuracy(predictions, target)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += (loss.item() * predictions.shape[0])\n",
        "        train_loss_denom += predictions.shape[0]\n",
        "        train_acc += (acc * predictions.shape[0])\n",
        "        \n",
        "    valid_loss, valid_acc = eval_data(dev_iter, model, loss_fn)\n",
        "\n",
        "    # Normalize everything\n",
        "    train_loss /= train_loss_denom\n",
        "    train_acc /= train_loss_denom\n",
        "\n",
        "    print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:6.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:6.2f}%')\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_g02vvYMSVZp",
        "outputId": "81b4ae96-04ae-4d71-d6ad-9cfd3fceccac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "N_OUT_CHANNELS = 100\n",
        "LRATE = 0.5\n",
        "DROPOUT = 0.4\n",
        "WINDOW_SIZE = 1\n",
        "\n",
        "# Construct the model\n",
        "model = CNN(len(text_field.vocab), EMBEDDING_DIM, N_OUT_CHANNELS, WINDOW_SIZE, OUTPUT_DIM, DROPOUT)\n",
        "print(model)\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=LRATE)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Start training\n",
        "train_model(train_iterator, valid_iterator, model, loss_fn, n_epochs=10)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN(\n",
            "  (embedding): Embedding(25002, 50, padding_idx=0)\n",
            "  (conv): Conv2d(1, 100, kernel_size=(1, 50), stride=(1, 1))\n",
            "  (dropout): Dropout(p=0.4, inplace=False)\n",
            "  (fc): Linear(in_features=100, out_features=1, bias=True)\n",
            ")\n",
            "Starting epoch 1\n",
            "| Epoch: 01 | Train Loss: 0.749 | Train Acc:  61.50% | Val. Loss: 0.573 | Val. Acc:  69.12%\n",
            "Starting epoch 2\n",
            "| Epoch: 02 | Train Loss: 0.563 | Train Acc:  71.86% | Val. Loss: 0.489 | Val. Acc:  76.72%\n",
            "Starting epoch 3\n",
            "| Epoch: 03 | Train Loss: 0.513 | Train Acc:  75.33% | Val. Loss: 0.419 | Val. Acc:  81.60%\n",
            "Starting epoch 4\n",
            "| Epoch: 04 | Train Loss: 0.486 | Train Acc:  77.33% | Val. Loss: 0.441 | Val. Acc:  79.36%\n",
            "Starting epoch 5\n",
            "| Epoch: 05 | Train Loss: 0.463 | Train Acc:  78.53% | Val. Loss: 0.391 | Val. Acc:  82.16%\n",
            "Starting epoch 6\n",
            "| Epoch: 06 | Train Loss: 0.452 | Train Acc:  79.32% | Val. Loss: 0.365 | Val. Acc:  82.72%\n",
            "Starting epoch 7\n",
            "| Epoch: 07 | Train Loss: 0.442 | Train Acc:  79.67% | Val. Loss: 0.380 | Val. Acc:  83.44%\n",
            "Starting epoch 8\n",
            "| Epoch: 08 | Train Loss: 0.425 | Train Acc:  80.92% | Val. Loss: 0.404 | Val. Acc:  82.00%\n",
            "Starting epoch 9\n",
            "| Epoch: 09 | Train Loss: 0.424 | Train Acc:  81.06% | Val. Loss: 0.391 | Val. Acc:  81.92%\n",
            "Starting epoch 10\n",
            "| Epoch: 10 | Train Loss: 0.412 | Train Acc:  81.43% | Val. Loss: 0.365 | Val. Acc:  84.64%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0DSqbKBRCKy"
      },
      "source": [
        "**Q: The paper Convolutional Neural Networks for Sentence Classification (Kim, 2014) applies 3 convolutional layers in parallel with window sizes [3, 4, 5]. Try to extend our CNN model with 2 more convolution layers and apply these window sizes. Outputs of the pooling layers are concatenated. What will be the effect on the model performance?**\n",
        "\n",
        "**Hint:** you can use the `nn.ModuleList function`.\n",
        "\n",
        "**Q: Pre-processing: experiment with filtering out stop words from input data. What will be the effect on the performance? You may choose to use spaCy to get a list of stop words. Here's an example:**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQpqpHFKRYOD",
        "outputId": "6717149a-3763-4c0f-ac32-5a92ae0769b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Example\n",
        "spacy_nlp = spacy.load('en_core_web_sm')\n",
        "spacy_stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "print(spacy_stop_words)\n",
        "text_field = data.Field(tokenize='spacy', lower=True, stop_words=spacy_stop_words)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'nor', 'third', 'mine', 'yourself', 'is', 'beside', 'put', 'meanwhile', 'except', 'call', 'within', 'nothing', 'much', 'have', 'whenever', 'ever', 'thereafter', 'cannot', '‘s', 'any', 'already', 'her', 'some', 'whole', 'however', 'those', 'together', 'and', 'thru', 'by', 'himself', 'my', 'ten', 'wherever', 'such', 'whereby', 'been', 'themselves', 'who', 'hereby', 'we', 'me', 'using', 'everything', 'none', 'up', 'hence', 'were', 'may', 'will', 'through', 'without', 'rather', 'per', 'neither', 'your', 'amongst', 'perhaps', 'until', 'they', 'how', 'do', 'becomes', 'own', 'same', '’ve', 'yet', 'other', \"'ve\", '’re', 'seeming', 'these', 'onto', 'if', 'side', 'done', 'would', 'around', 'please', 'yours', 'to', 'alone', 'among', 'often', 'always', 'whereas', 'his', 'nobody', 'several', 'because', 'down', 'next', 'not', \"'ll\", 'behind', 'after', 'hundred', '‘ve', 'forty', 'make', 'give', 'most', 'be', 'then', 'back', 'latterly', 'every', 'somewhere', 'seemed', 'against', 'off', 'herein', 'ourselves', 'see', 'say', 'namely', 'everywhere', 'our', 'was', 'what', 'sometime', 'an', 'others', 'become', 'more', 'ours', 'has', 'at', 'though', 'beyond', 'thereupon', 'for', 'nevertheless', 'of', 'their', 'where', 'name', '’d', 'first', 'else', 'last', 'since', 'serious', 'the', 'three', 'part', 'used', 'on', 'but', 'so', 'hereupon', 'hers', \"'d\", 'itself', '’ll', 'someone', 'over', 'here', 'than', 'us', 'quite', 'she', 'throughout', 'sometimes', 'one', 'again', 'during', 'nowhere', 'elsewhere', 'another', 'anyway', 'under', 'also', 'moreover', 'fifteen', 'somehow', 'which', 'really', 'as', 'does', 'whence', 'keep', 'many', '‘re', 'besides', 'go', 'well', 'had', 'wherein', 'something', 'all', 'from', 'mostly', 'least', 'top', 'thence', 'yourselves', 'each', \"'m\", 'whether', 'between', 'six', 'once', 'get', 'seems', 'noone', 'n’t', 'him', 'across', 'either', 'while', 'should', 'very', 'anywhere', 'four', 'although', 'before', 'upon', 'full', 'everyone', 'sixty', 'therefore', 'take', 'could', 'fifty', 'via', 'latter', 'seem', 'otherwise', 're', 'below', 'beforehand', \"'re\", 'due', 'toward', 'above', 'formerly', 'towards', 'herself', 'this', 'less', 'whoever', 'anyhow', 'thereby', \"n't\", 'i', 'eight', 'eleven', '‘m', 'bottom', 'move', 'whatever', 'them', '’s', 'out', 'with', 'n‘t', 'never', '’m', 'became', 'no', 'whereupon', 'did', 'a', 'two', 'show', 'in', 'whereafter', 'indeed', \"'s\", 'former', 'few', 'myself', 'front', 'both', 'whom', 'about', 'amount', 'becoming', 'into', 'why', 'anything', 'regarding', 'only', 'thus', 'that', 'now', 'still', 'therein', 'nine', 'its', 'when', 'too', 'you', 'made', 'there', 'whose', 'or', 'it', 'he', 'five', 'being', 'just', 'various', '‘ll', 'afterwards', 'doing', 'twelve', 'must', '‘d', 'can', 'enough', 'am', 'twenty', 'along', 'anyone', 'whither', 'unless', 'empty', 'almost', 'even', 'further', 'ca', 'hereafter', 'might', 'are'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ely0azrRsW5"
      },
      "source": [
        "**Q: Apply a Naive Bayes classifier to the problem. How would it perform for this task? You can use the `sklearn.naive_bayes.MultinomialNB` implementation from the popular `scikit-learn` toolkit. Extraction of the data for this purpose could be performed as follows:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ1EoMnuXgCK"
      },
      "source": [
        "for example in train:\n",
        "  if example.label == 'pos':\n",
        "      label = 1\n",
        "  else:\n",
        "      label = 0"
      ],
      "execution_count": 29,
      "outputs": []
    }
  ]
}