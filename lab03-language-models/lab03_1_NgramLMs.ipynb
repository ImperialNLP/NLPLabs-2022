{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbsta7lIHhz"
      },
      "source": [
        "# Download WikiText-2 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.265129Z",
          "start_time": "2020-01-29T18:30:17.460952Z"
        },
        "id": "jfGVtATnIHh2"
      },
      "outputs": [],
      "source": [
        "! wget https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/train.txt\n",
        "! wget https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/valid.txt\n",
        "! wget https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/test.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW7UOlBNIHh5"
      },
      "source": [
        "# Helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.271133Z",
          "start_time": "2020-01-29T18:30:18.266713Z"
        },
        "id": "u5edQma8IHh7"
      },
      "outputs": [],
      "source": [
        "from collections import Counter, defaultdict\n",
        "import math\n",
        "import copy\n",
        "import random\n",
        "import operator\n",
        "\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "\n",
        "# some helper functions\n",
        "def prepare_data(filename):\n",
        "    data = [l.strip().lower().split() + ['</s>'] for l in open(filename) if l.strip()]\n",
        "    corpus = flatten(data)\n",
        "    vocab = set(corpus)\n",
        "    return vocab, data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvjT2fTlIHh9"
      },
      "source": [
        "# N-Gram LM\n",
        "A language model assigns a probability to each possible next word given a history of previous words (context). \n",
        "\n",
        "$P(w_t|w_{t-1}, w_{t-2}, ... , w_1)$\n",
        "\n",
        "### Markov Assumption\n",
        "Since calculating the probability of the whole sentence is not feasible, the Markov Assumption is introduced.\n",
        "\n",
        "It assumes that each next word only depends on the previous K words (in an N-Gram language model, K = N-1).\n",
        "- Unigram: $P(w_t|w_{t-1}, w_{t-2}, ... , w_1) = P(w_t)$\n",
        "- Bigram: $P(w_t|w_{t-1}, w_{t-2}, ... , w_1) = P(w_t|w_{t-1}) $\n",
        "- Trigram: $P(w_t|w_{t-1}, w_{t-2}, ... , w_1) = P(w_t|w_{t-1}, w_{t-2})$\n",
        "\n",
        "For an N-Gram language model, the probability is calculated by counting the frequency:\n",
        "\n",
        "$P(w_t|w_{t-1}, w_{t-2}, ... ,w_{t-N+1}) = \\frac{C(w_t, w_{t-1}, w_{t-2}, ... ,w_{t-N+1} )}{C(w_{t-1}, w_{t-2}, ... ,w_{t-N+1})}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.290242Z",
          "start_time": "2020-01-29T18:30:18.272506Z"
        },
        "id": "Ql01Z8IiIHh_"
      },
      "outputs": [],
      "source": [
        "class NGramLM():\n",
        "    def __init__(self, N):\n",
        "        self.N = N\n",
        "        self.vocab = set()\n",
        "        self.data = []\n",
        "        self.prob = {}\n",
        "        self.counts = defaultdict(Counter)\n",
        "    \n",
        "    # For N = 1, the probability is stored in a dict   P = prob[next_word]\n",
        "    # For N > 1, the probability is in a nested dict   P = prob[context][next_word]\n",
        "    def train(self, vocab, data, smoothing_k=0):\n",
        "        self.vocab = vocab\n",
        "        self.data = data\n",
        "        self.smoothing_k = smoothing_k\n",
        "\n",
        "        if self.N == 1:\n",
        "            self.counts = Counter(flatten(data))\n",
        "            self.prob = self.get_prob(self.counts)\n",
        "        else:\n",
        "            self.vocab.add('<s>')\n",
        "            counts = self.count_ngram()\n",
        "            \n",
        "            self.prob = {}\n",
        "            for context, counter in counts.items():\n",
        "                self.prob[context] = self.get_prob(counter)\n",
        "    \n",
        "    def count_ngram(self):\n",
        "        counts = defaultdict(Counter)\n",
        "        for sentence in self.data:\n",
        "            sentence = (self.N - 1) * ['<s>'] + sentence \n",
        "            for i in range(len(sentence)-self.N+1):\n",
        "                context = sentence[i:i+self.N-1]\n",
        "                context = \" \".join(context)\n",
        "                word = sentence[i+self.N-1]\n",
        "                counts[context][word] += 1\n",
        "\n",
        "        self.counts = counts\n",
        "        return counts\n",
        "        \n",
        "    # normalize counts into probability(considering smoothing)\n",
        "    def get_prob(self, counter):\n",
        "        total = float(sum(counter.values()))\n",
        "        k = self.smoothing_k\n",
        "        \n",
        "        prob = {}\n",
        "        for word, count in counter.items():\n",
        "            prob[word] = (count + k) / (total + len(self.vocab) * k)\n",
        "        return prob\n",
        "        \n",
        "    def get_ngram_logprob(self, word, seq_len, context=\"\"):\n",
        "        if self.N == 1 and word in self.prob.keys():\n",
        "            return math.log(self.prob[word]) / seq_len\n",
        "        elif self.N > 1 and not self._is_unseen_ngram(context, word):\n",
        "            return math.log(self.prob[context][word]) / seq_len\n",
        "        else:\n",
        "            # assign a small probability to the unseen ngram\n",
        "            # to avoid log of zero and to penalise unseen word or context\n",
        "            return math.log(1/len(self.vocab)) / seq_len\n",
        "        \n",
        "    def get_ngram_prob(self, word, context=\"\"):\n",
        "        if self.N == 1 and word in self.prob.keys():\n",
        "            return self.prob[word]\n",
        "        elif self.N > 1 and not self._is_unseen_ngram(context, word):\n",
        "            return self.prob[context][word]\n",
        "        elif word in self.vocab and self.smoothing_k > 0:\n",
        "            # probability assigned by smoothing\n",
        "            return self.smoothing_k / (sum(self.counts[context].values()) + self.smoothing_k*len(self.vocab))\n",
        "        else:\n",
        "            # unseen word or context\n",
        "            return 0\n",
        "            \n",
        "    # In this method, the perplexity is measured at the sentence-level, averaging over all sentences.\n",
        "    # Actually, it is also possible to calculate perplexity by merging all sentences into a long one.\n",
        "    def perplexity(self, test_data):\n",
        "        log_ppl = 0\n",
        "        if self.N == 1:\n",
        "            for sentence in test_data:\n",
        "                for word in sentence:\n",
        "                    log_ppl += self.get_ngram_logprob(word=word, seq_len=len(sentence))\n",
        "        else:\n",
        "            for sentence in test_data:\n",
        "                for i in range(len(sentence)-self.N+1):\n",
        "                    context = sentence[i:i+self.N-1]\n",
        "                    context = \" \".join(context)\n",
        "                    word = sentence[i+self.N-1]\n",
        "                    log_ppl += self.get_ngram_logprob(context=context, word=word, seq_len=len(sentence))\n",
        "                        \n",
        "        log_ppl /= len(test_data)\n",
        "        ppl = math.exp(-log_ppl)\n",
        "        return ppl\n",
        "    \n",
        "    def _is_unseen_ngram(self, context, word):\n",
        "        if context not in self.prob.keys() or word not in self.prob[context].keys():\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    \n",
        "    # generate the most probable k words\n",
        "    def generate_next(self, context, k):\n",
        "        context = (self.N-1) * '<s> ' + context\n",
        "        context = context.split()\n",
        "        ngram_context_list = context[-self.N+1:]\n",
        "        ngram_context = \" \".join(ngram_context_list)\n",
        "        \n",
        "        if ngram_context in self.prob.keys():\n",
        "            candidates = self.prob[ngram_context]\n",
        "            most_probable_words = sorted(candidates.items(), key=lambda kv: kv[1], reverse=True)\n",
        "            for i in range(min(k, len(most_probable_words))):\n",
        "                print(\" \".join(context[self.N-1:])+\" \"+most_probable_words[i][0]+\"\\t P={}\".format(most_probable_words[i][1]))\n",
        "        else:\n",
        "            print(\"Unseen context!\")\n",
        "            \n",
        "    # generate the next n words with greedy search\n",
        "    def generate_next_n(self, context, n):\n",
        "        context = (self.N-1) * '<s> ' + context\n",
        "        context = context.split()\n",
        "        ngram_context_list = context[-self.N+1:]\n",
        "        ngram_context = \" \".join(ngram_context_list)\n",
        "        \n",
        "        for i in range(n):\n",
        "            try:\n",
        "                candidates = self.prob[ngram_context]\n",
        "                most_likely_next = max(candidates.items(), key=operator.itemgetter(1))[0]\n",
        "                context += [most_likely_next]\n",
        "                ngram_context_list = ngram_context_list[1:] + [most_likely_next]\n",
        "                ngram_context = \" \".join(ngram_context_list)\n",
        "            except:\n",
        "                break\n",
        "        print(\" \".join(context[self.N-1:]))\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VwN9CXyIHiC"
      },
      "source": [
        "# Train with toy dataset\n",
        "\n",
        "At this step, let's train a Bigram language model on the toy dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.297355Z",
          "start_time": "2020-01-29T18:30:18.291387Z"
        },
        "id": "sabDGGntIHiC"
      },
      "outputs": [],
      "source": [
        "corpus = [\"I like ice cream\",\n",
        "         \"I like chocolate\",\n",
        "         \"I hate beans\"]\n",
        "data = [l.strip().lower().split() + ['</s>'] for l in corpus if l.strip()]\n",
        "vocab = set(flatten(data))\n",
        "print(data)\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.301895Z",
          "start_time": "2020-01-29T18:30:18.298317Z"
        },
        "id": "-HKqP6zvIHiE"
      },
      "outputs": [],
      "source": [
        "def print_probability(lm):\n",
        "    for context in lm.vocab:\n",
        "        for word in lm.vocab:\n",
        "            prob = lm.get_ngram_prob(word, context)\n",
        "            print(\"P({}\\t|{}) = {}\".format(word, context, prob))\n",
        "        print(\"--------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y-jHF6bIHiF"
      },
      "source": [
        "# Smoothing\n",
        "Smoothing method is used to deal with the sparsity problem in N-Gram language modelling.\n",
        "The probability mass is shifted towards the less frequent words.\n",
        "\n",
        "For example, with an add-1 smoothing, the probability is calculated as:\n",
        "\n",
        "$$P(w_t | context) = \\frac{C(w_t, context)+1}{C(context)+|V|}$$\n",
        "\n",
        "Q1: What is the disadvantage of smoothing?\n",
        "\n",
        "A: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.310269Z",
          "start_time": "2020-01-29T18:30:18.302761Z"
        },
        "id": "nBS-MIWiIHiG"
      },
      "outputs": [],
      "source": [
        "lm = NGramLM(2)\n",
        "lm.train(vocab, data, smoothing_k=0)\n",
        "\n",
        "######################################################\n",
        "# Q2: try with add-1 smoothing and see the probability\n",
        "######################################################\n",
        "# lm.train(vocab, data, smoothing_k=1)\n",
        "\n",
        "print_probability(lm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG35kkhVIHiH"
      },
      "source": [
        "# Train on WikiText-2 dataset and evaluate perplexity\n",
        "### Evaluating perplexity\n",
        "\n",
        "Q3: Why do we need to calculate log perplexity?\n",
        "\n",
        "A:\n",
        "\n",
        "$ PPL(W) = P(w_1, w_2, ... , w_n)^{-\\frac{1}{n}}$\n",
        "\n",
        "$ log(PPL(W)) = -\\frac{1}{n}\\sum^n_{k=1}log(P(w_k|w_1, w_2, ... , w_{k-1}))$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.712903Z",
          "start_time": "2020-01-29T18:30:18.312677Z"
        },
        "id": "sWJEjssXIHiI"
      },
      "outputs": [],
      "source": [
        "vocab, train_data = prepare_data('train.txt')\n",
        "_, valid_data = prepare_data('valid.txt')\n",
        "_, test_data = prepare_data('test.txt')\n",
        "print(len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:22.224402Z",
          "start_time": "2020-01-29T18:30:18.714081Z"
        },
        "id": "rYcGOj6cIHiI"
      },
      "outputs": [],
      "source": [
        "lm = NGramLM(3)\n",
        "lm.train(vocab, train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:22.811004Z",
          "start_time": "2020-01-29T18:30:22.225566Z"
        },
        "id": "Qh0NmO16IHiI"
      },
      "outputs": [],
      "source": [
        "print(lm.perplexity(valid_data))\n",
        "print(lm.perplexity(test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw9EMl6XIHiK"
      },
      "source": [
        "# Generating sentences\n",
        "With a pre-trained N-Gram language model, we can predict possible next words given context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:22.815241Z",
          "start_time": "2020-01-29T18:30:22.812141Z"
        },
        "id": "LVE1LyfxIHiL"
      },
      "outputs": [],
      "source": [
        "# generate the most probable following words given the context\n",
        "print(\" \".join(valid_data[12]))\n",
        "\n",
        "# actually the only useful context in the Trigram language model is [\"where\", \"they\"]\n",
        "context = \"the eggs hatch at night , and the larvae swim to the water surface where they\"  \n",
        "lm.generate_next(context, 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can also generate with shorter contexts, even shorter than N-1\n",
        "\n",
        "contexts = [\"the eggs\",\n",
        "            \"the\",\n",
        "            \"\"]\n",
        "for context in contexts:\n",
        "  lm.generate_next(context, 3)\n",
        "  print(\"---\")"
      ],
      "metadata": {
        "id": "e1y3MW9DZ-nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:22.828378Z",
          "start_time": "2020-01-29T18:30:22.816839Z"
        },
        "id": "sIiRSWPoIHiM"
      },
      "outputs": [],
      "source": [
        "context = \"the eggs hatch at night , and the larvae swim to the water surface where they\"  \n",
        "\n",
        "# generate the next n words with greedy search\n",
        "lm.generate_next_n(context, 10)\n",
        "\n",
        "# This is not a good method in practice,\n",
        "# because wrong predictions in the early steps would introduce errors to the following predictions\n",
        "lm.generate_next_n(context, 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSEVmuouIHiN"
      },
      "source": [
        "# Effect of N\n",
        "\n",
        "Q4: Why does the perplexity increase when N is large?\n",
        "\n",
        "A:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:41.930826Z",
          "start_time": "2020-01-29T18:30:22.829892Z"
        },
        "id": "wLVeKE_JIHiN"
      },
      "outputs": [],
      "source": [
        "for n in range(1,6):\n",
        "    lm = NGramLM(n)\n",
        "    lm.train(vocab, train_data)\n",
        "    print(\"************************\")\n",
        "    print(\"{}-gram LM perplexity on valid set: {}\".format(n, lm.perplexity(valid_data)))\n",
        "    print(\"{}-gram LM perplexity on test  set: {}\".format(n, lm.perplexity(test_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgM5mePhIHiO"
      },
      "source": [
        "# Interpolation\n",
        "In interpolation, we mix the probability estimates from all the n-gram estimators to alleviate the sparsity problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:41.937643Z",
          "start_time": "2020-01-29T18:30:41.932047Z"
        },
        "id": "5f57uTZvIHiO"
      },
      "outputs": [],
      "source": [
        "class InterpolateNGramLM(NGramLM):\n",
        "    \n",
        "    def __init__(self, N):\n",
        "        super(InterpolateNGramLM, self).__init__(N)\n",
        "        self.ngram_lms = []\n",
        "        self.lambdas = []\n",
        "        \n",
        "    def train(self, vocab, data, smoothing_k=0, lambdas=[]):\n",
        "        assert len(lambdas) == self.N\n",
        "        assert sum(lambdas) - 1 < 1e-9\n",
        "        self.vocab = vocab\n",
        "        self.lambdas = lambdas\n",
        "        \n",
        "        for i in range(self.N, 0, -1):\n",
        "            lm = NGramLM(i)\n",
        "            print(\"Training {}-gram language model\".format(i))\n",
        "            lm.train(vocab, data, smoothing_k)\n",
        "            self.ngram_lms.append(lm)\n",
        "    \n",
        "    def get_ngram_logprob(self, word, seq_len, context):\n",
        "        prob = 0\n",
        "        for i, (coef, lm) in enumerate(zip(self.lambdas, self.ngram_lms)):\n",
        "            context_words = context.split()\n",
        "            cutted_context = \" \".join(context_words[-self.N + i + 1:])\n",
        "            prob += coef * lm.get_ngram_prob(context=cutted_context, word=word)\n",
        "        return math.log(prob) / seq_len\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:46.507705Z",
          "start_time": "2020-01-29T18:30:41.939076Z"
        },
        "id": "6TafZ7KpIHiO"
      },
      "outputs": [],
      "source": [
        "###################################################\n",
        "# Q5: tune your coefficients to decrease perplexity\n",
        "###################################################\n",
        "ilm = InterpolateNGramLM(3)\n",
        "ilm.train(vocab, train_data, lambdas=[0.5, 0.4, 0.1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:48.376911Z",
          "start_time": "2020-01-29T18:30:46.509150Z"
        },
        "id": "UZGf_gQOIHiP"
      },
      "outputs": [],
      "source": [
        "print(ilm.perplexity(valid_data))\n",
        "print(ilm.perplexity(test_data))"
      ]
    }
  ],
  "metadata": {
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "version": 3,
    "colab": {
      "name": "lab03_1_NgramLMs.ok.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}