{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbsta7lIHhz"
      },
      "source": [
        "# Download WikiText-2 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.265129Z",
          "start_time": "2020-01-29T18:30:17.460952Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfGVtATnIHh2",
        "outputId": "9bb0da5f-75f8-485e-a011-481750050e22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-03 02:47:47--  https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10797148 (10M) [text/plain]\n",
            "Saving to: ‘train.txt.3’\n",
            "\n",
            "train.txt.3         100%[===================>]  10.30M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-02-03 02:47:47 (105 MB/s) - ‘train.txt.3’ saved [10797148/10797148]\n",
            "\n",
            "--2022-02-03 02:47:48--  https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/valid.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1121681 (1.1M) [text/plain]\n",
            "Saving to: ‘valid.txt.3’\n",
            "\n",
            "valid.txt.3         100%[===================>]   1.07M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2022-02-03 02:47:48 (19.9 MB/s) - ‘valid.txt.3’ saved [1121681/1121681]\n",
            "\n",
            "--2022-02-03 02:47:48--  https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1256449 (1.2M) [text/plain]\n",
            "Saving to: ‘test.txt.3’\n",
            "\n",
            "test.txt.3          100%[===================>]   1.20M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2022-02-03 02:47:48 (22.0 MB/s) - ‘test.txt.3’ saved [1256449/1256449]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/train.txt\n",
        "! wget https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/valid.txt\n",
        "! wget https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/test.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW7UOlBNIHh5"
      },
      "source": [
        "# Helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.271133Z",
          "start_time": "2020-01-29T18:30:18.266713Z"
        },
        "id": "u5edQma8IHh7"
      },
      "outputs": [],
      "source": [
        "from collections import Counter, defaultdict\n",
        "import math\n",
        "import copy\n",
        "import random\n",
        "import operator\n",
        "\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "\n",
        "# some helper functions\n",
        "def prepare_data(filename):\n",
        "    data = [l.strip().lower().split() + ['</s>'] for l in open(filename) if l.strip()]\n",
        "    corpus = flatten(data)\n",
        "    vocab = set(corpus)\n",
        "    return vocab, data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvjT2fTlIHh9"
      },
      "source": [
        "# N-Gram LM\n",
        "A language model assigns a probability to each possible next word given a history of previous words (context). \n",
        "\n",
        "$P(w_t|w_{t-1}, w_{t-2}, ... , w_1)$\n",
        "\n",
        "### Markov Assumption\n",
        "Since calculating the probability of the whole sentence is not feasible, the Markov Assumption is introduced.\n",
        "\n",
        "It assumes that each next word only depends on the previous K words (in an N-Gram language model, K = N-1).\n",
        "- Unigram: $P(w_t|w_{t-1}, w_{t-2}, ... , w_1) = P(w_t)$\n",
        "- Bigram: $P(w_t|w_{t-1}, w_{t-2}, ... , w_1) = P(w_t|w_{t-1}) $\n",
        "- Trigram: $P(w_t|w_{t-1}, w_{t-2}, ... , w_1) = P(w_t|w_{t-1}, w_{t-2})$\n",
        "\n",
        "For an N-Gram language model, the probability is calculated by counting the frequency:\n",
        "\n",
        "$P(w_t|w_{t-1}, w_{t-2}, ... ,w_{t-N+1}) = \\frac{C(w_t, w_{t-1}, w_{t-2}, ... ,w_{t-N+1} )}{C(w_{t-1}, w_{t-2}, ... ,w_{t-N+1})}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.290242Z",
          "start_time": "2020-01-29T18:30:18.272506Z"
        },
        "id": "Ql01Z8IiIHh_"
      },
      "outputs": [],
      "source": [
        "class NGramLM():\n",
        "    def __init__(self, N):\n",
        "        self.N = N\n",
        "        self.vocab = set()\n",
        "        self.data = []\n",
        "        self.prob = {}\n",
        "        self.counts = defaultdict(Counter)\n",
        "    \n",
        "    # For N = 1, the probability is stored in a dict   P = prob[next_word]\n",
        "    # For N > 1, the probability is in a nested dict   P = prob[context][next_word]\n",
        "    def train(self, vocab, data, smoothing_k=0):\n",
        "        self.vocab = vocab\n",
        "        self.data = data\n",
        "        self.smoothing_k = smoothing_k\n",
        "\n",
        "        if self.N == 1:\n",
        "            self.counts = Counter(flatten(data))\n",
        "            self.prob = self.get_prob(self.counts)\n",
        "        else:\n",
        "            self.vocab.add('<s>')\n",
        "            counts = self.count_ngram()\n",
        "            \n",
        "            self.prob = {}\n",
        "            for context, counter in counts.items():\n",
        "                self.prob[context] = self.get_prob(counter)\n",
        "    \n",
        "    def count_ngram(self):\n",
        "        counts = defaultdict(Counter)\n",
        "        for sentence in self.data:\n",
        "            sentence = (self.N - 1) * ['<s>'] + sentence \n",
        "            for i in range(len(sentence)-self.N+1):\n",
        "                context = sentence[i:i+self.N-1]\n",
        "                context = \" \".join(context)\n",
        "                word = sentence[i+self.N-1]\n",
        "                counts[context][word] += 1\n",
        "\n",
        "        self.counts = counts\n",
        "        return counts\n",
        "        \n",
        "    # normalize counts into probability(considering smoothing)\n",
        "    def get_prob(self, counter):\n",
        "        total = float(sum(counter.values()))\n",
        "        k = self.smoothing_k\n",
        "        \n",
        "        prob = {}\n",
        "        for word, count in counter.items():\n",
        "            prob[word] = (count + k) / (total + len(self.vocab) * k)\n",
        "        return prob\n",
        "        \n",
        "    def get_ngram_logprob(self, word, seq_len, context=\"\"):\n",
        "        if self.N == 1 and word in self.prob.keys():\n",
        "            return math.log(self.prob[word]) / seq_len\n",
        "        elif self.N > 1 and not self._is_unseen_ngram(context, word):\n",
        "            return math.log(self.prob[context][word]) / seq_len\n",
        "        else:\n",
        "            # assign a small probability to the unseen ngram\n",
        "            # to avoid log of zero and to penalise unseen word or context\n",
        "            return math.log(1/len(self.vocab)) / seq_len\n",
        "        \n",
        "    def get_ngram_prob(self, word, context=\"\"):\n",
        "        if self.N == 1 and word in self.prob.keys():\n",
        "            return self.prob[word]\n",
        "        elif self.N > 1 and not self._is_unseen_ngram(context, word):\n",
        "            return self.prob[context][word]\n",
        "        elif word in self.vocab and self.smoothing_k > 0:\n",
        "            # probability assigned by smoothing\n",
        "            return self.smoothing_k / (sum(self.counts[context].values()) + self.smoothing_k*len(self.vocab))\n",
        "        else:\n",
        "            # unseen word or context\n",
        "            return 0\n",
        "            \n",
        "    # In this method, the perplexity is measured at the sentence-level, averaging over all sentences.\n",
        "    # Actually, it is also possible to calculate perplexity by merging all sentences into a long one.\n",
        "    def perplexity(self, test_data):\n",
        "        log_ppl = 0\n",
        "        if self.N == 1:\n",
        "            for sentence in test_data:\n",
        "                for word in sentence:\n",
        "                    log_ppl += self.get_ngram_logprob(word=word, seq_len=len(sentence))\n",
        "        else:\n",
        "            for sentence in test_data:\n",
        "                for i in range(len(sentence)-self.N+1):\n",
        "                    context = sentence[i:i+self.N-1]\n",
        "                    context = \" \".join(context)\n",
        "                    word = sentence[i+self.N-1]\n",
        "                    log_ppl += self.get_ngram_logprob(context=context, word=word, seq_len=len(sentence))\n",
        "                        \n",
        "        log_ppl /= len(test_data)\n",
        "        ppl = math.exp(-log_ppl)\n",
        "        return ppl\n",
        "    \n",
        "    def _is_unseen_ngram(self, context, word):\n",
        "        if context not in self.prob.keys() or word not in self.prob[context].keys():\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    \n",
        "    # generate the most probable k words\n",
        "    def generate_next(self, context, k):\n",
        "        context = (self.N-1) * '<s> ' + context\n",
        "        context = context.split()\n",
        "        ngram_context_list = context[-self.N+1:]\n",
        "        ngram_context = \" \".join(ngram_context_list)\n",
        "        \n",
        "        if ngram_context in self.prob.keys():\n",
        "            candidates = self.prob[ngram_context]\n",
        "            most_probable_words = sorted(candidates.items(), key=lambda kv: kv[1], reverse=True)\n",
        "            for i in range(min(k, len(most_probable_words))):\n",
        "                print(\" \".join(context[self.N-1:])+\" \"+most_probable_words[i][0]+\"\\t P={}\".format(most_probable_words[i][1]))\n",
        "        else:\n",
        "            print(\"Unseen context!\")\n",
        "            \n",
        "    # generate the next n words with greedy search\n",
        "    def generate_next_n(self, context, n):\n",
        "        context = (self.N-1) * '<s> ' + context\n",
        "        context = context.split()\n",
        "        ngram_context_list = context[-self.N+1:]\n",
        "        ngram_context = \" \".join(ngram_context_list)\n",
        "        \n",
        "        for i in range(n):\n",
        "            try:\n",
        "                candidates = self.prob[ngram_context]\n",
        "                most_likely_next = max(candidates.items(), key=operator.itemgetter(1))[0]\n",
        "                context += [most_likely_next]\n",
        "                ngram_context_list = ngram_context_list[1:] + [most_likely_next]\n",
        "                ngram_context = \" \".join(ngram_context_list)\n",
        "            except:\n",
        "                break\n",
        "        print(\" \".join(context[self.N-1:]))\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VwN9CXyIHiC"
      },
      "source": [
        "# Train with toy dataset\n",
        "\n",
        "At this step, let's train a Bigram language model on the toy dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.297355Z",
          "start_time": "2020-01-29T18:30:18.291387Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sabDGGntIHiC",
        "outputId": "b08e4db4-c8ed-4083-fdd2-c1d9e26bcc6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['i', 'like', 'ice', 'cream', '</s>'], ['i', 'like', 'chocolate', '</s>'], ['i', 'hate', 'beans', '</s>']]\n",
            "{'cream', 'beans', 'like', 'chocolate', 'hate', 'ice', 'i', '</s>'}\n"
          ]
        }
      ],
      "source": [
        "corpus = [\"I like ice cream\",\n",
        "         \"I like chocolate\",\n",
        "         \"I hate beans\"]\n",
        "data = [l.strip().lower().split() + ['</s>'] for l in corpus if l.strip()]\n",
        "vocab = set(flatten(data))\n",
        "print(data)\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.301895Z",
          "start_time": "2020-01-29T18:30:18.298317Z"
        },
        "id": "-HKqP6zvIHiE"
      },
      "outputs": [],
      "source": [
        "def print_probability(lm):\n",
        "    for context in lm.vocab:\n",
        "        for word in lm.vocab:\n",
        "            prob = lm.get_ngram_prob(word, context)\n",
        "            print(\"P({}\\t|{}) = {}\".format(word, context, prob))\n",
        "        print(\"--------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y-jHF6bIHiF"
      },
      "source": [
        "# Smoothing\n",
        "Smoothing method is used to deal with the sparsity problem in N-Gram language modelling.\n",
        "The probability mass is shifted towards the less frequent words.\n",
        "\n",
        "For example, with an add-1 smoothing, the probability is calculated as:\n",
        "\n",
        "$$P(w_t | context) = \\frac{C(w_t, context)+1}{C(context)+|V|}$$\n",
        "\n",
        "Q1: What is the disadvantage of smoothing?\n",
        "\n",
        "A: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.310269Z",
          "start_time": "2020-01-29T18:30:18.302761Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBS-MIWiIHiG",
        "outputId": "9093f13e-ef3e-402c-c167-bea3ebb812a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(cream\t|cream) = 0\n",
            "P(beans\t|cream) = 0\n",
            "P(like\t|cream) = 0\n",
            "P(chocolate\t|cream) = 0\n",
            "P(hate\t|cream) = 0\n",
            "P(<s>\t|cream) = 0\n",
            "P(ice\t|cream) = 0\n",
            "P(i\t|cream) = 0\n",
            "P(</s>\t|cream) = 1.0\n",
            "--------------------------\n",
            "P(cream\t|beans) = 0\n",
            "P(beans\t|beans) = 0\n",
            "P(like\t|beans) = 0\n",
            "P(chocolate\t|beans) = 0\n",
            "P(hate\t|beans) = 0\n",
            "P(<s>\t|beans) = 0\n",
            "P(ice\t|beans) = 0\n",
            "P(i\t|beans) = 0\n",
            "P(</s>\t|beans) = 1.0\n",
            "--------------------------\n",
            "P(cream\t|like) = 0\n",
            "P(beans\t|like) = 0\n",
            "P(like\t|like) = 0\n",
            "P(chocolate\t|like) = 0.5\n",
            "P(hate\t|like) = 0\n",
            "P(<s>\t|like) = 0\n",
            "P(ice\t|like) = 0.5\n",
            "P(i\t|like) = 0\n",
            "P(</s>\t|like) = 0\n",
            "--------------------------\n",
            "P(cream\t|chocolate) = 0\n",
            "P(beans\t|chocolate) = 0\n",
            "P(like\t|chocolate) = 0\n",
            "P(chocolate\t|chocolate) = 0\n",
            "P(hate\t|chocolate) = 0\n",
            "P(<s>\t|chocolate) = 0\n",
            "P(ice\t|chocolate) = 0\n",
            "P(i\t|chocolate) = 0\n",
            "P(</s>\t|chocolate) = 1.0\n",
            "--------------------------\n",
            "P(cream\t|hate) = 0\n",
            "P(beans\t|hate) = 1.0\n",
            "P(like\t|hate) = 0\n",
            "P(chocolate\t|hate) = 0\n",
            "P(hate\t|hate) = 0\n",
            "P(<s>\t|hate) = 0\n",
            "P(ice\t|hate) = 0\n",
            "P(i\t|hate) = 0\n",
            "P(</s>\t|hate) = 0\n",
            "--------------------------\n",
            "P(cream\t|<s>) = 0\n",
            "P(beans\t|<s>) = 0\n",
            "P(like\t|<s>) = 0\n",
            "P(chocolate\t|<s>) = 0\n",
            "P(hate\t|<s>) = 0\n",
            "P(<s>\t|<s>) = 0\n",
            "P(ice\t|<s>) = 0\n",
            "P(i\t|<s>) = 1.0\n",
            "P(</s>\t|<s>) = 0\n",
            "--------------------------\n",
            "P(cream\t|ice) = 1.0\n",
            "P(beans\t|ice) = 0\n",
            "P(like\t|ice) = 0\n",
            "P(chocolate\t|ice) = 0\n",
            "P(hate\t|ice) = 0\n",
            "P(<s>\t|ice) = 0\n",
            "P(ice\t|ice) = 0\n",
            "P(i\t|ice) = 0\n",
            "P(</s>\t|ice) = 0\n",
            "--------------------------\n",
            "P(cream\t|i) = 0\n",
            "P(beans\t|i) = 0\n",
            "P(like\t|i) = 0.6666666666666666\n",
            "P(chocolate\t|i) = 0\n",
            "P(hate\t|i) = 0.3333333333333333\n",
            "P(<s>\t|i) = 0\n",
            "P(ice\t|i) = 0\n",
            "P(i\t|i) = 0\n",
            "P(</s>\t|i) = 0\n",
            "--------------------------\n",
            "P(cream\t|</s>) = 0\n",
            "P(beans\t|</s>) = 0\n",
            "P(like\t|</s>) = 0\n",
            "P(chocolate\t|</s>) = 0\n",
            "P(hate\t|</s>) = 0\n",
            "P(<s>\t|</s>) = 0\n",
            "P(ice\t|</s>) = 0\n",
            "P(i\t|</s>) = 0\n",
            "P(</s>\t|</s>) = 0\n",
            "--------------------------\n"
          ]
        }
      ],
      "source": [
        "lm = NGramLM(2)\n",
        "lm.train(vocab, data, smoothing_k=0)\n",
        "\n",
        "######################################################\n",
        "# Q2: try with add-1 smoothing and see the probability\n",
        "######################################################\n",
        "# lm.train(vocab, data, smoothing_k=1)\n",
        "\n",
        "print_probability(lm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG35kkhVIHiH"
      },
      "source": [
        "# Train on WikiText-2 dataset and evaluate perplexity\n",
        "### Evaluating perplexity\n",
        "\n",
        "Q3: Why do we need to calculate log perplexity?\n",
        "\n",
        "A:\n",
        "\n",
        "$ PPL(W) = P(w_1, w_2, ... , w_n)^{-\\frac{1}{n}}$\n",
        "\n",
        "$ log(PPL(W)) = -\\frac{1}{n}\\sum^n_{k=1}log(P(w_k|w_1, w_2, ... , w_{k-1}))$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.712903Z",
          "start_time": "2020-01-29T18:30:18.312677Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWJEjssXIHiI",
        "outputId": "b7996eb1-2d10-4530-da20-299c05c6dcb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28912\n"
          ]
        }
      ],
      "source": [
        "vocab, train_data = prepare_data('train.txt')\n",
        "_, valid_data = prepare_data('valid.txt')\n",
        "_, test_data = prepare_data('test.txt')\n",
        "print(len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:22.224402Z",
          "start_time": "2020-01-29T18:30:18.714081Z"
        },
        "id": "rYcGOj6cIHiI"
      },
      "outputs": [],
      "source": [
        "lm = NGramLM(3)\n",
        "lm.train(vocab, train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:22.811004Z",
          "start_time": "2020-01-29T18:30:22.225566Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qh0NmO16IHiI",
        "outputId": "e88a2ef5-6496-4c41-e52b-0ea642ac0777"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "387.1190374834364\n",
            "310.6880368161233\n"
          ]
        }
      ],
      "source": [
        "print(lm.perplexity(valid_data))\n",
        "print(lm.perplexity(test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw9EMl6XIHiK"
      },
      "source": [
        "# Generating sentences\n",
        "With a pre-trained N-Gram language model, we can predict possible next words given context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:22.815241Z",
          "start_time": "2020-01-29T18:30:22.812141Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVE1LyfxIHiL",
        "outputId": "79059f50-1f03-457c-d863-b34c6e4ab45e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the eggs hatch at night , and the larvae swim to the water surface where they drift with the ocean currents , preying on <unk> . this stage involves three <unk> and lasts for 15 – 35 days . after the third moult , the juvenile takes on a form closer to the adult , and adopts a <unk> lifestyle . the juveniles are rarely seen in the wild , and are poorly known , although they are known to be capable of digging extensive burrows . it is estimated that only 1 larva in every 20 @,@ 000 survives to the <unk> phase . when they reach a carapace length of 15 mm ( 0 @.@ 59 in ) , the juveniles leave their burrows and start their adult lives . </s>\n",
            "the eggs hatch at night , and the larvae swim to the water surface where they were\t P=0.12408759124087591\n",
            "the eggs hatch at night , and the larvae swim to the water surface where they are\t P=0.08029197080291971\n",
            "the eggs hatch at night , and the larvae swim to the water surface where they would\t P=0.051094890510948905\n"
          ]
        }
      ],
      "source": [
        "# generate the most probable following words given the context\n",
        "print(\" \".join(valid_data[12]))\n",
        "\n",
        "# actually the only useful context in the Trigram language model is [\"where\", \"they\"]\n",
        "context = \"the eggs hatch at night , and the larvae swim to the water surface where they\"  \n",
        "lm.generate_next(context, 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can also generate with shorter contexts, even shorter than N-1\n",
        "\n",
        "contexts = [\"the eggs\",\n",
        "            \"the\",\n",
        "            \"\"]\n",
        "for context in contexts:\n",
        "  lm.generate_next(context, 3)\n",
        "  print(\"---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1y3MW9DZ-nb",
        "outputId": "4bbeb89e-d8fd-4029-b2a1-7832a3fe5f49"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the eggs hatch\t P=0.18181818181818182\n",
            "the eggs of\t P=0.18181818181818182\n",
            "the eggs are\t P=0.13636363636363635\n",
            "---\n",
            "the first\t P=0.03398926654740608\n",
            "the <unk>\t P=0.020274299344066785\n",
            "the episode\t P=0.015802027429934407\n",
            "---\n",
            " =\t P=0.26132873311734756\n",
            " the\t P=0.14112004039214035\n",
            " in\t P=0.06353347077881095\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:22.828378Z",
          "start_time": "2020-01-29T18:30:22.816839Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIiRSWPoIHiM",
        "outputId": "bb234bd9-b24e-4b86-d301-a253385763d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the eggs hatch at night , and the larvae swim to the water surface where they were not able to get the part of the <unk>\n",
            "the eggs hatch at night , and the larvae swim to the water surface where they were not able to get the part of the <unk> of the <unk> of the <unk> of the <unk> of\n"
          ]
        }
      ],
      "source": [
        "context = \"the eggs hatch at night , and the larvae swim to the water surface where they\"  \n",
        "\n",
        "# generate the next n words with greedy search\n",
        "lm.generate_next_n(context, 10)\n",
        "\n",
        "# This is not a good method in practice,\n",
        "# because wrong predictions in the early steps would introduce errors to the following predictions\n",
        "lm.generate_next_n(context, 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSEVmuouIHiN"
      },
      "source": [
        "# Effect of N\n",
        "\n",
        "Q4: Why does the perplexity increase when N is large?\n",
        "\n",
        "A:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:41.930826Z",
          "start_time": "2020-01-29T18:30:22.829892Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLVeKE_JIHiN",
        "outputId": "d6d01a21-6187-4260-e195-9159151b44ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************************\n",
            "1-gram LM perplexity on valid set: 599.6450225274759\n",
            "1-gram LM perplexity on test  set: 553.0467265986567\n",
            "************************\n",
            "2-gram LM perplexity on valid set: 130.75949726991627\n",
            "2-gram LM perplexity on test  set: 114.76592406312065\n",
            "************************\n",
            "3-gram LM perplexity on valid set: 387.1190374834364\n",
            "3-gram LM perplexity on test  set: 310.6880368161233\n",
            "************************\n",
            "4-gram LM perplexity on valid set: 1091.9348578736633\n",
            "4-gram LM perplexity on test  set: 846.7563102382584\n",
            "************************\n",
            "5-gram LM perplexity on valid set: 1558.823545905558\n",
            "5-gram LM perplexity on test  set: 1201.746345143052\n"
          ]
        }
      ],
      "source": [
        "for n in range(1,6):\n",
        "    lm = NGramLM(n)\n",
        "    lm.train(vocab, train_data)\n",
        "    print(\"************************\")\n",
        "    print(\"{}-gram LM perplexity on valid set: {}\".format(n, lm.perplexity(valid_data)))\n",
        "    print(\"{}-gram LM perplexity on test  set: {}\".format(n, lm.perplexity(test_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgM5mePhIHiO"
      },
      "source": [
        "# Interpolation\n",
        "In interpolation, we mix the probability estimates from all the n-gram estimators to alleviate the sparsity problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:41.937643Z",
          "start_time": "2020-01-29T18:30:41.932047Z"
        },
        "id": "5f57uTZvIHiO"
      },
      "outputs": [],
      "source": [
        "class InterpolateNGramLM(NGramLM):\n",
        "    \n",
        "    def __init__(self, N):\n",
        "        super(InterpolateNGramLM, self).__init__(N)\n",
        "        self.ngram_lms = []\n",
        "        self.lambdas = []\n",
        "        \n",
        "    def train(self, vocab, data, smoothing_k=0, lambdas=[]):\n",
        "        assert len(lambdas) == self.N\n",
        "        assert sum(lambdas) - 1 < 1e-9\n",
        "        self.vocab = vocab\n",
        "        self.lambdas = lambdas\n",
        "        \n",
        "        for i in range(self.N, 0, -1):\n",
        "            lm = NGramLM(i)\n",
        "            print(\"Training {}-gram language model\".format(i))\n",
        "            lm.train(vocab, data, smoothing_k)\n",
        "            self.ngram_lms.append(lm)\n",
        "    \n",
        "    def get_ngram_logprob(self, word, seq_len, context):\n",
        "        prob = 0\n",
        "        for i, (coef, lm) in enumerate(zip(self.lambdas, self.ngram_lms)):\n",
        "            context_words = context.split()\n",
        "            cutted_context = \" \".join(context_words[-self.N + i + 1:])\n",
        "            prob += coef * lm.get_ngram_prob(context=cutted_context, word=word)\n",
        "        return math.log(prob) / seq_len\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:46.507705Z",
          "start_time": "2020-01-29T18:30:41.939076Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TafZ7KpIHiO",
        "outputId": "a48d6c8c-f84d-4946-a2bd-6c103727f325"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training 3-gram language model\n",
            "Training 2-gram language model\n",
            "Training 1-gram language model\n"
          ]
        }
      ],
      "source": [
        "###################################################\n",
        "# Q5: tune your coefficients to decrease perplexity\n",
        "###################################################\n",
        "ilm = InterpolateNGramLM(3)\n",
        "ilm.train(vocab, train_data, lambdas=[0.5, 0.4, 0.1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:48.376911Z",
          "start_time": "2020-01-29T18:30:46.509150Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZGf_gQOIHiP",
        "outputId": "fd33af89-2ecf-4994-af02-531359cf1b90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "126.26886448793077\n",
            "103.0773276220259\n"
          ]
        }
      ],
      "source": [
        "print(ilm.perplexity(valid_data))\n",
        "print(ilm.perplexity(test_data))"
      ]
    }
  ],
  "metadata": {
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "version": 3,
    "colab": {
      "name": "lab03_1_NgramLMs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}