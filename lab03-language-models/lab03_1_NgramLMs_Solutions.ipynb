{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5iv46PHbD3Y"
      },
      "source": [
        "# Download WikiText-2 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.265129Z",
          "start_time": "2020-01-29T18:30:17.460952Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekifjjo9bD3c",
        "outputId": "a4008684-8a78-4850-8b37-48f1b363f1b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-03 02:47:58--  https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10797148 (10M) [text/plain]\n",
            "Saving to: ‘train.txt’\n",
            "\n",
            "train.txt           100%[===================>]  10.30M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2022-02-03 02:47:58 (147 MB/s) - ‘train.txt’ saved [10797148/10797148]\n",
            "\n",
            "--2022-02-03 02:47:59--  https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/valid.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1121681 (1.1M) [text/plain]\n",
            "Saving to: ‘valid.txt’\n",
            "\n",
            "valid.txt           100%[===================>]   1.07M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-02-03 02:47:59 (80.3 MB/s) - ‘valid.txt’ saved [1121681/1121681]\n",
            "\n",
            "--2022-02-03 02:47:59--  https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1256449 (1.2M) [text/plain]\n",
            "Saving to: ‘test.txt’\n",
            "\n",
            "test.txt            100%[===================>]   1.20M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-02-03 02:47:59 (84.3 MB/s) - ‘test.txt’ saved [1256449/1256449]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/train.txt\n",
        "! wget https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/valid.txt\n",
        "! wget https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/test.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDTTxkl9bD3g"
      },
      "source": [
        "# Helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.271133Z",
          "start_time": "2020-01-29T18:30:18.266713Z"
        },
        "id": "tlLaWSlbbD3i"
      },
      "outputs": [],
      "source": [
        "from collections import Counter, defaultdict\n",
        "import math\n",
        "import copy\n",
        "import random\n",
        "import operator\n",
        "\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "\n",
        "# some helper functions\n",
        "def prepare_data(filename):\n",
        "    data = [l.strip().lower().split() + ['</s>'] for l in open(filename) if l.strip()]\n",
        "    corpus = flatten(data)\n",
        "    vocab = set(corpus)\n",
        "    return vocab, data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uSNl03XbD3l"
      },
      "source": [
        "# N-Gram LM\n",
        "A language model assign a probability to each possible next word given a history of previous words (context). \n",
        "\n",
        "$P(w_t|w_{t-1}, w_{t-2}, ... , w_1)$\n",
        "\n",
        "### Markov Assumption\n",
        "Since calculating the probability of the whole sentence is not feasible, the Markov Assumption is introduced.\n",
        "\n",
        "It assumes that each next word only depends on the previous K words (In an N-Gram language model, K = N-1).\n",
        "- Unigram: $P(w_t|w_{t-1}, w_{t-2}, ... , w_1) = P(w_t)$\n",
        "- Bigram: $P(w_t|w_{t-1}, w_{t-2}, ... , w_1) = P(w_t|w_{t-1}) $\n",
        "- Trigram: $P(w_t|w_{t-1}, w_{t-2}, ... , w_1) = P(w_t|w_{t-1}, w_{t-2})$\n",
        "\n",
        "For an N-Gram language model, the probability is calculated by counting the frequency:\n",
        "\n",
        "$P(w_t|w_{t-1}, w_{t-2}, ... ,w_{t-N+1}) = \\frac{C(w_t, w_{t-1}, w_{t-2}, ... ,w_{t-N+1} )}{C(w_{t-1}, w_{t-2}, ... ,w_{t-N+1})}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.290242Z",
          "start_time": "2020-01-29T18:30:18.272506Z"
        },
        "id": "r54xOxTcbD3p"
      },
      "outputs": [],
      "source": [
        "class NGramLM():\n",
        "    def __init__(self, N):\n",
        "        self.N = N\n",
        "        self.vocab = set()\n",
        "        self.data = []\n",
        "        self.prob = {}\n",
        "        self.count = defaultdict(Counter)\n",
        "    \n",
        "    # For N = 1, the probability is stored in a dict   P = prob[next_word]\n",
        "    # For N > 1, the probability is in a nested dict   P = prob[context][next_word]\n",
        "    def train(self, vocab, data, smoothing_k=0):\n",
        "        self.vocab = vocab\n",
        "        self.data = data\n",
        "        self.smoothing_k = smoothing_k\n",
        "\n",
        "        if self.N == 1:\n",
        "            self.counts = Counter(flatten(data))\n",
        "            self.prob = self.get_prob(self.counts)\n",
        "        else:\n",
        "            self.vocab.add('<s>')\n",
        "            counts = self.count_ngram()\n",
        "            \n",
        "            self.prob = {}\n",
        "            for context, counter in counts.items():\n",
        "                self.prob[context] = self.get_prob(counter)\n",
        "    \n",
        "    def count_ngram(self):\n",
        "        counts = defaultdict(Counter)\n",
        "        for sentence in self.data:\n",
        "            sentence = (self.N - 1) * ['<s>'] + sentence \n",
        "            for i in range(len(sentence)-self.N+1):\n",
        "                context = sentence[i:i+self.N-1]\n",
        "                context = \" \".join(context)\n",
        "                word = sentence[i+self.N-1]\n",
        "                counts[context][word] += 1\n",
        "\n",
        "        self.counts = counts\n",
        "        return counts\n",
        "        \n",
        "    # normalize counts into probability(considering smoothing)\n",
        "    def get_prob(self, counter):\n",
        "        total = float(sum(counter.values()))\n",
        "        k = self.smoothing_k\n",
        "        \n",
        "        prob = {}\n",
        "        for word, count in counter.items():\n",
        "            prob[word] = (count + k) / (total + len(self.vocab) * k)\n",
        "        return prob\n",
        "        \n",
        "    def get_ngram_logprob(self, word, seq_len, context=\"\"):\n",
        "        if self.N == 1 and word in self.prob.keys():\n",
        "            return math.log(self.prob[word]) / seq_len\n",
        "        elif self.N > 1 and not self._is_unseen_ngram(context, word):\n",
        "            return math.log(self.prob[context][word]) / seq_len\n",
        "        else:\n",
        "            # assign a small probability to the unseen ngram\n",
        "            # to avoid log of zero and to penalise unseen word or context\n",
        "            return math.log(1/len(self.vocab)) / seq_len\n",
        "        \n",
        "    def get_ngram_prob(self, word, context=\"\"):\n",
        "        if self.N == 1 and word in self.prob.keys():\n",
        "            return self.prob[word]\n",
        "        elif self.N > 1 and not self._is_unseen_ngram(context, word):\n",
        "            return self.prob[context][word]\n",
        "        elif word in self.vocab and self.smoothing_k > 0:\n",
        "            # probability assigned by smoothing\n",
        "            return self.smoothing_k / (sum(self.counts[context].values()) + self.smoothing_k*len(self.vocab))\n",
        "        else:\n",
        "            # unseen word or context\n",
        "            return 0\n",
        "            \n",
        "    # In this method, the perplexity is measured at the sentence-level, averaging over all sentences.\n",
        "    # Actually, it is also possible to calculate perplexity by merging all sentences into a long one.\n",
        "    def perplexity(self, test_data):\n",
        "        log_ppl = 0\n",
        "        if self.N == 1:\n",
        "            for sentence in test_data:\n",
        "                for word in sentence:\n",
        "                    log_ppl += self.get_ngram_logprob(word=word, seq_len=len(sentence))\n",
        "        else:\n",
        "            for sentence in test_data:\n",
        "                for i in range(len(sentence)-self.N+1):\n",
        "                    context = sentence[i:i+self.N-1]\n",
        "                    context = \" \".join(context)\n",
        "                    word = sentence[i+self.N-1]\n",
        "                    log_ppl += self.get_ngram_logprob(context=context, word=word, seq_len=len(sentence))\n",
        "                        \n",
        "        log_ppl /= len(test_data)\n",
        "        ppl = math.exp(-log_ppl)\n",
        "        return ppl\n",
        "    \n",
        "    def _is_unseen_ngram(self, context, word):\n",
        "        if context not in self.prob.keys() or word not in self.prob[context].keys():\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    \n",
        "    # generate the most probable k words\n",
        "    def generate_next(self, context, k):\n",
        "        context = (self.N-1) * '<s> ' + context\n",
        "        context = context.split()\n",
        "        ngram_context_list = context[-self.N+1:]\n",
        "        ngram_context = \" \".join(ngram_context_list)\n",
        "        \n",
        "        if ngram_context in self.prob.keys():\n",
        "            candidates = self.prob[ngram_context]\n",
        "            most_probable_words = sorted(candidates.items(), key=lambda kv: kv[1], reverse=True)\n",
        "            for i in range(min(k, len(most_probable_words))):\n",
        "                print(\" \".join(context[self.N-1:])+\" \"+most_probable_words[i][0]+\"\\t P={}\".format(most_probable_words[i][1]))\n",
        "        else:\n",
        "            print(\"Unseen context!\")\n",
        "            \n",
        "    # generate the next n words with greedy search\n",
        "    def generate_next_n(self, context, n):\n",
        "        context = (self.N-1) * '<s> ' + context\n",
        "        context = context.split()\n",
        "        ngram_context_list = context[-self.N+1:]\n",
        "        ngram_context = \" \".join(ngram_context_list)\n",
        "        \n",
        "        for i in range(n):\n",
        "            try:\n",
        "                candidates = self.prob[ngram_context]\n",
        "                most_likely_next = max(candidates.items(), key=operator.itemgetter(1))[0]\n",
        "                context += [most_likely_next]\n",
        "                ngram_context_list = ngram_context_list[1:] + [most_likely_next]\n",
        "                ngram_context = \" \".join(ngram_context_list)\n",
        "            except:\n",
        "                break\n",
        "        print(\" \".join(context[self.N-1:]))\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuiN6WWxbD3w"
      },
      "source": [
        "# Train with toy dataset\n",
        "\n",
        "At this step, let's train a Bigram language model on the toy dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.297355Z",
          "start_time": "2020-01-29T18:30:18.291387Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQLC3GRPbD3y",
        "outputId": "7652305d-280f-48eb-8906-69961781a854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['i', 'like', 'ice', 'cream', '</s>'], ['i', 'like', 'chocolate', '</s>'], ['i', 'hate', 'beans', '</s>']]\n",
            "{'ice', 'beans', '</s>', 'i', 'chocolate', 'hate', 'like', 'cream'}\n"
          ]
        }
      ],
      "source": [
        "corpus = [\"I like ice cream\",\n",
        "         \"I like chocolate\",\n",
        "         \"I hate beans\"]\n",
        "data = [l.strip().lower().split() + ['</s>'] for l in corpus if l.strip()]\n",
        "vocab = set(flatten(data))\n",
        "print(data)\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.301895Z",
          "start_time": "2020-01-29T18:30:18.298317Z"
        },
        "id": "tviMvbjAbD33"
      },
      "outputs": [],
      "source": [
        "def print_probability(lm):\n",
        "    for context in lm.vocab:\n",
        "        for word in lm.vocab:\n",
        "            prob = lm.get_ngram_prob(word, context)\n",
        "            print(\"P({}\\t|{}) = {}\".format(word, context, prob))\n",
        "        print(\"--------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwjLGR9fbD35"
      },
      "source": [
        "# smoothing\n",
        "Smoothing method is used to deal with the sparsity problem in the N-Gram LM.\n",
        "The probability mass is shifted towards the less frequent words.\n",
        "\n",
        "For example, with an add-1 smoothing, the probability is calculated as:\n",
        "\n",
        "$$P(w_t | context) = \\frac{C(w_t, context)+1}{C(context)+|V|}$$\n",
        "\n",
        "Q1: What is the disadvantage of smoothing?\n",
        "\n",
        "A: If there are too many word with zero counts, the frequent words will sacrifice more probability, which might lead to higher perplexity on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.310269Z",
          "start_time": "2020-01-29T18:30:18.302761Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdrXSa4IbD37",
        "outputId": "fd141af0-f830-4b69-baaf-2d2c1046ae2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(ice\t|ice) = 0\n",
            "P(beans\t|ice) = 0\n",
            "P(</s>\t|ice) = 0\n",
            "P(i\t|ice) = 0\n",
            "P(chocolate\t|ice) = 0\n",
            "P(hate\t|ice) = 0\n",
            "P(like\t|ice) = 0\n",
            "P(cream\t|ice) = 1.0\n",
            "P(<s>\t|ice) = 0\n",
            "--------------------------\n",
            "P(ice\t|beans) = 0\n",
            "P(beans\t|beans) = 0\n",
            "P(</s>\t|beans) = 1.0\n",
            "P(i\t|beans) = 0\n",
            "P(chocolate\t|beans) = 0\n",
            "P(hate\t|beans) = 0\n",
            "P(like\t|beans) = 0\n",
            "P(cream\t|beans) = 0\n",
            "P(<s>\t|beans) = 0\n",
            "--------------------------\n",
            "P(ice\t|</s>) = 0\n",
            "P(beans\t|</s>) = 0\n",
            "P(</s>\t|</s>) = 0\n",
            "P(i\t|</s>) = 0\n",
            "P(chocolate\t|</s>) = 0\n",
            "P(hate\t|</s>) = 0\n",
            "P(like\t|</s>) = 0\n",
            "P(cream\t|</s>) = 0\n",
            "P(<s>\t|</s>) = 0\n",
            "--------------------------\n",
            "P(ice\t|i) = 0\n",
            "P(beans\t|i) = 0\n",
            "P(</s>\t|i) = 0\n",
            "P(i\t|i) = 0\n",
            "P(chocolate\t|i) = 0\n",
            "P(hate\t|i) = 0.3333333333333333\n",
            "P(like\t|i) = 0.6666666666666666\n",
            "P(cream\t|i) = 0\n",
            "P(<s>\t|i) = 0\n",
            "--------------------------\n",
            "P(ice\t|chocolate) = 0\n",
            "P(beans\t|chocolate) = 0\n",
            "P(</s>\t|chocolate) = 1.0\n",
            "P(i\t|chocolate) = 0\n",
            "P(chocolate\t|chocolate) = 0\n",
            "P(hate\t|chocolate) = 0\n",
            "P(like\t|chocolate) = 0\n",
            "P(cream\t|chocolate) = 0\n",
            "P(<s>\t|chocolate) = 0\n",
            "--------------------------\n",
            "P(ice\t|hate) = 0\n",
            "P(beans\t|hate) = 1.0\n",
            "P(</s>\t|hate) = 0\n",
            "P(i\t|hate) = 0\n",
            "P(chocolate\t|hate) = 0\n",
            "P(hate\t|hate) = 0\n",
            "P(like\t|hate) = 0\n",
            "P(cream\t|hate) = 0\n",
            "P(<s>\t|hate) = 0\n",
            "--------------------------\n",
            "P(ice\t|like) = 0.5\n",
            "P(beans\t|like) = 0\n",
            "P(</s>\t|like) = 0\n",
            "P(i\t|like) = 0\n",
            "P(chocolate\t|like) = 0.5\n",
            "P(hate\t|like) = 0\n",
            "P(like\t|like) = 0\n",
            "P(cream\t|like) = 0\n",
            "P(<s>\t|like) = 0\n",
            "--------------------------\n",
            "P(ice\t|cream) = 0\n",
            "P(beans\t|cream) = 0\n",
            "P(</s>\t|cream) = 1.0\n",
            "P(i\t|cream) = 0\n",
            "P(chocolate\t|cream) = 0\n",
            "P(hate\t|cream) = 0\n",
            "P(like\t|cream) = 0\n",
            "P(cream\t|cream) = 0\n",
            "P(<s>\t|cream) = 0\n",
            "--------------------------\n",
            "P(ice\t|<s>) = 0\n",
            "P(beans\t|<s>) = 0\n",
            "P(</s>\t|<s>) = 0\n",
            "P(i\t|<s>) = 1.0\n",
            "P(chocolate\t|<s>) = 0\n",
            "P(hate\t|<s>) = 0\n",
            "P(like\t|<s>) = 0\n",
            "P(cream\t|<s>) = 0\n",
            "P(<s>\t|<s>) = 0\n",
            "--------------------------\n"
          ]
        }
      ],
      "source": [
        "lm = NGramLM(2)\n",
        "lm.train(vocab, data, smoothing_k=0)\n",
        "\n",
        "######################################################\n",
        "# Q2: try with add-1 smoothing and see the probability\n",
        "######################################################\n",
        "# lm.train(vocab, data, smoothing_k=1)\n",
        "\n",
        "print_probability(lm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_oOB1NTbD3-"
      },
      "source": [
        "# Train on WikiText-2 dataset and evaluate perplexity\n",
        "### Evaluating perplexity\n",
        "\n",
        "Q3: why do we need to calculate log perplexity?\n",
        "\n",
        "A: The chain rule in the Markov Assumption would lead to a continuous multiplication of many small probabilities, which might lead to underflow. We use log perplexity to replace multiplication into addition.\n",
        "\n",
        "$ PPL(W) = P(w_1, w_2, ... , w_n)^{-\\frac{1}{n}}$\n",
        "\n",
        "$ log(PPL(W)) = -\\frac{1}{n}\\sum^n_{k=1}log(P(w_k|w_1, w_2, ... , w_{k-1}))$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.712903Z",
          "start_time": "2020-01-29T18:30:18.312677Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8cHU8V4bD3_",
        "outputId": "0d17d758-3624-4498-fbc3-5407c650ff4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28912\n"
          ]
        }
      ],
      "source": [
        "vocab, train_data = prepare_data('train.txt')\n",
        "_, valid_data = prepare_data('valid.txt')\n",
        "_, test_data = prepare_data('test.txt')\n",
        "print(len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:22.224402Z",
          "start_time": "2020-01-29T18:30:18.714081Z"
        },
        "id": "x0ZSMbzIbD4A"
      },
      "outputs": [],
      "source": [
        "lm = NGramLM(3)\n",
        "lm.train(vocab, train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:22.811004Z",
          "start_time": "2020-01-29T18:30:22.225566Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RZRGUdjbD4B",
        "outputId": "1871f8c0-0045-45b0-e777-78714d3251b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "387.1190374834364\n",
            "310.6880368161233\n"
          ]
        }
      ],
      "source": [
        "print(lm.perplexity(valid_data))\n",
        "print(lm.perplexity(test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHYanSFDbD4D"
      },
      "source": [
        "# Generating sentence\n",
        "With the pre-trained N-Gram language model, we can predict possible next words with given context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:22.815241Z",
          "start_time": "2020-01-29T18:30:22.812141Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e9972lxbD4D",
        "outputId": "8dc1886f-87b4-4667-8347-05c4343b54cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the eggs hatch at night , and the larvae swim to the water surface where they drift with the ocean currents , preying on <unk> . this stage involves three <unk> and lasts for 15 – 35 days . after the third moult , the juvenile takes on a form closer to the adult , and adopts a <unk> lifestyle . the juveniles are rarely seen in the wild , and are poorly known , although they are known to be capable of digging extensive burrows . it is estimated that only 1 larva in every 20 @,@ 000 survives to the <unk> phase . when they reach a carapace length of 15 mm ( 0 @.@ 59 in ) , the juveniles leave their burrows and start their adult lives . </s>\n",
            "The eggs hatch at night , and the larvae swim to the water surface where they were\t P=0.12408759124087591\n",
            "The eggs hatch at night , and the larvae swim to the water surface where they are\t P=0.08029197080291971\n",
            "The eggs hatch at night , and the larvae swim to the water surface where they would\t P=0.051094890510948905\n"
          ]
        }
      ],
      "source": [
        "# generate the most probable following words given the context\n",
        "print(\" \".join(valid_data[12]))\n",
        "\n",
        "# actually the only useful contexts in the Trigram LM is [\"where\", \"they\"]\n",
        "context = \"The eggs hatch at night , and the larvae swim to the water surface where they\"  \n",
        "\n",
        "lm.generate_next(context, 3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can also generate with shorter contexts, even shorter than N-1\n",
        "\n",
        "contexts = [\"the eggs\",\n",
        "            \"the\",\n",
        "            \"\"]\n",
        "for context in contexts:\n",
        "  lm.generate_next(context, 3)\n",
        "  print(\"---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QyA4VDsb063",
        "outputId": "c1e55c14-8e0d-4ffa-d060-24c44fef7e51"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the eggs hatch\t P=0.18181818181818182\n",
            "the eggs of\t P=0.18181818181818182\n",
            "the eggs are\t P=0.13636363636363635\n",
            "---\n",
            "the first\t P=0.03398926654740608\n",
            "the <unk>\t P=0.020274299344066785\n",
            "the episode\t P=0.015802027429934407\n",
            "---\n",
            " =\t P=0.26132873311734756\n",
            " the\t P=0.14112004039214035\n",
            " in\t P=0.06353347077881095\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:22.828378Z",
          "start_time": "2020-01-29T18:30:22.816839Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4voAwzLbD4F",
        "outputId": "4457671f-b4db-4937-f527-77dd438475c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the eggs hatch at night , and the larvae swim to the water surface where they were not able to get the part of the <unk>\n",
            "the eggs hatch at night , and the larvae swim to the water surface where they were not able to get the part of the <unk> of the <unk> of the <unk> of the <unk> of\n"
          ]
        }
      ],
      "source": [
        "context = \"the eggs hatch at night , and the larvae swim to the water surface where they\"  \n",
        "\n",
        "# generate the next n words with greedy search\n",
        "lm.generate_next_n(context, 10)\n",
        "\n",
        "# This is not a good method in practice,\n",
        "# because wrong predictions in the early steps would introduce errors to the following predictions\n",
        "lm.generate_next_n(context, 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yDsf3qqbD4G"
      },
      "source": [
        "# Effect of N\n",
        "\n",
        "Q4: Why does the perplexity increase when N is large?\n",
        "\n",
        "A: Because with larger N, the context (previous N-1 words) in the valid/test data is not likely to be seen in the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:41.930826Z",
          "start_time": "2020-01-29T18:30:22.829892Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teJv6ZV7bD4H",
        "outputId": "33742b9e-3910-45a1-fd51-c295229a4a5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************************\n",
            "1-gram LM perplexity on valid set: 599.6450225274759\n",
            "1-gram LM perplexity on test  set: 553.0467265986567\n",
            "************************\n",
            "2-gram LM perplexity on valid set: 130.75949726991627\n",
            "2-gram LM perplexity on test  set: 114.76592406312065\n",
            "************************\n",
            "3-gram LM perplexity on valid set: 387.1190374834364\n",
            "3-gram LM perplexity on test  set: 310.6880368161233\n",
            "************************\n",
            "4-gram LM perplexity on valid set: 1091.9348578736633\n",
            "4-gram LM perplexity on test  set: 846.7563102382584\n",
            "************************\n",
            "5-gram LM perplexity on valid set: 1558.823545905558\n",
            "5-gram LM perplexity on test  set: 1201.746345143052\n"
          ]
        }
      ],
      "source": [
        "for n in range(1,6):\n",
        "    lm = NGramLM(n)\n",
        "    lm.train(vocab, train_data)\n",
        "    print(\"************************\")\n",
        "    print(\"{}-gram LM perplexity on valid set: {}\".format(n, lm.perplexity(valid_data)))\n",
        "    print(\"{}-gram LM perplexity on test  set: {}\".format(n, lm.perplexity(test_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mr5dgPebD4J"
      },
      "source": [
        "# Interpolation\n",
        "In interpolation, we mix the probability estimates from all the n-gram estimators to alleviate the sparsity problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:41.937643Z",
          "start_time": "2020-01-29T18:30:41.932047Z"
        },
        "id": "DPTm2tyWbD4N"
      },
      "outputs": [],
      "source": [
        "class InterpolateNGramLM(NGramLM):\n",
        "    \n",
        "    def __init__(self, N):\n",
        "        super(InterpolateNGramLM, self).__init__(N)\n",
        "        self.ngram_lms = []\n",
        "        self.lambdas = []\n",
        "        \n",
        "    def train(self, vocab, data, smoothing_k=0, lambdas=[]):\n",
        "        assert len(lambdas) == self.N\n",
        "        assert sum(lambdas) - 1 < 1e-9\n",
        "        self.vocab = vocab\n",
        "        self.lambdas = lambdas\n",
        "        \n",
        "        for i in range(self.N, 0, -1):\n",
        "            lm = NGramLM(i)\n",
        "            print(\"Training {}-gram language model\".format(i))\n",
        "            lm.train(vocab, data, smoothing_k)\n",
        "            self.ngram_lms.append(lm)\n",
        "    \n",
        "    def get_ngram_logprob(self, word, seq_len, context):\n",
        "        prob = 0\n",
        "        for i, (coef, lm) in enumerate(zip(self.lambdas, self.ngram_lms)):\n",
        "            context_words = context.split()\n",
        "            cutted_context = \" \".join(context_words[-self.N + i + 1:])\n",
        "            prob += coef * lm.get_ngram_prob(context=cutted_context, word=word)\n",
        "        return math.log(prob) / seq_len\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:46.507705Z",
          "start_time": "2020-01-29T18:30:41.939076Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxRgykSIbD4P",
        "outputId": "1d5b687f-e2eb-486a-881a-868022747fd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training 3-gram language model\n",
            "Training 2-gram language model\n",
            "Training 1-gram language model\n"
          ]
        }
      ],
      "source": [
        "###################################################\n",
        "# Q5: tune your coefficients to decrease perplexity\n",
        "###################################################\n",
        "ilm = InterpolateNGramLM(3)\n",
        "ilm.train(vocab, train_data, lambdas=[0.5, 0.4, 0.1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:48.376911Z",
          "start_time": "2020-01-29T18:30:46.509150Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRrK01C1bD4P",
        "outputId": "d09fff02-abc5-4e1a-dea7-16832ff16343"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "126.26886448793077\n",
            "103.0773276220259\n"
          ]
        }
      ],
      "source": [
        "print(ilm.perplexity(valid_data))\n",
        "print(ilm.perplexity(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "VliUlTrLbD4R"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "version": 3,
    "colab": {
      "name": "lab03_1_NgramLMs_Solutions.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}