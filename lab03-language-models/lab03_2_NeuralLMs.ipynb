{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "lab03_2_NeuralLMs.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJMQLZ_uf6tI"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this session, you will experiment with feed-forward neural language models (FFLM) and recurrent language models (RNNLM) using [PyTorch](https://www.pytorch.org). To train the models, you will be using the [WikiText-2](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) corpus, which is a popular LM dataset introduced in 2016:\n",
        "\n",
        "> The WikiText language modeling dataset is a collection of texts extracted from Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License. Compared to the preprocessed version of Penn Treebank (PTB), `WikiText-2` is over 2 times larger. The dataset also features a far larger vocabulary and retains the original case, punctuation and numbers - all of which are removed in PTB. As it is composed of full articles, the dataset is well suited for models that can take advantage of long term dependencies.\n",
        "\n",
        "**NOTE:** Training on the whole corpus is time consuming on CPU. Make sure that you switch to a GPU runtime in Colab or use the `train_small` corpus which is a subset of the WikiText-2 dataset.\n",
        "\n",
        "**Let's start by downloading the corpus:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eLd2J2B1i2u"
      },
      "source": [
        "# Download the corpus\n",
        "%%bash\n",
        "URL=\"https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2\"\n",
        "\n",
        "for split in \"train\" \"valid\" \"test\"; do\n",
        "  if [ ! -f \"${split}.txt\" ]; then\n",
        "    echo \"Downloading ${split}.txt\"\n",
        "    wget -q \"${URL}/${split}.txt\"\n",
        "    # Remove empty lines\n",
        "    sed -i '/^ *$/d' \"${split}.txt\"\n",
        "    # Remove article titles starting with = and ending with =\n",
        "    sed -i '/^ *= .* = $/d' \"${split}\".txt\n",
        "  fi\n",
        "done\n",
        "\n",
        "# Prepare smaller version for fast training neural LMs\n",
        "head -n 5000 < train.txt > train_small.txt\n",
        "\n",
        "# Print the first 10 lines with line numbers\n",
        "cat -n train.txt | head -n10\n",
        "echo\n",
        "\n",
        "# Print some statistics\n",
        "echo -e \"\\n   Line,   word,   character counts\"\n",
        "wc *.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzLYCAuD1-Ij"
      },
      "source": [
        "## Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSFcmlFdf6tK"
      },
      "source": [
        "import math\n",
        "import time\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Fancy progress bar\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "###############\n",
        "# Torch setup #\n",
        "###############\n",
        "print('Torch version: {}, CUDA: {}'.format(torch.__version__, torch.version.cuda))\n",
        "cuda_available = torch.cuda.is_available()\n",
        "if not torch.cuda.is_available():\n",
        "  print('WARNING: You may want to change the runtime to GPU for Neural LM experiments!')\n",
        "  DEVICE = 'cpu'\n",
        "else:\n",
        "  DEVICE = 'cuda:0'\n",
        "\n",
        "#######################\n",
        "# Some helper functions\n",
        "#######################\n",
        "def fix_seed(seed=None):\n",
        "  \"\"\"Sets the seeds of random number generators.\"\"\"\n",
        "  if seed is None:\n",
        "    # Take a random seed\n",
        "    seed = time.time()\n",
        "  seed = int(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  return seed\n",
        "\n",
        "def readable_size(n):\n",
        "  \"\"\"Returns a readable size string for model parameters count.\"\"\"\n",
        "  sizes = ['K', 'M', 'G']\n",
        "  fmt = ''\n",
        "  size = n\n",
        "  for i, s in enumerate(sizes):\n",
        "    nn = n / (1000 ** (i + 1))\n",
        "    if nn >= 1:\n",
        "      size = nn\n",
        "      fmt = sizes[i]\n",
        "    else:\n",
        "      break\n",
        "  return '%.2f%s' % (size, fmt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmP0ZWbvB1aa"
      },
      "source": [
        "# Feed-forward Language Models (FFLM)\n",
        "\n",
        "FFLMs are similar to $n$-gram language models in the sense that the choice of $n$ is a hyperparameter for the network architecture. A basic FFLM constructs a  $C=n\\mathrm{-1}$ length context window before the word to be predicted. If the word embedding size is $E$, the feature vector for the context window becomes a vector of size $E\\times C$, resulting from the **concatenation** of individual word embeddings of context words. Hence, the choice of $C$ for FFLMs, affects the number of final learnable parameters in the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWMhsDZOf6tS"
      },
      "source": [
        "## Representing the vocabulary\n",
        "\n",
        "The below `Vocabulary` class encapsulates the **word-to-idx** and **idx-to-word** mapping that you should now be familiar with from the previous lab sessions. Read it to understand how the vocabulary is constructed from a plain text file, within the `build_from_file()` method. Special `<.>` markers are also included in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlxMOBzPf6tT"
      },
      "source": [
        "class Vocabulary(object):\n",
        "  \"\"\"Data structure representing the vocabulary of a corpus.\"\"\"\n",
        "  def __init__(self):\n",
        "    # Mapping from tokens to integers\n",
        "    self._word2idx = {}\n",
        "\n",
        "    # Reverse-mapping from integers to tokens\n",
        "    self.idx2word = []\n",
        "\n",
        "    # 0-padding token\n",
        "    self.add_word('<pad>')\n",
        "    # sentence start\n",
        "    self.add_word('<s>')\n",
        "    # sentence end\n",
        "    self.add_word('</s>')\n",
        "    # Unknown words\n",
        "    self.add_word('<unk>')\n",
        "\n",
        "    self._unk_idx = self._word2idx['<unk>']\n",
        "\n",
        "  def word2idx(self, word):\n",
        "    \"\"\"Returns the integer ID of the word or <unk> if not found.\"\"\"\n",
        "    return self._word2idx.get(word, self._unk_idx)\n",
        "\n",
        "  def add_word(self, word):\n",
        "    \"\"\"Adds the `word` into the vocabulary.\"\"\"\n",
        "    if word not in self._word2idx:\n",
        "      self.idx2word.append(word)\n",
        "      self._word2idx[word] = len(self.idx2word) - 1\n",
        "\n",
        "  def build_from_file(self, fname):\n",
        "    \"\"\"Builds a vocabulary from a given corpus file.\"\"\"\n",
        "    with open(fname) as f:\n",
        "      for line in f:\n",
        "        words = line.strip().split()\n",
        "        for word in words:\n",
        "          self.add_word(word)\n",
        "\n",
        "  def convert_idxs_to_words(self, idxs):\n",
        "    \"\"\"Converts a list of indices to words.\"\"\"\n",
        "    return ' '.join(self.idx2word[idx] for idx in idxs)\n",
        "\n",
        "  def convert_words_to_idxs(self, words):\n",
        "    \"\"\"Converts a list of words to a list of indices.\"\"\"\n",
        "    return [self.word2idx(w) for w in words]\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"Returns the size of the vocabulary.\"\"\"\n",
        "    return len(self.idx2word)\n",
        "  \n",
        "  def __repr__(self):\n",
        "    return \"Vocabulary with {} items\".format(self.__len__())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hE1Lr6j_oYB"
      },
      "source": [
        "Let's construct the vocabulary for the training set and analyse the token indices for a sentence with an unknown word.\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Why do we map unknown tokens to a special `<unk>` token? Do you think the network will learn a useful embedding for that? If not, how can you let the network to learn an embedding for it?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcNMhXqB5wwT"
      },
      "source": [
        "vocab = Vocabulary()\n",
        "vocab.build_from_file('train.txt')\n",
        "print(vocab)\n",
        "\n",
        "# Convert sentence to list of indices, note how the last word is mapped to 3 (<unk>)\n",
        "print(vocab.convert_words_to_idxs('the cat sat on a probably_an_unknown_word'.split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-4PQ9_8f6tV"
      },
      "source": [
        "## Representing the corpus\n",
        "\n",
        "Let's process the corpus for PyTorch: all splits will end up being a large, 1D token sequences. Note that, in `corpus_to_tensor()`, every line is wrapped between `<s> .. </s>` tags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5s9MMt06YsL"
      },
      "source": [
        "def corpus_to_tensor(_vocab, filename):\n",
        "  # Final token indices\n",
        "  idxs = []\n",
        "  \n",
        "  with open(filename) as data:\n",
        "    for line in tqdm(data, ncols=80, unit=' line', desc=f'Reading {filename} '):\n",
        "      line = line.strip()\n",
        "      # Skip empty lines if any\n",
        "      if line:\n",
        "        # Each line is considered as a long sentence for WikiText-2\n",
        "        line = f\"<s> {line} </s>\"\n",
        "        # Split from whitespace and add sentence markers\n",
        "        idxs.extend(_vocab.convert_words_to_idxs(line.split()))\n",
        "  return torch.LongTensor(idxs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl6xuwZS9uO1"
      },
      "source": [
        "# Read the files, prepare the small one as well\n",
        "train = corpus_to_tensor(vocab, 'train.txt')\n",
        "train_small = corpus_to_tensor(vocab, 'train_small.txt')\n",
        "\n",
        "valid = corpus_to_tensor(vocab, 'valid.txt')\n",
        "test = corpus_to_tensor(vocab, 'test.txt')\n",
        "print('\\n')\n",
        "\n",
        "print(f'Small training size in tokens: {readable_size(len(train_small))}')\n",
        "print(f'Training size in tokens: {readable_size(len(train))}')\n",
        "print(f'Validation size in tokens: {readable_size(len(valid))}')\n",
        "print(f'Test size in tokens: {readable_size(len(test))}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNxNrdhPECML"
      },
      "source": [
        "**Q: Print the first 20 token indices from the training set. And then print the sentence in actual words corresponding to these 20 tokens by using one of the provided methods in the `Vocabulary` class.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXobO9_n-Giz"
      },
      "source": [
        "########\n",
        "# Answer\n",
        "########"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiroR-NszhGY"
      },
      "source": [
        "## Model definition\n",
        "\n",
        "Now that we are done with data loading and vocabulary construction, we can define the actual FFLM model in PyTorch. Recall from the lectures that this model requires a pre-defined context window size $C$ which will affect the way you set up some of the linear layers. **Note that**, in contrast to the model depicted in the lecture, this model has an additional layer `ff_ctx`, which projects the context vector $c_k$ to hidden dimension $H$. This ensures that the number of parameters in the output layer does not depend on the context size, i.e. it is always $H\\times V$ instead of $CE\\times V$.\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Follow the comments in `__init__()` and `forward()` to fill in the missing parts with some actual code.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9ttYW2IC_UV"
      },
      "source": [
        "class FFLM(nn.Module):\n",
        "  def __init__(self, vocab_size, emb_dim, hid_dim, context_size, dropout=0.5):\n",
        "    # Call parent's __init__ first\n",
        "    super(FFLM, self).__init__()\n",
        "    \n",
        "    # Store arguments\n",
        "    self.vocab_size = vocab_size\n",
        "    self.emb_dim = emb_dim\n",
        "    self.hid_dim = hid_dim\n",
        "    self.context_size = context_size\n",
        "\n",
        "    # Create the loss, don't sum or average, we'll take care of it\n",
        "    # in the training loop for logging purposes\n",
        "    self.loss = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "    # Create the non-linearity\n",
        "    self.nonlin = torch.nn.Tanh()\n",
        "\n",
        "    # Dropout regularizer\n",
        "    self.drop = nn.Dropout(p=dropout)\n",
        "\n",
        "    ##############################\n",
        "    # Fill the missing parts below\n",
        "    ##############################\n",
        "    # Q: Compute the dimension of the context vector\n",
        "    self.context_dim = \"<TODO>\"\n",
        "    \n",
        "    # Create the embedding layer (i.e. lookup table tokens->vectors)\n",
        "    self.emb = nn.Embedding(\n",
        "        num_embeddings=self.vocab_size, embedding_dim=self.emb_dim,\n",
        "        padding_idx=0)\n",
        " \n",
        "    # This cuts the number of parameters a bit\n",
        "    self.ff_ctx = nn.Linear(self.context_dim, self.hid_dim)\n",
        "\n",
        "    ############################################\n",
        "    # Output layer mapping from the output of `ff_ctx` to vocabulary size\n",
        "    # Q: Fill the dimensions of the output layer\n",
        "    ############################################\n",
        "    self.out = nn.Linear(\"<TODO>\")\n",
        "\n",
        "    # Purely for informational purposes: compute # of total params\n",
        "    self.n_params = 0\n",
        "    for param in self.parameters():\n",
        "        self.n_params += np.cumprod(param.data.size())[-1]\n",
        "    self.n_params = readable_size(self.n_params)\n",
        "      \n",
        "  def forward(self, x, y):\n",
        "    \"\"\"Forward-pass of the module.\"\"\"\n",
        "    # Shape of x is (batch_size, context_size)\n",
        "\n",
        "    # Get the embeddings for the token indices in `x`\n",
        "    embs = self.emb(x)\n",
        "\n",
        "    ##########################################################\n",
        "    # Q: Concatenate the embeddings to form the context vector\n",
        "    ##########################################################\n",
        "    ctx = \"<TODO>\"\n",
        "\n",
        "    #######################################################\n",
        "    # Q: Apply ff_ctx -> non-lin -> dropout -> output layer\n",
        "    # to obtain the logits i.e. unnormalized scores   \n",
        "    #######################################################\n",
        "    logits = \"<TODO>\"\n",
        "\n",
        "    ###########################################################\n",
        "    # Q: Use self.loss to compute the losses, return the losses\n",
        "    # (true labels are in `y`)\n",
        "    ###########################################################\n",
        "    return \"<TODO>\"\n",
        "\n",
        "  def get_batches(self, data_tensor, batch_size=64):\n",
        "    \"\"\"Returns a tensor of size (n_batches, batch_size, context_size + 1).\"\"\"\n",
        "    # Split data into rows of n-grams followed by the (n+1)th true label\n",
        "    x_y = data_tensor.unfold(0, self.context_size + 1, step=1)\n",
        "\n",
        "    # Get the number of training n-grams\n",
        "    n_samples = x_y.size()[0]\n",
        "\n",
        "    # Hack: discard the last uneven batch for simplicity\n",
        "    n_batches = n_samples // batch_size\n",
        "    n_samples = n_batches * batch_size\n",
        "    # Split nicely into batches, i.e. (n_batches, batch_size, context_size + 1)\n",
        "    # The final element in each row is the ID of the true label to predict\n",
        "    x_y = x_y[:n_samples].view(n_batches, batch_size, -1)\n",
        "\n",
        "    # A particular batch for context_size=2 will now look like below in\n",
        "    # word format. Last element for every array is the next token to be predicted\n",
        "    #\n",
        "    # [[<s>, cat, sat],\n",
        "    #  [cat, sat, on],\n",
        "    #  [sat, on,  the],\n",
        "    #  [on,  the, mat],\n",
        "    #   ....\n",
        "    return x_y\n",
        "\n",
        "  def train_model(self, optim, train_tensor, valid_tensor, test_tensor, n_epochs=5,\n",
        "                 batch_size=64, shuffle=False):\n",
        "    \"\"\"Trains the model.\"\"\"\n",
        "    # Get batches for the training data\n",
        "    batches = self.get_batches(train_tensor, batch_size)\n",
        "    \n",
        "    print(f'Will do {batches.size(0)} batches for an epoch.')\n",
        "\n",
        "    for eidx in range(1, n_epochs + 1):\n",
        "      start_time = time.time()\n",
        "      epoch_loss = 0\n",
        "      epoch_items = 0\n",
        "\n",
        "      # Enable training mode\n",
        "      self.train()\n",
        "\n",
        "      # Shuffle the batch order or not\n",
        "      if shuffle:\n",
        "        batch_order = torch.randperm(batches.size(0))\n",
        "      else:\n",
        "        batch_order = torch.arange(batches.size(0))\n",
        "\n",
        "      # Start training\n",
        "      for iter_count, idx in enumerate(batch_order):\n",
        "        batch = batches[idx].to(DEVICE)\n",
        "\n",
        "        # split into inputs `x` and labels `y`\n",
        "        x, y = batch[:, :self.context_size], batch[:, -1]\n",
        "\n",
        "        # Clear the gradients\n",
        "        optim.zero_grad()\n",
        "\n",
        "        # loss will be a vector of size (batch_size, ) with losses per every sample\n",
        "        loss = self.forward(x, y)\n",
        "\n",
        "        # Backprop the average loss and update parameters\n",
        "        loss.mean().backward()\n",
        "        optim.step()\n",
        "\n",
        "        # sum the loss for reporting, along with the denominator\n",
        "        epoch_loss += loss.detach().sum()\n",
        "        epoch_items += loss.numel()\n",
        "\n",
        "        if iter_count % 1000 == 0:\n",
        "          # Print progress\n",
        "          loss_per_token = epoch_loss / epoch_items\n",
        "          ppl = math.exp(loss_per_token)\n",
        "          print(f'[Epoch {eidx:<3}] loss: {loss_per_token:6.2f}, perplexity: {ppl:6.2f}')\n",
        "\n",
        "      time_spent = time.time() - start_time\n",
        "\n",
        "      print(f'\\n[Epoch {eidx:<3}] ended with train_loss: {loss_per_token:6.2f}, ppl: {ppl:6.2f}')\n",
        "      # Evaluate on valid set\n",
        "      valid_loss, valid_ppl = self.evaluate(test_set=valid_tensor)\n",
        "      print(f'[Epoch {eidx:<3}] ended with valid_loss: {valid_loss:6.2f}, valid_ppl: {valid_ppl:6.2f}')\n",
        "      print(f'[Epoch {eidx:<3}] completed in {time_spent:.2f} seconds\\n')\n",
        "\n",
        "    # Evaluate the final model on test set\n",
        "    test_loss, test_ppl = self.evaluate(test_set=test_tensor)\n",
        "    print(f' ---> Final test set performance: {test_loss:6.2f}, test_ppl: {test_ppl:6.2f}')\n",
        "\n",
        "  def evaluate(self, test_set, batch_size=32):\n",
        "    \"\"\"Evaluates and computes perplexity for the given test set.\"\"\"\n",
        "    loss = 0\n",
        "\n",
        "    # Get the batches\n",
        "    batches = self.get_batches(test_set, batch_size)\n",
        "\n",
        "    # Eval mode\n",
        "    self.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch in batches:\n",
        "        batch = batch.to(DEVICE)\n",
        "\n",
        "        # split into inputs `x` and labels `y`\n",
        "        x, y = batch[:, :self.context_size], batch[:, -1]\n",
        "\n",
        "        # loss will be a vector of size (batch_size, ) with losses per every sample\n",
        "        # sum the loss for reporting, along with the denominator\n",
        "        loss += self.forward(x, y).sum()\n",
        "    \n",
        "    # Normalize by the number of tokens in the test set\n",
        "    loss /= batches.size()[:2].numel()\n",
        "\n",
        "    # Switch back to training mode\n",
        "    self.train()\n",
        "\n",
        "    # return the perplexity and loss\n",
        "    return loss, math.exp(loss)\n",
        "\n",
        "  def __repr__(self):\n",
        "    \"\"\"String representation for pretty-printing.\"\"\"\n",
        "    s = super(FFLM, self).__repr__()\n",
        "    return f\"{s}\\n# of parameters: {self.n_params}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0eHpYiRUHDe"
      },
      "source": [
        "## Training\n",
        "\n",
        "We can now launch training using a set of sane hyper-parameters for our model. This is a 3-gram FFLM since the context size is set to 2. On a Colab GPU, a single epoch should take around 1 minute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rjwvEYYFjjE"
      },
      "source": [
        "# Set the seed for reproducible results\n",
        "fix_seed(30494)\n",
        "\n",
        "fflm_model = FFLM(\n",
        "    len(vocab),       # vocabulary size\n",
        "    emb_dim=128,      # word embedding dim\n",
        "    hid_dim=128,      # hidden layer dim\n",
        "    context_size=2,   # C = (N-1) if you think in n-gram LM terminology\n",
        "    dropout=0.4,      # dropout probability\n",
        ")\n",
        "\n",
        "# move to device\n",
        "fflm_model.to(DEVICE)\n",
        "\n",
        "# Initial learning rate for the optimizer\n",
        "FFLM_INIT_LR = 0.001\n",
        "\n",
        "# Create the optimizer\n",
        "fflm_optimizer = torch.optim.Adam(fflm_model.parameters(), lr=FFLM_INIT_LR)\n",
        "print(fflm_model)\n",
        "\n",
        "print('Starting training!')\n",
        "# NOTE: If you happen to have memory errors, try decreasing the batch size\n",
        "# It will print progress every 1000 batches\n",
        "fflm_model.train_model(fflm_optimizer, train, valid, test, n_epochs=5, batch_size=256, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAHN-C-XHuVs"
      },
      "source": [
        "**Q: If everything goes well, you should see a loss of around ~10.4 printed as the first loss. This will still be the case if you change the random seed to some other number before model construction i.e. the culprit is not the exact values that they take. Can you come up with a simple mathematical formula which yields that value?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiG_vI1uJj1X"
      },
      "source": [
        "##########################\n",
        "# Answer to question above\n",
        "##########################\n",
        "print(\"<TODO: put the formula here which computes the value>\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsYXRU68SWF7"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "With the default settings above, you should end up with a validation perplexity of $\\sim258$ and a final test set perplexity of $\\sim238$ at the end of 5th epoch. Now here are some exercises that you can proceed with:\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Remove the `tanh()` non-linearity from the code so that the context is computed as a linear combination of its embeddings. How does the results compare to the initial one? Do you think non-linearity helps?**\n",
        "\n",
        "**Q: Compare the results by rerunning the training with unshuffled batches i.e. with `shuffle=False`. What do you notice in terms of results?**\n",
        "\n",
        "**Q: Play with hyper-parameters related to dimensions and dropout. Could you find a model with smaller perplexity?**\n",
        "\n",
        "**Q: Try with different context sizes such as 3, 5, 7, etc. What is the best perplexity you can get?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5cOyX2Kf6td"
      },
      "source": [
        "# Recurrent Language Models (RNNLM)\n",
        "\n",
        "It is now time to switch to more complex LMs, basically the recurrent ones which have access to large context windows. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG3k2NS8qFYk"
      },
      "source": [
        "## Model definition\n",
        "\n",
        "You will notice that apart from the `train_model()` and `get_batches()` methods, the remaining parts look similar to `FFLM`. Take your time to compare both models. Read `get_batches()` thoroughly as preparing the batches and chunking into further fragments for BPTT is pretty interesting and may be unintuitive at first look.\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Follow the comments in `forward()` method to fill in the missing parts with some actual code. You will come back here when you reach the question regarding the support for LSTM. So, skip LSTM-related TODO's for now.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7FcID_kf6te"
      },
      "source": [
        "class RNNLM(nn.Module):\n",
        "  \"\"\"RNN-based LM module.\"\"\"\n",
        "  def __init__(self, vocab_size, emb_dim, hid_dim, rnn_type='RNN',\n",
        "               n_layers=1, dropout=0.5, clip_gradient_norm=1.0,\n",
        "               bptt_steps=35):\n",
        "    # Call parent's __init__ first\n",
        "    super(RNNLM, self).__init__()\n",
        "    \n",
        "    # Store arguments\n",
        "    self.vocab_size = vocab_size\n",
        "    self.emb_dim = emb_dim\n",
        "    self.hid_dim = hid_dim\n",
        "    self.clip_gradient_norm = clip_gradient_norm\n",
        "    self.bptt_steps = bptt_steps\n",
        "    self.n_layers = n_layers\n",
        "    self.rnn_type = rnn_type.upper()\n",
        "\n",
        "    # This will be used to store the detached histories for truncated BPTT\n",
        "    self.prev_histories = None\n",
        "  \n",
        "    # Create the loss, don't sum or average, we'll take care of it\n",
        "    # in the training loop for logging purposes\n",
        "    self.loss = nn.CrossEntropyLoss(reduction='none')\n",
        "    \n",
        "    # Create the dropout\n",
        "    self.drop = nn.Dropout(p=dropout)\n",
        "    \n",
        "    # Create the embedding layer as usual\n",
        "    self.emb = nn.Embedding(\n",
        "      num_embeddings=self.vocab_size, embedding_dim=self.emb_dim,\n",
        "      padding_idx=0)\n",
        "    \n",
        "    # Create the RNN layer\n",
        "    if self.rnn_type == 'RNN':\n",
        "      self.rnn = nn.RNN( \n",
        "          input_size=self.emb_dim, hidden_size=self.hid_dim,\n",
        "          num_layers=self.n_layers, nonlinearity='tanh')\n",
        "    elif self.rnn_type == 'GRU':\n",
        "      self.rnn = nn.GRU(\n",
        "          input_size=self.emb_dim, hidden_size=self.hid_dim,\n",
        "          num_layers=self.n_layers)\n",
        "    elif self.rnn_type == 'LSTM':\n",
        "      #####################################\n",
        "      # Q: Fill in to create the LSTM layer\n",
        "      #####################################\n",
        "      self.rnn = \"<TODO>\"\n",
        "   \n",
        "    # Create the output layer: maps the hidden state of the RNN to vocabulary\n",
        "    self.out = nn.Linear(self.hid_dim, self.vocab_size)\n",
        "\n",
        "    # Compute number of parameters for information\n",
        "    self.n_params = 0\n",
        "    for param in self.parameters():\n",
        "      self.n_params += np.cumprod(param.data.size())[-1]\n",
        "    self.n_params = readable_size(self.n_params)\n",
        "\n",
        "  def init_state(self, batch_size):\n",
        "    \"\"\"Returns the initial 0 states.\"\"\"\n",
        "    if self.rnn_type != 'LSTM':\n",
        "      # for every layer and every sample -> 0 hidden state vector\n",
        "      return torch.zeros(self.n_layers, batch_size, self.hid_dim, device=DEVICE)\n",
        "    else:\n",
        "      #################################################################\n",
        "      # Q: Adapt the above snippet to LSTM. Check PyTorch docs\n",
        "      # to understand what is the expectation of LSTM's forward() call\n",
        "      # in terms of initial states.\n",
        "      #################################################################\n",
        "      return \"<TODO>\"\n",
        "\n",
        "  def clear_hidden_states(self):\n",
        "    \"\"\"Set the relevant instance attribute to None.\"\"\"\n",
        "    self.prev_histories = None\n",
        "\n",
        "  def save_hidden_states(self, last_states):\n",
        "    \"\"\"Save the detached states into the model for the next batch. `last_states`\n",
        "    is the second return value of RNN/GRU/LSTM's forward() methods.\"\"\"\n",
        "    if isinstance(last_states, tuple):\n",
        "      # This is true for LSTM\n",
        "      self.prev_histories = tuple(r.detach() for r in last_states)\n",
        "    else:\n",
        "      self.prev_histories = last_states.detach()\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    \"\"\"Forward-pass of the module.\"\"\"\n",
        "    # Detached previous histories for a batch. If `None`, we assume\n",
        "    # start of an epoch or start of an evaluation and create 0\n",
        "    # vector(s) to start with.\n",
        "    if self.prev_histories is None:\n",
        "      self.prev_histories = self.init_state(x.shape[1])\n",
        "\n",
        "    # Tokens -> Embeddings -> Dropout\n",
        "    embs = self.drop(self.emb(x))\n",
        "\n",
        "    # an RNN in PyTorch returns two values:\n",
        "    # (1) All hidden states of the last RNN layer\n",
        "    #     Shape -> (bptt_steps, batch_size, hid_dim)\n",
        "    #     You'll plug the output layer on top of this to obtain\n",
        "    #     the logits for each prediction.\n",
        "    # (2) Hidden state h_t of last timestep for EVERY layer\n",
        "    #     Shape -> (self.n_layers, batch_size, hid_dim)\n",
        "    #     This is what we'll store as the previous history\n",
        "    #     (NOTE: this is a tuple for LSTM which contains h_t and c_t)\n",
        "    all_hids, last_hid = self.rnn(embs, self.prev_histories)\n",
        "\n",
        "    # Detach the computation graph since we are done with BPTT for this batch\n",
        "    self.save_hidden_states(last_hid)\n",
        "\n",
        "    ##########################################################\n",
        "    # Q: Apply dropout on all_hids and pass it to output layer\n",
        "    ##########################################################\n",
        "    logits = \"<TODO>\"\n",
        "\n",
        "    # Return the losses per token/position\n",
        "    return self.loss(logits.view(-1, self.vocab_size), y)\n",
        "\n",
        "  def get_batches(self, data_tensor, batch_size):\n",
        "    # NOTE: There is absolutely no shuffling here, which\n",
        "    # will totally break the histories coming from previous steps.\n",
        "    # The document is evenly divided into independent `batch_size` portions.\n",
        "    # At every iteration, the BPTT window will slide over each of these\n",
        "    # portions, by keeping track of the previous h_t's as discussed\n",
        "    # in the lecture.\n",
        "    \n",
        "    # Imagine this as `batch_size` pointers running over the text, each\n",
        "    # processing its share in a continuous. Although the portions may have\n",
        "    # been splitted in a noisy way (one pointer can be starting from the\n",
        "    # middle of a sentence for example), this makes training faster.\n",
        "    # For instance, with the alphabet as the dataset and batch size 4, we'd get\n",
        "    # ┌ a g m s ┐\n",
        "    # │ b h n t │\n",
        "    # │ c i o u │\n",
        "    # │ d j p v │\n",
        "    # │ e k q w │\n",
        "    # └ f l r x ┘.\n",
        "    # These columns are treated as \"independent\" by the model, which means that\n",
        "    # the dependence of 'g' on 'f' can not be learned, but allows more efficient\n",
        "    # batch processing. The view above will further be splitted into chunks\n",
        "    # of size `bptt_steps` to apply truncated BPTT. For example, with\n",
        "    # `bptt_steps == 2`, we'll have the following `x` and `y` tensors. The\n",
        "    # first batch will be processing \"a, b\" to predict \"b, c\", \n",
        "    # the second batch will be processing \"g, h\" to predict \"h, i\", and so on.\n",
        "    #\n",
        "    #       X          Y\n",
        "    #   ----->>------\n",
        "    #   |           |\n",
        "    # ┌ a g m s ┐ ┌ b h n t ┐\n",
        "    # └ b h n t ┘ └ c i o u ┘\n",
        "    #   |           |\n",
        "    #   ----->>------\n",
        "\n",
        "    # Work out how cleanly we can divide the dataset into batch_size parts.\n",
        "    n_batches = data_tensor.size(0) // batch_size\n",
        "\n",
        "    # Trim off the remainder tokens to evenly split\n",
        "    # Evenly divide the data across the batches\n",
        "    data = data_tensor[:n_batches * batch_size].view(\n",
        "        batch_size, n_batches).t().contiguous()\n",
        "\n",
        "    batches = []\n",
        "\n",
        "    for i in range(0, data.size(0) - 1, self.bptt_steps):\n",
        "      # seq_len can be less than bptt_steps in the final parts of the data\n",
        "      seq_len = min(self.bptt_steps, len(data) - i - 1)\n",
        "\n",
        "      # x shape => (seq_len, batch_size)\n",
        "      x = data[i: i + seq_len]\n",
        "      # flatten the ground-truth labels (shifted inputs for LM)\n",
        "      y = data[i + 1: i + 1 + seq_len].view(-1)\n",
        "      batches.append((x, y))\n",
        "\n",
        "    return batches\n",
        " \n",
        "  def train_model(self, optim, train_tensor, valid_tensor, test_tensor, n_epochs=5,\n",
        "                 batch_size=64):\n",
        "    \"\"\"Trains the model.\"\"\"\n",
        "    # Get batches for all splits at once\n",
        "    train_batches = self.get_batches(train_tensor, batch_size)\n",
        "    valid_batches = self.get_batches(valid_tensor, batch_size)\n",
        "    test_batches = self.get_batches(test_tensor, batch_size)\n",
        "\n",
        "    for eidx in range(1, n_epochs + 1):\n",
        "      start_time = time.time()\n",
        "      epoch_loss = 0\n",
        "      epoch_items = 0\n",
        "\n",
        "      # Enable training mode\n",
        "      self.train()\n",
        "\n",
        "      # Start training\n",
        "      for iter_count, (x, y) in enumerate(train_batches):\n",
        "        # Clear the gradients\n",
        "        optim.zero_grad()\n",
        "\n",
        "        loss = self.forward(x.to(DEVICE), y.to(DEVICE))\n",
        "\n",
        "        # Backprop the average loss and update parameters\n",
        "        loss.mean().backward()\n",
        "\n",
        "        # Clip the gradients to avoid exploding gradients\n",
        "        if self.clip_gradient_norm > 0:\n",
        "          torch.nn.utils.clip_grad_norm_(self.parameters(), self.clip_gradient_norm)\n",
        "\n",
        "        # Update parameters\n",
        "        optim.step()\n",
        "\n",
        "        # sum the loss for reporting, along with the denominator\n",
        "        epoch_loss += loss.detach().sum()\n",
        "        epoch_items += loss.numel()\n",
        "\n",
        "        if iter_count % 500 == 0:\n",
        "          # Print progress\n",
        "          loss_per_token = epoch_loss / epoch_items\n",
        "          ppl = math.exp(loss_per_token)\n",
        "          print(f'[Epoch {eidx:<3}] loss: {loss_per_token:6.2f}, perplexity: {ppl:6.2f}')\n",
        "\n",
        "      time_spent = time.time() - start_time\n",
        "\n",
        "      # Clear stale h_t history before evaluation\n",
        "      self.clear_hidden_states()\n",
        "\n",
        "      print(f'\\n[Epoch {eidx:<3}] ended with train_loss: {loss_per_token:6.2f}, ppl: {ppl:6.2f}')\n",
        "      # Evaluate on valid set\n",
        "      valid_loss, valid_ppl = self.evaluate(valid_batches)\n",
        "      print(f'[Epoch {eidx:<3}] ended with valid_loss: {valid_loss:6.2f}, valid_ppl: {valid_ppl:6.2f}')\n",
        "      print(f'[Epoch {eidx:<3}] completed in {time_spent:.2f} seconds\\n')\n",
        "\n",
        "    # Evaluate the final model on test set\n",
        "    test_loss, test_ppl = self.evaluate(test_batches)\n",
        "    print(f' ---> Final test set performance: {test_loss:6.2f}, test_ppl: {test_ppl:6.2f}')\n",
        "\n",
        "  def evaluate(self, batches):\n",
        "    # Clear stale h_t history before evaluation\n",
        "    self.clear_hidden_states()\n",
        "\n",
        "    # Switch to eval mode\n",
        "    self.eval()\n",
        "\n",
        "    total_loss = 0.\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for iter_count, (x, y) in enumerate(batches):\n",
        "        loss = self.forward(x.to(DEVICE), y.to(DEVICE))\n",
        "        \n",
        "        total_loss += loss.sum().item()\n",
        "        total_tokens += loss.size(0)\n",
        "    total_loss /= total_tokens\n",
        "\n",
        "    self.clear_hidden_states()\n",
        "    return total_loss, math.exp(total_loss)\n",
        "\n",
        "  def __repr__(self):\n",
        "    \"\"\"String representation for pretty-printing.\"\"\"\n",
        "    s = super(RNNLM, self).__repr__()\n",
        "    return \"{}\\n# of parameters: {}\".format(s, self.n_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beEYKqQNf6tj"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kzOvvY6f6tx"
      },
      "source": [
        "# Set the seed for reproducible results\n",
        "fix_seed(30494)\n",
        "\n",
        "rnnlm_model = RNNLM(\n",
        "    vocab_size=len(vocab),  # vocabulary size\n",
        "    emb_dim=128,            # word embedding dim\n",
        "    hid_dim=128,            # hidden layer dim\n",
        "    rnn_type='GRU',         # RNN type\n",
        "    n_layers=1,             # Number of stacked RNN layers\n",
        "    clip_gradient_norm=1.0, # gradient clip threshold\n",
        "    bptt_steps=35,          # Truncated BPTT window size\n",
        "    dropout=0.4,            # dropout probability\n",
        ")\n",
        "\n",
        "# move to device\n",
        "rnnlm_model.to(DEVICE)\n",
        "\n",
        "# Initial learning rate for the optimizer\n",
        "RNNLM_INIT_LR = 0.002\n",
        "\n",
        "# Create the optimizer\n",
        "rnnlm_optimizer = torch.optim.Adam(rnnlm_model.parameters(), lr=RNNLM_INIT_LR)\n",
        "print(rnnlm_model)\n",
        "\n",
        "print('Starting training!')\n",
        "# NOTE: If you happen to have memory errors, try decreasing the batch size\n",
        "rnnlm_model.train_model(\n",
        "    rnnlm_optimizer, train, valid, test, n_epochs=5, batch_size=16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIhbuAPkaMds"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "---\n",
        "\n",
        "**Q: How does the results compare to FFLMs? Feel free to play with different hyper-parameters here as well. Especially, try with different BPTT steps.**\n",
        "\n",
        "**Q: Play with hyper-parameters related to dimensions and dropout. Could you find a model with smaller perplexity?**\n",
        "\n",
        "**Q: There are missing parts related to LSTM support in the implementation. Try filling those parts and train an LSTM-based model. How does the performance compare to vanilla RNN and GRU? Compare model sizes of three variants with the default set of hyper-parameters.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbvkysROCmht"
      },
      "source": [
        "# More exercises\n",
        "\n",
        "**We do not provide any codes or answers for these extra questions. They are meant to motivate the interested ones to further go into the details of language modelling.**\n",
        "\n",
        "- Modify the `Vocabulary` class so that it knows about the counts of the tokens in the training set to apply a frequency threshold on the words that will be accepted to the vocabulary. Try with different thresholds such as 2, 3, 5 and train one of the models on top to see what is the impact of reducing the size of the vocabulary and eventually letting the model to learn an embedding for `<unk>`.\n",
        "\n",
        "- Implement a function to generate sentences from the RNNLM language model. How would you do that? There will not be any batches involved so you can directly feed the model with some prefix embeddings and sample (or take the word with the maximum probability, i.e. greedy search) from the output probability distribution.\n",
        "\n",
        "- Try implementing **weight tying** for RNNLM, which is an [approach](https://arxiv.org/abs/1608.05859) to reduce the number of parameters in sequence-to-sequence models. Notice that, we actually have 2 embeddings in the network: first is used to encode a given input (`self.emb`), second is the output layer! Yes the output layer is also a sort of embedding layer since it has a dimension of $V$ in its weight matrix. If you set the correct sizes in your network in a way that the embedding layer and the output layer has exactly the same sizes, you can let PyTorch share/tie those matrices, effectively removing one of them completely! The solution is actually a one-liner with PyTorch.  \n",
        "\n",
        "- Try taking one of the LSTM-papers below and implement one of the ideas that you like from there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq1fn-x9aS5e"
      },
      "source": [
        "# Further Reading\n",
        " - [Original FFLM paper from Bengio et al. 2003](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
        " - [Original RNNLM paper from Mikolov et al. 2010](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)\n",
        " - Some recent state-of-the-art LSTM-based RNNLMs\n",
        "\n",
        "  - [Regularizing and Optimizing LSTM Language Models](https://arxiv.org/pdf/1708.02182.pdf)\n",
        "  - [An Analysis of Neural Language Modeling at Multiple Scales](https://arxiv.org/pdf/1803.08240.pdf)\n",
        "  - [Scalable Language Modeling: WikiText-103 on a Single GPU in 12 hours](https://mlsys.org/Conferences/2019/doc/2018/50.pdf)"
      ]
    }
  ]
}