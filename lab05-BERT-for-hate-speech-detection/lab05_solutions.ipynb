{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "military-character",
      "metadata": {
        "id": "military-character"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EslAmsE-tWMB",
      "metadata": {
        "id": "EslAmsE-tWMB"
      },
      "source": [
        "In this lab, we will take you through a practical use of Transformers. This notebook shows you how to use [Hugging face](https://huggingface.co/)'s package to import and train pretrained models for the tasks of hate speech classification and machine translation.\n",
        "\n",
        "We first show you all necessay components to use the ``transformers`` package before asking you to implement some code in the later sections.\n",
        "\n",
        "\n",
        "**Note:** The training of models will take quite some time so make sure to run this session with the GPU enabled. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rwXjbNUJHzZ0",
      "metadata": {
        "id": "rwXjbNUJHzZ0"
      },
      "source": [
        "## Setting up the Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Vina6dhaHz1N",
      "metadata": {
        "id": "Vina6dhaHz1N"
      },
      "source": [
        "First, we need to install Hugging Face [transformers](https://huggingface.co/transformers/index.html) and [Sentence piece Tokenizers](https://github.com/google/sentencepiece) with the following commands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "other-scottish",
      "metadata": {
        "id": "other-scottish"
      },
      "outputs": [],
      "source": [
        "#! pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "modern-olympus",
      "metadata": {
        "id": "modern-olympus"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install ipywidgets\n",
        "!jupyter nbextension enable --py widgetsnbextension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "developing-france",
      "metadata": {
        "id": "developing-france"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertPreTrainedModel, BertModel\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "  print('WARNING: You may want to change the runtime to GPU for faster training!')\n",
        "  DEVICE = 'cpu'\n",
        "else:\n",
        "  DEVICE = 'cuda:0'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58jPlYaWk7fh",
      "metadata": {
        "id": "58jPlYaWk7fh"
      },
      "source": [
        "If you work in Colab, mount your google drive to save models and training checkpoints. Run the following code to connect your google drive to colab. Click on the link and copy and past the code you saw into the input box."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qjdWhQHBk6RW",
      "metadata": {
        "id": "qjdWhQHBk6RW"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd '/content/drive/MyDrive/Colab Notebooks/'\n",
        "%mkdir './Lab 7'\n",
        "%cd './Lab 7' "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qJ-jH0JoVG1v",
      "metadata": {
        "id": "qJ-jH0JoVG1v"
      },
      "source": [
        "# Hate Speech Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cognitive-syndicate",
      "metadata": {
        "id": "cognitive-syndicate"
      },
      "source": [
        "## Downloading the dataset\n",
        "\n",
        "For the task of hate speech classification, we will work with the [Offensive Language Identification Dataset - OLID ](https://scholar.harvard.edu/malmasi/olid). It is a dataset of tweets hierarchically annotated on three levels: \n",
        "\n",
        "* Level A: Offensive Language Detection\n",
        "* Level B: Categorization of Offensive Language\n",
        "* Level C: Offensive Language Target Identification\n",
        "\n",
        "\n",
        "Let's download it first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eISPHZ-7HwBk",
      "metadata": {
        "id": "eISPHZ-7HwBk"
      },
      "outputs": [],
      "source": [
        "%mkdir ./data\n",
        "%cd ./data\n",
        "\n",
        "if not os.path.isfile('pretrain.txt'): \n",
        "  !wget -O pretrain.txt https://www.dropbox.com/s/bavjtyx0ndty7xt/pretrain.txt?dl=0\n",
        "\n",
        "if not os.path.isfile('OLIDv1.0.zip'): \n",
        "  !wget -O OLIDv1.0.zip https://sites.google.com/site/offensevalsharedtask/olid/OLIDv1.0.zip?attredirects=0&d=1\n",
        "  ! unzip OLIDv1.0.zip\n",
        "  \n",
        "%cd ..\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TsIt3OVmNnRY",
      "metadata": {
        "id": "TsIt3OVmNnRY"
      },
      "source": [
        "Let's have a look at the data we downloaded.\n",
        "\n",
        "As mentioned above, the ``OLID`` dataset has been labeled for three subtask, therefore we have three different labels sets per tweet: \n",
        "* Task A: Not Offensive (``NOT``) and Offensive (``OFF``).\n",
        "* Task B: Targeted Insult (``TIN``), Untargeted (``UNT``) and ``NULL`` for not offensive tweets.\n",
        "* Task C: Individual (``IND``), Group (``GRP``), Other (``OTH``) and ``NULL`` for not offensive and non targeted tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rFwKBesVNmmr",
      "metadata": {
        "id": "rFwKBesVNmmr"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('./data/olid-training-v1.0.tsv',delimiter=\"\\t\")\n",
        "\n",
        "print(f'Number of training samples: {len(df)}')\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "published-southeast",
      "metadata": {
        "id": "published-southeast"
      },
      "source": [
        "## Loading and preprocessing the corpus \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nw6m7B6rEL6E",
      "metadata": {
        "id": "nw6m7B6rEL6E"
      },
      "source": [
        "Let's define ``reader_train`` and ``reader_test`` that will prepare our data corpus and labels for both train and test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "correct-tuner",
      "metadata": {
        "id": "correct-tuner"
      },
      "outputs": [],
      "source": [
        "def reader_train(file_name):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    fin = open(file_name)\n",
        "    title = fin.readline()\n",
        "    set_a = ['NOT' , 'OFF']\n",
        "    set_b = ['NULL', 'TIN', 'UNT']\n",
        "    set_c = ['NULL', 'IND', 'GRP', 'OTH']\n",
        "    while True:\n",
        "        line = fin.readline()\n",
        "        if not line:\n",
        "            break\n",
        "        items = line.split('\\t')\n",
        "        text = items[1]\n",
        "        label_a = set_a.index(items[2].strip())\n",
        "        label_b = set_b.index(items[3].strip())\n",
        "        label_c = set_c.index(items[4].strip())\n",
        "\n",
        "        if len(text) > 0:\n",
        "            texts.append(text)\n",
        "            labels.append([label_a, label_b, label_c])\n",
        "            \n",
        "    return {'texts':texts, 'labels':labels}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "designed-screening",
      "metadata": {
        "id": "designed-screening"
      },
      "outputs": [],
      "source": [
        "def reader_test(test_textlist, test_labellist):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    text_dict = {}\n",
        "    \n",
        "    # build text_dict\n",
        "    for file_text in test_textlist:\n",
        "        fin = open(file_text)\n",
        "        title = fin.readline()\n",
        "        while True:\n",
        "            line = fin.readline()\n",
        "            if not line:\n",
        "                break\n",
        "            items = line.split('\\t')\n",
        "            if items[0] not in text_dict:\n",
        "                text_dict[items[0]] = items[1]\n",
        "        fin.close()\n",
        "    label_dict_list = []\n",
        "    \n",
        "    # build label_dict\n",
        "    for i, file_label in enumerate(test_labellist):\n",
        "        label_dict_list.append({})\n",
        "        fin = open(file_label)\n",
        "        title = fin.readline()\n",
        "        while True:\n",
        "            line = fin.readline()\n",
        "            if not line:\n",
        "                break\n",
        "            items = line.split(',')\n",
        "            label_dict_list[i][items[0]] = items[1]\n",
        "        fin.close()    \n",
        "    \n",
        "    set_a = ['NOT' , 'OFF']\n",
        "    set_b = ['NULL', 'TIN', 'UNT']\n",
        "    set_c = ['NULL', 'IND', 'GRP', 'OTH']\n",
        "    \n",
        "    for idx, text in text_dict.items():\n",
        "        if len(text) > 0:\n",
        "            texts.append(text)\n",
        "            if idx in label_dict_list[0]:\n",
        "                label_a = label_dict_list[0][idx]\n",
        "            else:\n",
        "                label_a = 'OFF'\n",
        "            if idx in label_dict_list[1]:\n",
        "                label_b = label_dict_list[1][idx]\n",
        "            else:\n",
        "                label_b = 'NULL'\n",
        "            if idx in label_dict_list[2]:\n",
        "                label_c = label_dict_list[2][idx]\n",
        "            else:\n",
        "                label_c = 'NULL'\n",
        "            \n",
        "            label_a = set_a.index(label_a.strip())\n",
        "            label_b = set_b.index(label_b.strip())\n",
        "            label_c = set_c.index(label_c.strip())\n",
        "        \n",
        "            labels.append([label_a, label_b, label_c])\n",
        "            \n",
        "    return {'texts':texts, 'labels':labels}            \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T4kLz8nca7ix",
      "metadata": {
        "id": "T4kLz8nca7ix"
      },
      "source": [
        "We also define our custom ``OlidDataset`` class which allows us to control how we handle the iteration and batches.\n",
        "\n",
        "At each iteration over the dataset object, the function ``__get_item__`` is called and returns a list of dictionnaries with the tweets and their 3 labels. \n",
        "Then, the ``collate_fn`` function will process the list of samples into their encodings and return a batch when called by the iterator during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "amber-exposure",
      "metadata": {
        "id": "amber-exposure"
      },
      "outputs": [],
      "source": [
        "class OlidDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, tokenizer, input_set):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.texts = input_set['texts']\n",
        "        self.labels = input_set['labels']\n",
        "        \n",
        "    def collate_fn(self, batch):\n",
        "\n",
        "        texts = []\n",
        "        labels_a = []\n",
        "        labels_b = []\n",
        "        labels_c = []\n",
        "\n",
        "        for b in batch:\n",
        "            texts.append(b['text'])\n",
        "            labels_a.append(b['label_a'])\n",
        "            labels_b.append(b['label_b'])\n",
        "            labels_c.append(b['label_c'])\n",
        "\n",
        "        #The maximum sequence size for BERT is 512 but here the tokenizer truncate sentences longer than 128 tokens.  \n",
        "        # We also pad shorter sentences to a length of 128 tokens\n",
        "        encodings = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
        "        labels = {}\n",
        "        encodings['label_a'] =  torch.tensor(labels_a)\n",
        "        encodings['label_b'] =  torch.tensor(labels_b)\n",
        "        encodings['label_c'] =  torch.tensor(labels_c)\n",
        "        \n",
        "        return encodings\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "       \n",
        "        item = {'text': self.texts[idx],\n",
        "                'label_a': self.labels[idx][0],\n",
        "                'label_b': self.labels[idx][1],\n",
        "                'label_c': self.labels[idx][2]}\n",
        "        return item"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66CvfwkHbXO_",
      "metadata": {
        "id": "66CvfwkHbXO_"
      },
      "source": [
        "\n",
        "Now let's put it all together and load our data. Here we use a pre-made tokenizer that was used for our BERT model. Here we pick the pre-trained model ``bert-base-cased``. There are several other models of various sizes (base, large).\n",
        "\n",
        "**Note:** ``bert-base-cased`` is case-sensitive and it differenciates English from english. An non case-sensitive variant is ``bert-base-uncased``.\n",
        "\n",
        "You can always use another [tokenizer](https://huggingface.co/transformers/main_classes/tokenizer.html), but we will get better results using the same tokenizer as the one used to pre-train the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nHzy19X9TUqc",
      "metadata": {
        "id": "nHzy19X9TUqc"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "# we can check the parameters of this tokenizer\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deluxe-biography",
      "metadata": {
        "id": "deluxe-biography"
      },
      "outputs": [],
      "source": [
        "trainset = reader_train('./data/olid-training-v1.0.tsv')\n",
        "testset = reader_test(['./data/testset-levela.tsv','./data/testset-levelb.tsv','./data/testset-levelc.tsv'], \n",
        "                      ['./data/labels-levela.csv','./data/labels-levelb.csv','./data/labels-levelc.csv'])\n",
        "\n",
        "train_dataset = OlidDataset(tokenizer, trainset)\n",
        "test_dataset = OlidDataset(tokenizer, testset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oxEXetnGKVj2",
      "metadata": {
        "id": "oxEXetnGKVj2"
      },
      "source": [
        "The following code let's you play around with our ``train_dataset`` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1Q2VvuoaKFXB",
      "metadata": {
        "id": "1Q2VvuoaKFXB"
      },
      "outputs": [],
      "source": [
        "#returns first item as dictionnary\n",
        "#print(train_dataset[0])\n",
        "\n",
        "# put all train set into one batch for the collate_fn function\n",
        "batch = [sample for sample in train_dataset]\n",
        "\n",
        "encodings = train_dataset.collate_fn(batch[:10])\n",
        "\n",
        "for key, value in encodings.items():\n",
        "  print(f\"{key}: {value.numpy().tolist()}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "opponent-closure",
      "metadata": {
        "id": "opponent-closure"
      },
      "source": [
        "## Finetuning a pre-trained BERT model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dpaMPoB5cNwt",
      "metadata": {
        "id": "dpaMPoB5cNwt"
      },
      "source": [
        "\n",
        "As you can recall from the lecture, BERT is a model trained on Masked language Modeling(MLM) and Next Sentence Prediction(NSP), however is not trained to do to do sentence classification. We then need to adapt it for hate speech classification and finetune the pre-trained model on our dataset.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n2zPF-H3nHDH",
      "metadata": {
        "id": "n2zPF-H3nHDH"
      },
      "source": [
        "Let's have a look at ``bert_base-uncased`` summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pk_QEpoGnZhL",
      "metadata": {
        "id": "pk_QEpoGnZhL"
      },
      "outputs": [],
      "source": [
        "model = BertModel.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "#180 M\n",
        "print(f\"Model size: {model.num_parameters()}\")\n",
        "\n",
        "#model summary\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GBOHvhd-n00L",
      "metadata": {
        "id": "GBOHvhd-n00L"
      },
      "source": [
        "Note that the model has only encoder layers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ESBsGVb_e2QT",
      "metadata": {
        "id": "ESBsGVb_e2QT"
      },
      "source": [
        "### BERT Model\n",
        "\n",
        "To define our model, we will build on top of a Huggingface pre-trained model and adapt it to our task. We will use ``BertModel`` to extract embeddings and add a ``Linear`` layer to classify samples. Hugging face implementation of BERT can handle different variations of the model, which we define and pass its parameter values via``config``.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feb9AN0YFWDe",
      "metadata": {
        "id": "feb9AN0YFWDe"
      },
      "source": [
        "The code below defines a model adapted to classify tweets on Level A, Offensive Language Detection. We will implement Task B and C later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bottom-tribute",
      "metadata": {
        "id": "bottom-tribute"
      },
      "outputs": [],
      "source": [
        "class BERT_hate_speech(BertPreTrainedModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        # BERT Model\n",
        "        self.bert = BertModel(config)\n",
        "        \n",
        "        # Task A\n",
        "        self.projection_a = torch.nn.Sequential(torch.nn.Dropout(0.2),\n",
        "                                                torch.nn.Linear(config.hidden_size, 2))\n",
        "        \n",
        "        # Task B\n",
        "        # TBA\n",
        "        \n",
        "        # Task C\n",
        "        # TBA\n",
        "        \n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None):\n",
        " \n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        # Logits A\n",
        "        logits_a = self.projection_a(outputs[1])\n",
        "        \n",
        "        return logits_a\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yMyL5gGKfECs",
      "metadata": {
        "id": "yMyL5gGKfECs"
      },
      "source": [
        "### Finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QaKWwqYMlCVX",
      "metadata": {
        "id": "QaKWwqYMlCVX"
      },
      "source": [
        "Finally, we should define our training loop. Fortunately, the ``transformers`` package provides us with a [``Trainer``](https://huggingface.co/transformers/main_classes/trainer.html#transformers.Trainer) class wich takes care of the training of transformers models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MjljHz4nlAbJ",
      "metadata": {
        "id": "MjljHz4nlAbJ"
      },
      "source": [
        "We build our custom ``Trainer`` class to incorporate our own ``compute_loss`` function over the three labels. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "engaged-perspective",
      "metadata": {
        "id": "engaged-perspective"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Trainer_hate_speech(Trainer):\n",
        "    def compute_loss(self, model, inputs):\n",
        "        labels = {}\n",
        "        labels['label_a'] = inputs.pop('label_a')\n",
        "        labels['label_b'] = inputs.pop('label_b')\n",
        "        labels['label_c'] = inputs.pop('label_c')\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # TASK A\n",
        "        loss_task_a = nn.CrossEntropyLoss()\n",
        "        labels_a = labels['label_a']\n",
        "        loss_a = loss_task_a(outputs.view(-1, 2), labels_a.view(-1))\n",
        "\n",
        "        loss = loss_a\n",
        "        \n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p0WDX456F4YB",
      "metadata": {
        "id": "p0WDX456F4YB"
      },
      "source": [
        "\n",
        "Now let's finetune the pretrained model on our ``OlidDataset``.\n",
        "\n",
        "In our function ``main_hate_speech``we define the arguments for the ``Trainer`` object and launch the training with ``trainer.train``. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "killing-population",
      "metadata": {
        "id": "killing-population"
      },
      "outputs": [],
      "source": [
        "def main_hate_speech():\n",
        "\n",
        "    #call our custom BERT model and pass as parameter the name of an available pretrained model\n",
        "    model = BERT_hate_speech.from_pretrained(\"bert-base-cased\")\n",
        "    \n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./experiment/hate_speech',\n",
        "        learning_rate = 0.0001,\n",
        "        logging_steps= 100,\n",
        "        per_device_train_batch_size=32,\n",
        "        num_train_epochs = 3,\n",
        "    )\n",
        "    trainer = Trainer_hate_speech(\n",
        "        model=model,                         \n",
        "        args=training_args,                 \n",
        "        train_dataset=train_dataset,                   \n",
        "        data_collator=train_dataset.collate_fn\n",
        "    )\n",
        "    trainer.train()\n",
        "\n",
        "    trainer.save_model('./models/ht_bert_finetuned/')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OanfT22IduY6",
      "metadata": {
        "id": "OanfT22IduY6"
      },
      "source": [
        "Let's run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "looking-escape",
      "metadata": {
        "id": "looking-escape"
      },
      "outputs": [],
      "source": [
        "main_hate_speech()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6Q0sYQeoM90U",
      "metadata": {
        "id": "6Q0sYQeoM90U"
      },
      "source": [
        "### Evaluation\n",
        "Once we trained our model, we can evaluate it on our test set."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tt8KRJs2OSo1",
      "metadata": {
        "id": "tt8KRJs2OSo1"
      },
      "source": [
        "Let's define a helper function ``predict_hatespeech`` that will extract the predicted label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "leJGKWvXGDf7",
      "metadata": {
        "id": "leJGKWvXGDf7"
      },
      "outputs": [],
      "source": [
        "def predict_hatespeech(input, tokenizer, model): \n",
        "  model.eval()\n",
        "  encodings = tokenizer(input, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
        "  \n",
        "  output = model(**encodings)\n",
        "  preds = torch.max(output, 1)\n",
        "\n",
        "  return {'prediction':preds[1], 'confidence':preds[0]}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_y2mgB3-PQpI",
      "metadata": {
        "id": "_y2mgB3-PQpI"
      },
      "source": [
        "Now let's define a function that will evaluate our model on the test set we prepared."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2olq7egUHYnH",
      "metadata": {
        "id": "2olq7egUHYnH"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, tokenizer, data_loader):\n",
        "\n",
        "  total_count = 0\n",
        "  correct_count = 0 \n",
        "\n",
        "  preds = []\n",
        "  tot_labels = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data in tqdm(data_loader): \n",
        "\n",
        "      labels = {}\n",
        "      labels['label_a'] = data['label_a']\n",
        "\n",
        "      tweets = data['text']\n",
        "\n",
        "      pred = predict_hatespeech(tweets, tokenizer, model)\n",
        "\n",
        "      preds.append(pred['prediction'].tolist())\n",
        "      tot_labels.append(labels['label_a'].tolist())\n",
        "\n",
        "  # with the saved predictions and labels we can compute accuracy, precision, recall and f1-score\n",
        "  report = classification_report(tot_labels, preds, target_names=[\"Not offensive\",\"Offensive\"], output_dict= True)\n",
        "\n",
        "  return report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Sps-kXvWeNoO",
      "metadata": {
        "id": "Sps-kXvWeNoO"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "#your saved model name here\n",
        "model_name = './models/ht_bert_finetuned/' \n",
        "model = BERT_hate_speech.from_pretrained(model_name)\n",
        "\n",
        "# we don't batch our test set unless it's too big\n",
        "test_loader = DataLoader(test_dataset)\n",
        "\n",
        "report = evaluate(model, tokenizer, test_loader)\n",
        "\n",
        "print(report)\n",
        "\n",
        "print(report['accuracy'])\n",
        "print(report['Not offensive']['f1-score'])\n",
        "print(report['Offensive']['f1-score'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g9eJi1IaOGZ6",
      "metadata": {
        "id": "g9eJi1IaOGZ6"
      },
      "source": [
        "Let's test our model on a few sentences to get an intuition. Feel free to play around."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49umxeBiK3Ec",
      "metadata": {
        "id": "49umxeBiK3Ec"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "model = BERT_hate_speech.from_pretrained('./models/ht_bert_finetuned/')\n",
        "\n",
        "print(predict_hatespeech(\"I go see pinguins at the zoo.\", tokenizer, model))\n",
        "print(predict_hatespeech(\"Bananas are stupid\", tokenizer, model))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JaD0CmXMhEmA",
      "metadata": {
        "id": "JaD0CmXMhEmA"
      },
      "source": [
        "## Pre-training and finetuning BERT\n",
        "\n",
        "In this section, we will implement our own masked language modeling (MLM) training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KmT9ztohfiW3",
      "metadata": {
        "id": "KmT9ztohfiW3"
      },
      "source": [
        "### Pre-training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9-mAsIUYZ7Ex",
      "metadata": {
        "id": "9-mAsIUYZ7Ex"
      },
      "source": [
        "**Question 1: Add MLM head for pretraining**\n",
        "Your task is to fill in the following classes to implement MLM training: \n",
        "\n",
        "* ``PretrainDataset()``\n",
        "* ``Trainer_MLM()``\n",
        "* ``BERT_pretrain()``\n",
        "* ``main_pretrain()``"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C-3Jvmryw-ko",
      "metadata": {
        "id": "C-3Jvmryw-ko"
      },
      "source": [
        "To train our model in a MLM fashion, we need to make some adjustment to our ``Dataset`` class. We want to train BERT to predict an X% of tokens (in the original paper it is 15%) of which 80% will be replaced by a ``[MASK]`` token, 10% with a random token and 10% remain the same token.\n",
        "\n",
        "We introduce the function ``mask_tokens`` that will take care of that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "differential-ordinary",
      "metadata": {
        "id": "differential-ordinary"
      },
      "outputs": [],
      "source": [
        "class PretrainDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, tokenizer, input_file):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.texts = self.read_text(input_file)\n",
        "\n",
        "        self.mlm_probability = 0.15\n",
        "        \n",
        "    def read_text(self, input_file):\n",
        "\n",
        "        ## Question 1 ##\n",
        "\n",
        "        fin = open(input_file)\n",
        "        return fin.readlines()\n",
        "        \n",
        "    def collate_fn(self, batch):\n",
        "       \n",
        "        ## Question 1 ##\n",
        "\n",
        "        batch = self.tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
        "\n",
        "        inputs, labels = self.mask_tokens(batch[\"input_ids\"])\n",
        "        return {\"input_ids\": inputs, \"labels\": labels}\n",
        "    \n",
        "        return encodings\n",
        "    \n",
        "    def mask_tokens(self, inputs):\n",
        "        \"\"\"\n",
        "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
        "        \"\"\"\n",
        "        if self.tokenizer.mask_token is None:\n",
        "            raise ValueError(\n",
        "                \"This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the --mlm flag if you want to use this tokenizer.\"\n",
        "            )\n",
        "        labels = inputs.clone()\n",
        "\n",
        "        # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
        "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
        "        special_tokens_mask = [\n",
        "            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
        "        ]\n",
        "        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
        "        \n",
        "        if self.tokenizer._pad_token is not None:\n",
        "            padding_mask = labels.eq(self.tokenizer.pad_token_id)\n",
        "            probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
        "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
        "\n",
        "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
        "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
        "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
        "\n",
        "        # 10% of the time, we replace masked input tokens with random word\n",
        "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
        "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
        "        inputs[indices_random] = random_words[indices_random]\n",
        "\n",
        "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
        "        return inputs, labels\n",
        "    \n",
        "    def __len__(self):\n",
        "        \n",
        "        ## Question 1 ##\n",
        "\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        ## Question 1 ##\n",
        " \n",
        "        text = self.texts[idx]\n",
        "        return text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-4yxeOG1Z94H",
      "metadata": {
        "id": "-4yxeOG1Z94H"
      },
      "source": [
        "The next step is to add a MLM head to our model. \n",
        "Use the ``BertOnlyMLMHead`` to add a MLM classifier to BERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beginning-fluid",
      "metadata": {
        "id": "beginning-fluid"
      },
      "outputs": [],
      "source": [
        "from transformers.models.bert.modeling_bert import BertOnlyMLMHead\n",
        "\n",
        "class BERT_pretrain(BertPreTrainedModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.config = config\n",
        "\n",
        "        ## Question 1 ##\n",
        "        # BERT Model\n",
        "        self.bert = BertModel(config)\n",
        "        \n",
        "        \n",
        "        ## Question 1 ##\n",
        "        # MLM head\n",
        "        self.cls = BertOnlyMLMHead(config)\n",
        "        \n",
        "        \n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None):\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        ## Question 1 ##\n",
        "\n",
        "        # MLM output\n",
        "        prediction_scores = self.cls(outputs[0])\n",
        "        \n",
        "        return prediction_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46_QOzQfLgME",
      "metadata": {
        "id": "46_QOzQfLgME"
      },
      "source": [
        "We will define a new Trainer class for pre-training. \n",
        "\n",
        "**Note:** We could use the standard ``Trainer`` class to train our model. Then we would need to make ``BERT_pretrain`` output  ``loss`` and BERT ``outputs`` as a tuple``(loss, outputs)``.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xoG-AvUdzyqt",
      "metadata": {
        "id": "xoG-AvUdzyqt"
      },
      "outputs": [],
      "source": [
        "class Trainer_MLM(Trainer):\n",
        "    def compute_loss(self, model, inputs):\n",
        "        \n",
        "        labels = inputs['labels']\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # MLM loss\n",
        "        lm_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "        loss_mlm = lm_loss(outputs.view(-1, model.config.vocab_size), labels.view(-1))\n",
        "        \n",
        "        loss = loss_mlm\n",
        "        \n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q3xGoHTSh-Yu",
      "metadata": {
        "id": "q3xGoHTSh-Yu"
      },
      "source": [
        "Finally, put everything together in the ``main_pretrain()`` class. \n",
        "\n",
        "In the code below, write code to pre-train your custom MLM model on ``pretrain.txt`` file found in the ``data`` folder.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "familiar-singles",
      "metadata": {
        "id": "familiar-singles"
      },
      "outputs": [],
      "source": [
        "def main_pretrain():\n",
        "    \n",
        "    ## Question 1 ##\n",
        "\n",
        "    model = BERT_pretrain.from_pretrained(\"bert-base-cased\")\n",
        "    \n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "    pretrain_dataset = PretrainDataset(tokenizer, 'data/pretrain.txt')\n",
        "    \n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./experiment/pretrain',\n",
        "        learning_rate = 0.00005,\n",
        "        num_train_epochs =1,\n",
        "        save_steps = 10000,  #saves a checkpoint file every 10000 iterations\n",
        "        per_device_train_batch_size=64,\n",
        "    )\n",
        "    trainer = Trainer_MLM(\n",
        "        model=model,                         \n",
        "        args=training_args,                 \n",
        "        train_dataset=pretrain_dataset,                    \n",
        "        data_collator=pretrain_dataset.collate_fn\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    \n",
        "    trainer.save_model('./models/ht_bert_pretrained/')\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4-tsbImSJvyf",
      "metadata": {
        "id": "4-tsbImSJvyf"
      },
      "source": [
        "Running the pretraining will take ~ 2 hours with one epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "greenhouse-signal",
      "metadata": {
        "id": "greenhouse-signal"
      },
      "outputs": [],
      "source": [
        " main_pretrain()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "incorporated-sleep",
      "metadata": {
        "id": "incorporated-sleep"
      },
      "source": [
        "### Finetuning\n",
        "\n",
        "**Question 2: Load the pretrained model for finetuning**\n",
        "\n",
        "In the code below modify the ``main_hate_speech`` function from earlier to import the model we just trained, and finetune it on our ``OlidDataset`` train sets.\n",
        "\n",
        "**Note**: Your pre-trained model is saved as checkpoint files in your ``output_dir`` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "metropolitan-cliff",
      "metadata": {
        "id": "metropolitan-cliff"
      },
      "outputs": [],
      "source": [
        "def main_hate_speech():\n",
        "\n",
        "    ## Question 2 ##\n",
        "\n",
        "    model = BERT_hate_speech.from_pretrained(\"./models/ht_bert_pretrained/\")\n",
        "    \n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./experiment/hate_speech',\n",
        "        learning_rate = 0.0001,\n",
        "        logging_steps= 500,\n",
        "        per_device_train_batch_size=32,\n",
        "        num_train_epochs = 1\n",
        "    )\n",
        "    trainer = Trainer_hate_speech(\n",
        "        model=model,                         \n",
        "        args=training_args,                 \n",
        "        train_dataset=train_dataset,        \n",
        "        eval_dataset=test_dataset,             \n",
        "        data_collator=train_dataset.collate_fn\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    trainer.save_model('./models/ht_bert_pretrained_finetuned/')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "simplified-exemption",
      "metadata": {
        "id": "simplified-exemption"
      },
      "outputs": [],
      "source": [
        "main_hate_speech()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8eqtu-ECSaUO",
      "metadata": {
        "id": "8eqtu-ECSaUO"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qCC3iV37M7np",
      "metadata": {
        "id": "qCC3iV37M7np"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "#your saved model name here\n",
        "model_name = './models/ht_bert_pretrained_finetuned/' \n",
        "model = BERT_hate_speech.from_pretrained(model_name)\n",
        "\n",
        "test_loader = DataLoader(test_dataset)\n",
        "\n",
        "report = evaluate(model, tokenizer, test_loader)\n",
        "print(report)\n",
        "print(report['accuracy'])\n",
        "print(report['Not offensive']['f1-score'])\n",
        "print(report['Offensive']['f1-score'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "existing-courtesy",
      "metadata": {
        "id": "existing-courtesy"
      },
      "source": [
        "## Multi-task Hate Speech Classification\n",
        "\n",
        "It's time to add the two other tasks to our implementation of ``BERT_hate_speech()``.\n",
        "\n",
        "**Question 3: Add multi-heads (task b, task c) for multi-task hatespeech classification**\n",
        "\n",
        "Fill in the missing code from the following classes:\n",
        "\n",
        "* ``BERT_hate_speech_multitask()``\n",
        "* `` Trainer_hate_speech_multitask()``\n",
        "* ``main_hate_speech_multitask()``"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ogHc8w4Kyew",
      "metadata": {
        "id": "9ogHc8w4Kyew"
      },
      "source": [
        "### Multi-task Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "personalized-teddy",
      "metadata": {
        "id": "personalized-teddy"
      },
      "outputs": [],
      "source": [
        "\n",
        "class BERT_hate_speech_multitask(BertPreTrainedModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        \n",
        "        # BERT Model\n",
        "        self.bert = BertModel(config)\n",
        "        \n",
        "        # Task A\n",
        "        self.projection_a = torch.nn.Sequential(torch.nn.Dropout(0.2),\n",
        "                                                torch.nn.Linear(config.hidden_size, 2))\n",
        "        \n",
        "        ##  Question 3 ##\n",
        "\n",
        "        # Task B\n",
        "        self.projection_b = torch.nn.Sequential(torch.nn.Dropout(0.2),\n",
        "                                                torch.nn.Linear(config.hidden_size, 3))\n",
        "\n",
        "        # Task C\n",
        "        self.projection_c = torch.nn.Sequential(torch.nn.Dropout(0.2),\n",
        "                                                torch.nn.Linear(config.hidden_size, 4))\n",
        "        \n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None):\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        # Task A\n",
        "        logits_a = self.projection_a(outputs[1])\n",
        "        \n",
        "        ##  Question 3 ##\n",
        "        \n",
        "        # Task B\n",
        "        logits_b = self.projection_b(outputs[1])\n",
        "      \n",
        "        # Task C \n",
        "        logits_c = self.projection_c(outputs[1])\n",
        "\n",
        "        return (logits_a, logits_b, logits_c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lwCad-uo1pnP",
      "metadata": {
        "id": "lwCad-uo1pnP"
      },
      "outputs": [],
      "source": [
        "class Trainer_hate_speech_multitask(Trainer):\n",
        "    def compute_loss(self, model, inputs):\n",
        "        labels = {}\n",
        "        labels['label_a'] = inputs.pop('label_a')\n",
        "        labels['label_b'] = inputs.pop('label_b')\n",
        "        labels['label_c'] = inputs.pop('label_c')\n",
        "\n",
        "        (out_a, out_b, out_c) = model(**inputs)\n",
        "\n",
        "        # LOSS A\n",
        "        loss_task_a = nn.CrossEntropyLoss()\n",
        "        labels_a = labels['label_a']\n",
        "        loss_a = loss_task_a(out_a.view(-1, 2), labels_a.view(-1))\n",
        "\n",
        "        ## QUESTION 3 ##        \n",
        "        # LOSS B\n",
        "        loss_task_b = nn.CrossEntropyLoss()\n",
        "        labels_b = labels['label_b']\n",
        "        loss_b = loss_task_b(out_b.view(-1, 3), labels_b.view(-1))\n",
        "\n",
        "        # LOSS C\n",
        "        loss_task_c = nn.CrossEntropyLoss()\n",
        "        labels_c = labels['label_c']\n",
        "        loss_c = loss_task_c(out_c.view(-1, 4), labels_c.view(-1))\n",
        "\n",
        "        loss = loss_a + loss_b + loss_c\n",
        "        \n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LSA-HdbWjpAl",
      "metadata": {
        "id": "LSA-HdbWjpAl"
      },
      "source": [
        "Just as in the finetuning task, instantiate a ``BERT_hate_speech_multitask`` model from an pre-trained model and finetune it on our ``train_dataset``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "satisfied-short",
      "metadata": {
        "id": "satisfied-short"
      },
      "outputs": [],
      "source": [
        "def main_hate_speech_multitask():\n",
        "    ##  Question 3 ##\n",
        "\n",
        "    model = BERT_hate_speech_multitask.from_pretrained(\"bert-base-cased\")\n",
        "    \n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./experiment/hate_speech_multitask',\n",
        "        learning_rate = 0.0001,\n",
        "        logging_steps= 100,\n",
        "        num_train_epochs = 3,\n",
        "        per_device_train_batch_size=64,\n",
        "    )\n",
        "    trainer = Trainer_hate_speech_multitask(\n",
        "        model=model,                         \n",
        "        args=training_args,                 \n",
        "        train_dataset=train_dataset,                 \n",
        "        data_collator=train_dataset.collate_fn\n",
        "    )\n",
        "    trainer.train()\n",
        "\n",
        "    trainer.save_model('./models/ht_bert_multi_finetuned/')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-P1ekYHaqPwR",
      "metadata": {
        "id": "-P1ekYHaqPwR"
      },
      "source": [
        "Running the code below should take ~10 min for 3 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wrapped-trading",
      "metadata": {
        "id": "wrapped-trading"
      },
      "outputs": [],
      "source": [
        "main_hate_speech_multitask()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p2EFyb3rKtVB",
      "metadata": {
        "id": "p2EFyb3rKtVB"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8EtaoYYGJ6J2",
      "metadata": {
        "id": "8EtaoYYGJ6J2"
      },
      "outputs": [],
      "source": [
        "def predict_hatespeech_multitask(input, tokenizer, model): \n",
        "  model.eval()\n",
        "  encodings = tokenizer(input, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
        "  \n",
        "  (out1, out2, out3) = model(**encodings)\n",
        "  \n",
        "  preds_a = torch.max(out1, 1)\n",
        "  preds_b = torch.max(out2, 1)\n",
        "  preds_c = torch.max(out3, 1)\n",
        "\n",
        "  preds = (preds_a[1], preds_b[1], preds_c[1])\n",
        "  scores = (preds_a[0], preds_b[0], preds_c[0])\n",
        "\n",
        "  return {'predictions':preds, 'confidences':scores}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "z5pIqvWoLbDy",
      "metadata": {
        "id": "z5pIqvWoLbDy"
      },
      "outputs": [],
      "source": [
        "def evaluate_multitask(model, tokenizer, data_loader):\n",
        "\n",
        "  task_num = 3\n",
        "  total_count = 0\n",
        "  correct_count = [0] * task_num  \n",
        "  accuracies = [0] * task_num\n",
        "\n",
        "  batch_size = data_loader.batch_size\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data in tqdm(data_loader): \n",
        "\n",
        "      labels = {}\n",
        "      labels['label_a'] = data['label_a']\n",
        "      labels['label_b'] = data['label_b']\n",
        "      labels['label_c'] = data['label_c']\n",
        "\n",
        "      tweets = data['text']\n",
        "\n",
        "      pred = predict_hatespeech_multitask(tweets, tokenizer, model)\n",
        "\n",
        "      preds = pred['predictions'] \n",
        "\n",
        "      for i, label in enumerate(labels):\n",
        "        correct_count[i]+= torch.mean((preds[i] == labels[label]).float())\n",
        "\n",
        "      total_count += np.float(batch_size)\n",
        "\n",
        "    for i, label in enumerate(labels):\n",
        "      accuracies[i] = (correct_count[i]/total_count)\n",
        "\n",
        " \n",
        "  return accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P24QVnAULiMd",
      "metadata": {
        "id": "P24QVnAULiMd"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = BERT_hate_speech_multitask.from_pretrained(\"./models/ht_bert_multi_finetuned/\")\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "test_loader = DataLoader(test_dataset)\n",
        "\n",
        "accuracies = evaluate_multitask(model, tokenizer, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-h8pw9i8L8NJ",
      "metadata": {
        "id": "-h8pw9i8L8NJ"
      },
      "outputs": [],
      "source": [
        "for i in range(3):\n",
        "    print('Task %d accuracy: %2.2f %%' % (i, 100.0*accuracies[i]))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yA_vw43f-UX4",
      "metadata": {
        "id": "yA_vw43f-UX4"
      },
      "outputs": [],
      "source": [
        "print(predict_hatespeech_multitask(\"I go see pinguins at the zoo.\", tokenizer, model)['predictions'])\n",
        "print(predict_hatespeech_multitask(\"Bananas are so stupid \", tokenizer, model)['predictions'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "lab06_solutions.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
