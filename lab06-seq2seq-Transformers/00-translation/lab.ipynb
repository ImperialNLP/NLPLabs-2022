{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "military-character",
      "metadata": {
        "id": "military-character"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EslAmsE-tWMB",
      "metadata": {
        "id": "EslAmsE-tWMB"
      },
      "source": [
        "In this lab, we will take you through a practical use of Transformers. This notebook shows you how to use [Hugging face](https://huggingface.co/)'s package to import and train pretrained models for the machine translation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rwXjbNUJHzZ0",
      "metadata": {
        "id": "rwXjbNUJHzZ0"
      },
      "source": [
        "## Setting up the Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Vina6dhaHz1N",
      "metadata": {
        "id": "Vina6dhaHz1N"
      },
      "source": [
        "First, we need to install Hugging Face [transformers](https://huggingface.co/transformers/index.html) and [Sentence piece Tokenizers](https://github.com/google/sentencepiece) with the following commands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "other-scottish",
      "metadata": {
        "id": "other-scottish"
      },
      "outputs": [],
      "source": [
        "#! pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "modern-olympus",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "modern-olympus",
        "outputId": "629d3a85-7a90-48b6-e132-bc979c12a741"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (4.16.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from transformers) (0.11.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: filelock in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from transformers) (2022.1.18)\n",
            "Requirement already satisfied: sacremoses in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from requests->transformers) (1.26.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from requests->transformers) (2.0.10)\n",
            "Requirement already satisfied: click in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\n",
            "Requirement already satisfied: joblib in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (0.1.96)\n",
            "Requirement already satisfied: ipywidgets in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (7.6.5)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from ipywidgets) (6.4.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from ipywidgets) (5.1.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from ipywidgets) (1.0.0)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from ipywidgets) (7.29.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from ipywidgets) (3.5.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from ipywidgets) (5.1.3)\n",
            "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.2)\n",
            "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: jupyter-client<8.0 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.1.0)\n",
            "Requirement already satisfied: argcomplete>=1.12.3 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.12.3)\n",
            "Requirement already satisfied: importlib-metadata<5 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets) (4.8.2)\n",
            "Requirement already satisfied: tornado<7.0,>=4.2 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from importlib-metadata<5->ipykernel>=4.5.1->ipywidgets) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from importlib-metadata<5->ipykernel>=4.5.1->ipywidgets) (3.10.0.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (58.0.4)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.20)\n",
            "Requirement already satisfied: backcall in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: pygments in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (2.10.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (0.18.0)\n",
            "Requirement already satisfied: pickleshare in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: decorator in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (5.1.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (4.9.1)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (22.3.0)\n",
            "Requirement already satisfied: entrypoints in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (0.3)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets) (3.2.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (21.4.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (1.16.0)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.18.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from widgetsnbextension~=3.5.0->ipywidgets) (6.4.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.9.4)\n",
            "Requirement already satisfied: prometheus-client in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.12.0)\n",
            "Requirement already satisfied: argon2-cffi in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (20.1.0)\n",
            "Requirement already satisfied: nbconvert in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (6.3.0)\n",
            "Requirement already satisfied: jinja2 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.0.2)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.21)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.0.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.3)\n",
            "Requirement already satisfied: defusedxml in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.4.3)\n",
            "Requirement already satisfied: jupyterlab-pygments in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.1.2)\n",
            "Requirement already satisfied: bleach in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (4.1.0)\n",
            "Requirement already satisfied: testpath in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.0)\n",
            "Requirement already satisfied: async-generator in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.10)\n",
            "Requirement already satisfied: webencodings in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: packaging in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /vol/bitbucket/aeg19/anaconda3/envs/NLPlabs/lib/python3.7/site-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.0.4)\n",
            "Traceback (most recent call last):\n",
            "  File \"/homes/aeg19/.local/bin/jupyter\", line 7, in <module>\n",
            "    from jupyter_core.command import main\n",
            "ModuleNotFoundError: No module named 'jupyter_core'\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install ipywidgets\n",
        "!jupyter nbextension enable --py widgetsnbextension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "developing-france",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "developing-france",
        "outputId": "bbc8a705-01c2-4b00-8081-1b619c887da1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertPreTrainedModel, BertModel\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "  print('WARNING: You may want to change the runtime to GPU for faster training!')\n",
        "  DEVICE = 'cpu'\n",
        "else:\n",
        "  DEVICE = 'cuda:0'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58jPlYaWk7fh",
      "metadata": {
        "id": "58jPlYaWk7fh"
      },
      "source": [
        "If you work in Colab, mount your google drive to save models and training checkpoints. Run the following code to connect your google drive to colab. Click on the link and copy and past the code you saw into the input box."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qjdWhQHBk6RW",
      "metadata": {
        "id": "qjdWhQHBk6RW"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# %cd '/content/drive/MyDrive/Colab Notebooks/'\n",
        "# %mkdir './Lab 7'\n",
        "# %cd './Lab 7' "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "exterior-afghanistan",
      "metadata": {
        "id": "exterior-afghanistan"
      },
      "source": [
        "# Machine Translation (MT)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "assigned-sapphire",
      "metadata": {
        "id": "assigned-sapphire"
      },
      "source": [
        "Recall the previous lab on machine translation [(Lab 6)](https://colab.research.google.com/github/ImperialNLP/NLPLabs/blob/master/lab05/lab05_solutions.ipynb) where we trained a recurrent neural network on the [Multi30k](https://github.com/multi30k/dataset) dataset. \n",
        "\n",
        "We will now train a Transformer model for the same task on the same dataset and compare the results. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wkbsnSm_oK8e",
      "metadata": {
        "id": "wkbsnSm_oK8e"
      },
      "source": [
        "## Downloading dataset and evaluation function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fr-2a2z5kS9a",
      "metadata": {
        "id": "fr-2a2z5kS9a"
      },
      "source": [
        "We will start by downloading the dataset for German, English and French and installing `sacreBLEU` to compute the BLEU score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "musical-series",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "musical-series",
        "outputId": "ece3cc61-ffb8-4d02-dcf9-08ddf6c4447e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sacrebleu==1.5.0\n",
            "  Downloading sacrebleu-1.5.0-py3-none-any.whl (65 kB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
            "Installing collected packages: portalocker, sacrebleu\n",
            "Successfully installed portalocker-2.3.2 sacrebleu-1.5.0\n",
            "\n",
            "Downloading train.lc.norm.tok.en\n",
            "Downloading train.lc.norm.tok.de\n",
            "Downloading train.lc.norm.tok.fr\n",
            "Downloading val.lc.norm.tok.en\n",
            "Downloading val.lc.norm.tok.de\n",
            "Downloading val.lc.norm.tok.fr\n",
            "Downloading test_2016_flickr.lc.norm.tok.en\n",
            "Downloading test_2016_flickr.lc.norm.tok.de\n",
            "Downloading test_2016_flickr.lc.norm.tok.fr\n",
            "\n",
            "     1\ttwo young , white males are outside near many bushes .\n",
            "     2\tseveral men in hard hats are operating a giant pulley system .\n",
            "     3\ta little girl climbing into a wooden playhouse .\n",
            "     4\ta man in a blue shirt is standing on a ladder cleaning a window .\n",
            "     5\ttwo men are at the stove preparing food .\n",
            "     6\ta man in green holds a guitar while the other man observes his shirt .\n",
            "     7\ta man is smiling at a stuffed lion\n",
            "     8\ta trendy girl talking on her cellphone while gliding slowly down the street .\n",
            "     9\ta woman with a large purse is walking by a gate .\n",
            "    10\tboys dancing on poles in the middle of the night .\n",
            "\n",
            "     1\tdeux jeunes hommes blancs sont dehors près de buissons .\n",
            "     2\tplusieurs hommes en casque font fonctionner un système de poulies géant .\n",
            "     3\tune petite fille grimpe dans une maisonnette en bois .\n",
            "     4\tun homme dans une chemise bleue se tient sur une échelle pour nettoyer une fenêtre .\n",
            "     5\tdeux hommes aux fourneaux préparent à manger .\n",
            "     6\tun homme en vert tient une guitare tandis qu&apos; un autre homme observe sa chemise .\n",
            "     7\tun homme sourit à un ours en peluche .\n",
            "     8\tune fille branchée parle à son portable tout en glissant lentement dans la rue .\n",
            "     9\tune femme avec un gros sac passe par une porte .\n",
            "    10\tdes garçons dansent sur des barres au milieu de la nuit .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "# install sacreBLEU\n",
        "pip install sacrebleu==1.5.0\n",
        "echo\n",
        "\n",
        "# Download the corpus\n",
        "URL=\"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/tok\"\n",
        "\n",
        "#cd data\n",
        "\n",
        "for split in \"train\" \"val\" \"test_2016_flickr\"; do\n",
        "    for lang in en de fr; do\n",
        "        fname=\"${split}.lc.norm.tok.${lang}\"\n",
        "        if [ ! -f $fname ]; then\n",
        "            echo \"Downloading $fname\"\n",
        "            wget -q \"${URL}/$fname\" -O \"${split/_2016_flickr/}.${lang}\"\n",
        "        fi\n",
        "    done\n",
        "done\n",
        "echo \n",
        "\n",
        "# Print the first 10 lines with line numbers of \n",
        "# the English and French training data\n",
        "cat -n train.en | head -n10\n",
        "echo\n",
        "cat -n train.fr | head -n10\n",
        "echo\n",
        "cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qAB2AV_Ns-aY",
      "metadata": {
        "id": "qAB2AV_Ns-aY"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Hs-Wy6uZkyJF",
      "metadata": {
        "id": "Hs-Wy6uZkyJF"
      },
      "source": [
        "Just as in the previous lab we define our own dataset class to handle ``Multi30k``. However, we modify it to adapt it to a transformer model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "wanted-venice",
      "metadata": {
        "id": "wanted-venice"
      },
      "outputs": [],
      "source": [
        "class Multi30K:\n",
        "    \"\"\"A dataset wrapper for Multi30K.\"\"\"\n",
        "    def __init__(self, tokenizer, src_file, trg_file):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "        self.src_sents, self.trg_sents = self.read_sentences(src_file, trg_file)\n",
        "\n",
        "    def read_sentences(self, src_file, trg_file):\n",
        "        src_sents = []\n",
        "        trg_sents = []\n",
        "\n",
        "        # Read source side\n",
        "        with open(src_file) as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                src_sents.append(line) \n",
        "            \n",
        "        # Read target side\n",
        "        with open(trg_file) as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                trg_sents.append(line)\n",
        "\n",
        "        assert len(src_sents) == len(trg_sents), \"Files are not aligned!\"\n",
        "        return src_sents, trg_sents\n",
        "    \n",
        "    def collate_fn(self, idx):\n",
        "        src_texts = [self.src_sents[i] for i in idx]\n",
        "        trg_texts = [self.trg_sents[i] for i in idx]\n",
        "        \n",
        "        output = self.tokenizer.prepare_seq2seq_batch(src_texts=src_texts, \n",
        "                                                      tgt_texts=trg_texts, \n",
        "                                                      max_length=128, \n",
        "                                                      max_target_length=128,\n",
        "                                                      return_tensors='pt',\n",
        "                                                      truncation=True)\n",
        "        return output\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.src_sents)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xyGLGLBArnHL",
      "metadata": {
        "id": "xyGLGLBArnHL"
      },
      "source": [
        "## Transformer model for MT"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Uh7mzoLBmQhJ",
      "metadata": {
        "id": "Uh7mzoLBmQhJ"
      },
      "source": [
        "\n",
        "For this task, we won't be using Bert but instead, we will import another model, [MarianMT](https://huggingface.co/transformers/model_doc/marian.html), which is a  specifically used for MT. \n",
        "\n",
        "Running the code below you can see all modules within MarianMT model and compare it with Bert. \n",
        "\n",
        "**Q4: What's missing in BERT architecture necessary for MT?**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "SABFaL3UlK6r",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3b078a8aa9b145dabf05a8ca93854fc3",
            "25e987dead5e40cbafb71feb099cda81",
            "0712735baf9b40dea3e3b38f00cb6f1e",
            "775b4ca6c0e94ba08c4ef32779311afa",
            "968b7b1d37064914a5c00b00f500d9e2",
            "91971772860941ac8ab83cde9292fc3a",
            "8a9c46189a7449939de1023699ce1ed0",
            "a6e9683c361240ec85d67aa39775b709",
            "7f51eba2be474e5486ca9a3dc1487037",
            "a6b2c87fafa04f3fa49c11cf47a0e527",
            "adba8794c6d6470083d805d42111b4d6",
            "c0d35b06063c4113aa88d899fc26a28a",
            "36f7441e69d24a359d26ebf9fa4af07f",
            "7f87a9f9fda245bab41de3aa49ac4c62",
            "980bf5380ccc467284f68a701985951e",
            "f81ac79ac55e45c7908113bf8a69c365"
          ]
        },
        "id": "SABFaL3UlK6r",
        "outputId": "9f7da913-bd30-4155-927f-3727243eb038"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "152ec345160348118c6fc27341452c64",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.30k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ffbca73f78844d0a5ba58249b3c5188",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/284M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "MarianMTModel(\n",
              "  (model): MarianModel(\n",
              "    (shared): Embedding(58101, 512, padding_idx=58100)\n",
              "    (encoder): MarianEncoder(\n",
              "      (embed_tokens): Embedding(58101, 512, padding_idx=58100)\n",
              "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
              "      (layers): ModuleList(\n",
              "        (0): MarianEncoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): MarianEncoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): MarianEncoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): MarianEncoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): MarianEncoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): MarianEncoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (decoder): MarianDecoder(\n",
              "      (embed_tokens): Embedding(58101, 512, padding_idx=58100)\n",
              "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
              "      (layers): ModuleList(\n",
              "        (0): MarianDecoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): MarianDecoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): MarianDecoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): MarianDecoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): MarianDecoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): MarianDecoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=58101, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import EncoderDecoderModel, MarianMTModel, MarianTokenizer, BartModel, BartConfig, BertConfig, BartForCausalLM,Trainer,TrainingArguments\n",
        "\n",
        "model = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-de')\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "linear-diabetes",
      "metadata": {
        "id": "linear-diabetes"
      },
      "source": [
        "\n",
        "**Question 5: Import Transformer model and train for Machine Translation**\n",
        "\n",
        "Modify ``main_mt()`` to:\n",
        "- Import the ``\"Helsinki-NLP/opus-mt-en-de\"`` variant of ``MarianMT`` model and the tokenizer it was trained with.\n",
        "- Create the train (``train.en`` and ``train.de`` files) and test (``test.en`` and ``test.de``) dataset objects using our ``Multi30k`` class.\n",
        "- Finetune the model on the train set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fewer-england",
      "metadata": {
        "id": "fewer-england"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "unexpected EOF while parsing (2459341363.py, line 7)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_21572/2459341363.py\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    # <TODO> #\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ],
      "source": [
        "from transformers import EncoderDecoderModel, MarianMTModel, MarianTokenizer, BartModel, BartConfig, BertConfig, BartForCausalLM,Trainer,TrainingArguments\n",
        "\n",
        "def main_mt():\n",
        "    \n",
        "    ## QUESTION 5 ##\n",
        "\n",
        "    # <TODO> #\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8LdM1J-wtPWd",
      "metadata": {
        "id": "8LdM1J-wtPWd"
      },
      "source": [
        "The following code should run for ~50min if you train the model for 3 epochs and batchsize 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "resident-sight",
      "metadata": {
        "id": "resident-sight"
      },
      "outputs": [],
      "source": [
        "main_mt()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mJcA9ihzY6Pf",
      "metadata": {
        "id": "mJcA9ihzY6Pf"
      },
      "source": [
        "Let's evaluate our model using BLEU metric\n",
        "."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rowUbyM_enUw",
      "metadata": {
        "id": "rowUbyM_enUw"
      },
      "outputs": [],
      "source": [
        "import sacrebleu\n",
        "\n",
        "def evaluate_mt(model,mt_tokenizer, mt_test_dataset):\n",
        "\n",
        "  bleu = []\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  for file in tqdm(range(len(mt_test_dataset))):\n",
        "\n",
        "    src_text = mt_test_dataset.src_sents[file]\n",
        "    targ_text_origin = mt_test_dataset.trg_sents[file]\n",
        "\n",
        "    translated = model.generate(**mt_tokenizer.prepare_seq2seq_batch(src_text, return_tensors=\"pt\"))\n",
        "    translated_text = [mt_tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
        "\n",
        "    bleu.append(sacrebleu.corpus_bleu(translated_text, targ_text_origin, force=True).score)\n",
        "\n",
        "  bleu = np.asarray(bleu)\n",
        "\n",
        "  return np.average(bleu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OghdALVBpz1b",
      "metadata": {
        "id": "OghdALVBpz1b"
      },
      "outputs": [],
      "source": [
        "#your model name here\n",
        "model_name = './models/mt_marianmt/'\n",
        "\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "mt_tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')\n",
        "mt_test_dataset = Multi30K(mt_tokenizer, 'test.en', 'test.de')\n",
        "\n",
        "bleu = evaluate_mt(model,mt_tokenizer,mt_test_dataset)\n",
        "\n",
        "print(bleu)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfQXV7TABFVw",
      "metadata": {
        "id": "dfQXV7TABFVw"
      },
      "source": [
        "What BLEU score do you get ? How does it compare to the performance of the RNN with and without attention? \n",
        "\n",
        "Can you improve the score ?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zrLXxS1hHjuu",
      "metadata": {
        "id": "zrLXxS1hHjuu"
      },
      "source": [
        "# Extra "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aIuj99ZNHl8z",
      "metadata": {
        "id": "aIuj99ZNHl8z"
      },
      "source": [
        "## Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MznOidHWnfD7",
      "metadata": {
        "id": "MznOidHWnfD7"
      },
      "source": [
        "**Q6.1**: Why do RNN suffer from long term dependency issues and how do Transformers fix that?\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Q6.2**: Which architecture is faster to train: RNN or Transformer? Why?\n",
        "\n",
        "---\n",
        "\n",
        "**Q6.3** What issues do Transformers have due to the attention mechanism? Why do models tend to have a limited input size? \n",
        " \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HPbsBF2LHpLV",
      "metadata": {
        "id": "HPbsBF2LHpLV"
      },
      "source": [
        "## MT Extension"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pgAFPMVHCCJX",
      "metadata": {
        "id": "pgAFPMVHCCJX"
      },
      "source": [
        "In this section, we mainly define the following classes:\n",
        "\n",
        "\n",
        "*   ``SinusoidalPositionalEmbedding`` -- *Note that this is different from the PositionalEmbedding in original Bert model.*\n",
        "*   ``Attention``\n",
        "*   ``PreTrainedModel``\n",
        "\n",
        "\n",
        "*   ``EncoderLayer``\n",
        "*   ``DecoderLayer``\n",
        "*   ``Encoder``\n",
        "*   ``Decoder``\n",
        "\n",
        "\n",
        "*   ``Model``\n",
        "*   ``MTModel``\n",
        "\n",
        "\n",
        "The task is to fill in the missing blocks in ***Model*** and ***MTModel*** and understand the process of transformer for MT. \n",
        "\n",
        "\n",
        "Search the coding blocks by ***TODO***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_eAzSyuhCCJm",
      "metadata": {
        "id": "_eAzSyuhCCJm"
      },
      "outputs": [],
      "source": [
        "from transformers import PreTrainedModel, MarianConfig as MyConfig\n",
        "from transformers.activations import ACT2FN\n",
        "from transformers.modeling_outputs import (BaseModelOutput, \n",
        "    BaseModelOutputWithPastAndCrossAttentions,\n",
        "    CausalLMOutputWithCrossAttentions,\n",
        "    Seq2SeqLMOutput,\n",
        "    Seq2SeqModelOutput)\n",
        "\n",
        "import copy\n",
        "import math\n",
        "import random\n",
        "import logging\n",
        "from typing import Optional, Tuple\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
        "    \"Helsinki-NLP/opus-mt-en-de\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SnhK_J42CCJu",
      "metadata": {
        "id": "SnhK_J42CCJu"
      },
      "outputs": [],
      "source": [
        "def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n",
        "    \"\"\"\n",
        "    Shift input ids one token to the right.\n",
        "    \"\"\"\n",
        "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
        "    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
        "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
        "\n",
        "    assert pad_token_id is not None, \"self.model.config.pad_token_id has to be defined.\"\n",
        "    # replace possible -100 values in labels by `pad_token_id`\n",
        "    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
        "\n",
        "    return shifted_input_ids\n",
        "\n",
        "\n",
        "\n",
        "def _make_causal_mask(input_ids_shape: torch.Size, dtype: torch.dtype, past_key_values_length: int = 0):\n",
        "    \"\"\"\n",
        "    Make causal mask used for bi-directional self-attention.\n",
        "    \"\"\"\n",
        "    bsz, tgt_len = input_ids_shape\n",
        "    mask = torch.full((tgt_len, tgt_len), float(\"-inf\"))\n",
        "    mask_cond = torch.arange(mask.size(-1))\n",
        "    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
        "    mask = mask.to(dtype)\n",
        "\n",
        "    if past_key_values_length > 0:\n",
        "        mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1)\n",
        "    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n",
        "\n",
        "\n",
        "\n",
        "def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n",
        "    \"\"\"\n",
        "    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n",
        "    \"\"\"\n",
        "    bsz, src_len = mask.size()\n",
        "    tgt_len = tgt_len if tgt_len is not None else src_len\n",
        "\n",
        "    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
        "\n",
        "    inverted_mask = 1.0 - expanded_mask\n",
        "\n",
        "    return inverted_mask.masked_fill(inverted_mask.bool(), torch.finfo(dtype).min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ad0_EWWLCCJw",
      "metadata": {
        "id": "Ad0_EWWLCCJw"
      },
      "outputs": [],
      "source": [
        "class SinusoidalPositionalEmbedding(nn.Embedding):\n",
        "    \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n",
        "\n",
        "    def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None):\n",
        "        super().__init__(num_positions, embedding_dim)\n",
        "        self.weight = self._init_weight(self.weight)\n",
        "\n",
        "    @staticmethod\n",
        "    def _init_weight(out: nn.Parameter):\n",
        "        \"\"\"\n",
        "        Identical to the XLM create_sinusoidal_embeddings except features are not interleaved. The cos features are in\n",
        "        the 2nd half of the vector. [dim // 2:]\n",
        "        \"\"\"\n",
        "        n_pos, dim = out.shape\n",
        "        position_enc = np.array(\n",
        "            [[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)]\n",
        "        )\n",
        "        out.requires_grad = False  # set early to avoid an error in pytorch-1.8+\n",
        "        sentinel = dim // 2 if dim % 2 == 0 else (dim // 2) + 1\n",
        "        out[:, 0:sentinel] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n",
        "        out[:, sentinel:] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n",
        "        out.detach_()\n",
        "        return out\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0):\n",
        "        \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n",
        "        bsz, seq_len = input_ids_shape[:2]\n",
        "        positions = torch.arange(\n",
        "            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n",
        "        )\n",
        "        return super().forward(positions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cVMACm5ECCJx",
      "metadata": {
        "id": "cVMACm5ECCJx"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        num_heads: int,\n",
        "        dropout: float = 0.0,\n",
        "        is_decoder: bool = False,\n",
        "        bias: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        assert (\n",
        "            self.head_dim * num_heads == self.embed_dim\n",
        "        ), f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`: {num_heads}).\"\n",
        "        self.scaling = self.head_dim ** -0.5\n",
        "        self.is_decoder = is_decoder\n",
        "\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "\n",
        "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
        "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        key_value_states: Optional[torch.Tensor] = None,\n",
        "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        layer_head_mask: Optional[torch.Tensor] = None,\n",
        "        output_attentions: bool = False,\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
        "\n",
        "        # if key_value_states are provided this layer is used as a cross-attention layer\n",
        "        # for the decoder\n",
        "        is_cross_attention = key_value_states is not None\n",
        "        bsz, tgt_len, embed_dim = hidden_states.size()\n",
        "\n",
        "        # get query proj\n",
        "        query_states = self.q_proj(hidden_states) * self.scaling\n",
        "        # get key, value proj\n",
        "        if is_cross_attention and past_key_value is not None:\n",
        "            # reuse k,v, cross_attentions\n",
        "            key_states = past_key_value[0]\n",
        "            value_states = past_key_value[1]\n",
        "        elif is_cross_attention:\n",
        "            # cross_attentions\n",
        "            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n",
        "            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n",
        "        elif past_key_value is not None:\n",
        "            # reuse k, v, self_attention\n",
        "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
        "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
        "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
        "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
        "        else:\n",
        "            # self_attention\n",
        "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
        "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
        "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
        "            # key/value_states (first \"if\" case)\n",
        "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
        "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
        "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
        "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
        "            past_key_value = (key_states, value_states)\n",
        "\n",
        "        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
        "        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
        "        key_states = key_states.view(*proj_shape)\n",
        "        value_states = value_states.view(*proj_shape)\n",
        "\n",
        "        src_len = key_states.size(1)\n",
        "        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
        "\n",
        "        assert attn_weights.size() == (\n",
        "            bsz * self.num_heads,\n",
        "            tgt_len,\n",
        "            src_len,\n",
        "        ), f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is {attn_weights.size()}\"\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            assert attention_mask.size() == (\n",
        "                bsz,\n",
        "                1,\n",
        "                tgt_len,\n",
        "                src_len,\n",
        "            ), f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n",
        "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "\n",
        "        if layer_head_mask is not None:\n",
        "            assert layer_head_mask.size() == (\n",
        "                self.num_heads,\n",
        "            ), f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is {layer_head_mask.size()}\"\n",
        "            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        if output_attentions:\n",
        "            # this operation is a bit akward, but it's required to\n",
        "            # make sure that attn_weights keeps its gradient.\n",
        "            # In order to do so, attn_weights have to reshaped\n",
        "            # twice and have to be reused in the following\n",
        "            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "        else:\n",
        "            attn_weights_reshaped = None\n",
        "\n",
        "        attn_probs = F.dropout(attn_weights, p=self.dropout, training=self.training)\n",
        "\n",
        "        attn_output = torch.bmm(attn_probs, value_states)\n",
        "\n",
        "        assert attn_output.size() == (\n",
        "            bsz * self.num_heads,\n",
        "            tgt_len,\n",
        "            self.head_dim,\n",
        "        ), f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is {attn_output.size()}\"\n",
        "\n",
        "        attn_output = (\n",
        "            attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
        "            .transpose(1, 2)\n",
        "            .reshape(bsz, tgt_len, embed_dim)\n",
        "        )\n",
        "\n",
        "        attn_output = self.out_proj(attn_output)\n",
        "\n",
        "        return attn_output, attn_weights_reshaped, past_key_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yF3CUTOOCCJ_",
      "metadata": {
        "id": "yF3CUTOOCCJ_"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, config: MyConfig):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.d_model\n",
        "        self.self_attn = Attention(\n",
        "            embed_dim=self.embed_dim,\n",
        "            num_heads=config.encoder_attention_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "        )\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "        self.dropout = config.dropout\n",
        "        self.activation_fn = ACT2FN[config.activation_function]\n",
        "        self.activation_dropout = config.activation_dropout\n",
        "        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)\n",
        "        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)\n",
        "        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: torch.Tensor,\n",
        "        layer_head_mask: torch.Tensor,\n",
        "        output_attentions: bool = False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hidden_states (:obj:`torch.FloatTensor`): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
        "            attention_mask (:obj:`torch.FloatTensor`): attention mask of size\n",
        "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
        "            layer_head_mask (:obj:`torch.FloatTensor`): mask for attention heads in a given layer of size\n",
        "                `(config.encoder_attention_heads,)`.\n",
        "            output_attentions (:obj:`bool`, `optional`):\n",
        "                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
        "                returned tensors for more detail.\n",
        "        \"\"\"\n",
        "        residual = hidden_states\n",
        "        hidden_states, attn_weights, _ = self.self_attn(\n",
        "            hidden_states=hidden_states,\n",
        "            attention_mask=attention_mask,\n",
        "            layer_head_mask=layer_head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "        hidden_states = residual + hidden_states\n",
        "        hidden_states = self.self_attn_layer_norm(hidden_states)\n",
        "\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
        "        hidden_states = F.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
        "        hidden_states = self.fc2(hidden_states)\n",
        "        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "        hidden_states = residual + hidden_states\n",
        "        hidden_states = self.final_layer_norm(hidden_states)\n",
        "\n",
        "        if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():\n",
        "            clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n",
        "            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "\n",
        "        if output_attentions:\n",
        "            outputs += (attn_weights,)\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5zZFD7_iCCKL",
      "metadata": {
        "id": "5zZFD7_iCCKL"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, config: MyConfig):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.d_model\n",
        "\n",
        "        self.self_attn = Attention(\n",
        "            embed_dim=self.embed_dim,\n",
        "            num_heads=config.decoder_attention_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "            is_decoder=True,\n",
        "        )\n",
        "        self.dropout = config.dropout\n",
        "        self.activation_fn = ACT2FN[config.activation_function]\n",
        "        self.activation_dropout = config.activation_dropout\n",
        "\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "        self.encoder_attn = Attention(\n",
        "            self.embed_dim,\n",
        "            config.decoder_attention_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "            is_decoder=True,\n",
        "        )\n",
        "        self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "        self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n",
        "        self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n",
        "        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
        "        layer_head_mask: Optional[torch.Tensor] = None,\n",
        "        encoder_layer_head_mask: Optional[torch.Tensor] = None,\n",
        "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "        use_cache: Optional[bool] = True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hidden_states (:obj:`torch.FloatTensor`): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
        "            attention_mask (:obj:`torch.FloatTensor`): attention mask of size\n",
        "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
        "            encoder_hidden_states (:obj:`torch.FloatTensor`): cross attention input to the layer of shape `(seq_len, batch, embed_dim)`\n",
        "            encoder_attention_mask (:obj:`torch.FloatTensor`): encoder attention mask of size\n",
        "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
        "            layer_head_mask (:obj:`torch.FloatTensor`): mask for attention heads in a given layer of size\n",
        "                `(config.encoder_attention_heads,)`.\n",
        "            encoder_layer_head_mask (:obj:`torch.FloatTensor`): mask for encoder attention heads in a given layer of\n",
        "                size `(config.encoder_attention_heads,)`.\n",
        "            past_key_value (:obj:`Tuple(torch.FloatTensor)`): cached past key and value projection states\n",
        "            output_attentions (:obj:`bool`, `optional`):\n",
        "                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
        "                returned tensors for more detail.\n",
        "        \"\"\"\n",
        "        residual = hidden_states\n",
        "\n",
        "        # Self Attention\n",
        "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
        "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
        "        # add present self-attn cache to positions 1,2 of present_key_value tuple\n",
        "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
        "            hidden_states=hidden_states,\n",
        "            past_key_value=self_attn_past_key_value,\n",
        "            attention_mask=attention_mask,\n",
        "            layer_head_mask=layer_head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "        hidden_states = residual + hidden_states\n",
        "        hidden_states = self.self_attn_layer_norm(hidden_states)\n",
        "\n",
        "        # Cross-Attention Block\n",
        "        cross_attn_present_key_value = None\n",
        "        cross_attn_weights = None\n",
        "        if encoder_hidden_states is not None:\n",
        "            residual = hidden_states\n",
        "\n",
        "            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n",
        "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
        "            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n",
        "                hidden_states=hidden_states,\n",
        "                key_value_states=encoder_hidden_states,\n",
        "                attention_mask=encoder_attention_mask,\n",
        "                layer_head_mask=encoder_layer_head_mask,\n",
        "                past_key_value=cross_attn_past_key_value,\n",
        "                output_attentions=output_attentions,\n",
        "            )\n",
        "            hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "            hidden_states = residual + hidden_states\n",
        "            hidden_states = self.encoder_attn_layer_norm(hidden_states)\n",
        "\n",
        "            # add cross-attn to positions 3,4 of present_key_value tuple\n",
        "            present_key_value = present_key_value + cross_attn_present_key_value\n",
        "\n",
        "        # Fully Connected\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
        "        hidden_states = F.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
        "        hidden_states = self.fc2(hidden_states)\n",
        "        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "        hidden_states = residual + hidden_states\n",
        "        hidden_states = self.final_layer_norm(hidden_states)\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "\n",
        "        if output_attentions:\n",
        "            outputs += (self_attn_weights, cross_attn_weights)\n",
        "\n",
        "        if use_cache:\n",
        "            outputs += (present_key_value,)\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fThRhT7GCCKM",
      "metadata": {
        "id": "fThRhT7GCCKM"
      },
      "outputs": [],
      "source": [
        "class PreTrainedModel(PreTrainedModel):\n",
        "    config_class = MyConfig\n",
        "    base_model_prefix = \"model\"\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        std = self.config.init_std\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, SinusoidalPositionalEmbedding):\n",
        "            pass\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "\n",
        "    @property\n",
        "    def dummy_inputs(self):\n",
        "        pad_token = self.config.pad_token_id\n",
        "        input_ids = torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device)\n",
        "        dummy_inputs = {\n",
        "            \"attention_mask\": input_ids.ne(pad_token),\n",
        "            \"input_ids\": input_ids,\n",
        "            \"decoder_input_ids\": input_ids,\n",
        "        }\n",
        "        return dummy_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LLgME8LfCCKN",
      "metadata": {
        "id": "LLgME8LfCCKN"
      },
      "outputs": [],
      "source": [
        "class Encoder(PreTrainedModel):\n",
        "    \"\"\"\n",
        "    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n",
        "    :class:`EncoderLayer`.\n",
        "\n",
        "    Args:\n",
        "        config: MyConfig\n",
        "        embed_tokens (torch.nn.Embedding): output embedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: MyConfig, embed_tokens: Optional[nn.Embedding] = None):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.dropout = config.dropout\n",
        "        self.layerdrop = config.encoder_layerdrop\n",
        "\n",
        "        embed_dim = config.d_model\n",
        "        self.padding_idx = config.pad_token_id\n",
        "        self.max_source_positions = config.max_position_embeddings\n",
        "        self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n",
        "\n",
        "        if embed_tokens is not None:\n",
        "            self.embed_tokens = embed_tokens\n",
        "        else:\n",
        "            self.embed_tokens = nn.Embedding(config.vocab_size, embed_dim, self.padding_idx)\n",
        "\n",
        "        self.embed_positions = SinusoidalPositionalEmbedding(\n",
        "            config.max_position_embeddings,\n",
        "            embed_dim,\n",
        "            self.padding_idx,\n",
        "        )\n",
        "        self.layers = nn.ModuleList([EncoderLayer(config) for _ in range(config.encoder_layers)])\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        Args:\n",
        "            input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):\n",
        "                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n",
        "                provide it.\n",
        "\n",
        "                Indices can be obtained using :class:`~transformers.MarianTokenizer`. See\n",
        "                :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__`\n",
        "                for details.\n",
        "\n",
        "                `What are input IDs? <../glossary.html#input-ids>`__\n",
        "            attention_mask (:obj:`torch.Tensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "                Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "                - 1 for tokens that are **not masked**,\n",
        "                - 0 for tokens that are **masked**.\n",
        "\n",
        "                `What are attention masks? <../glossary.html#attention-mask>`__\n",
        "            head_mask (:obj:`torch.Tensor` of shape :obj:`(num_layers, num_heads)`, `optional`):\n",
        "                Mask to nullify selected heads of the attention modules. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "                - 1 indicates the head is **not masked**,\n",
        "                - 0 indicates the heas is **masked**.\n",
        "\n",
        "            inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
        "                Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded\n",
        "                representation. This is useful if you want more control over how to convert :obj:`input_ids` indices\n",
        "                into associated vectors than the model's internal embedding lookup matrix.\n",
        "            output_attentions (:obj:`bool`, `optional`):\n",
        "                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
        "                returned tensors for more detail.\n",
        "            output_hidden_states (:obj:`bool`, `optional`):\n",
        "                Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors\n",
        "                for more detail.\n",
        "            return_dict (:obj:`bool`, `optional`):\n",
        "                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # retrieve input_ids and inputs_embeds\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "            input_ids = input_ids.view(-1, input_shape[-1])\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n",
        "\n",
        "        embed_pos = self.embed_positions(input_shape)\n",
        "\n",
        "        hidden_states = inputs_embeds + embed_pos\n",
        "        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "\n",
        "        # expand attention_mask\n",
        "        if attention_mask is not None:\n",
        "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
        "            attention_mask = _expand_mask(attention_mask, inputs_embeds.dtype)\n",
        "\n",
        "        encoder_states = () if output_hidden_states else None\n",
        "        all_attentions = () if output_attentions else None\n",
        "\n",
        "        # check if head_mask has a correct number of layers specified if desired\n",
        "        if head_mask is not None:\n",
        "            assert head_mask.size()[0] == (\n",
        "                len(self.layers)\n",
        "            ), f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n",
        "        for idx, encoder_layer in enumerate(self.layers):\n",
        "            if output_hidden_states:\n",
        "                encoder_states = encoder_states + (hidden_states,)\n",
        "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
        "            dropout_probability = random.uniform(0, 1)\n",
        "            if self.training and (dropout_probability < self.layerdrop):  # skip the layer\n",
        "                layer_outputs = (None, None)\n",
        "            else:\n",
        "                if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n",
        "\n",
        "                    def create_custom_forward(module):\n",
        "                        def custom_forward(*inputs):\n",
        "                            return module(*inputs, output_attentions)\n",
        "\n",
        "                        return custom_forward\n",
        "\n",
        "                    layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                        create_custom_forward(encoder_layer),\n",
        "                        hidden_states,\n",
        "                        attention_mask,\n",
        "                        (head_mask[idx] if head_mask is not None else None),\n",
        "                    )\n",
        "                else:\n",
        "                    layer_outputs = encoder_layer(\n",
        "                        hidden_states,\n",
        "                        attention_mask,\n",
        "                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n",
        "                        output_attentions=output_attentions,\n",
        "                    )\n",
        "\n",
        "                hidden_states = layer_outputs[0]\n",
        "\n",
        "            if output_attentions:\n",
        "                all_attentions = all_attentions + (layer_outputs[1],)\n",
        "\n",
        "        if output_hidden_states:\n",
        "            encoder_states = encoder_states + (hidden_states,)\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n",
        "        return BaseModelOutput(\n",
        "            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bE7GE-8PCCKP",
      "metadata": {
        "id": "bE7GE-8PCCKP"
      },
      "outputs": [],
      "source": [
        "class Decoder(PreTrainedModel):\n",
        "    \"\"\"\n",
        "    Transformer decoder consisting of *config.decoder_layers* layers. Each layer is a :class:`DecoderLayer`\n",
        "\n",
        "    Args:\n",
        "        config: MyConfig\n",
        "        embed_tokens (torch.nn.Embedding): output embedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: MyConfig, embed_tokens: Optional[nn.Embedding] = None):\n",
        "        super().__init__(config)\n",
        "        self.dropout = config.dropout\n",
        "        self.layerdrop = config.decoder_layerdrop\n",
        "        self.padding_idx = config.pad_token_id\n",
        "        self.max_target_positions = config.max_position_embeddings\n",
        "        self.embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n",
        "\n",
        "        if embed_tokens is not None:\n",
        "            self.embed_tokens = embed_tokens\n",
        "        else:\n",
        "            self.embed_tokens = nn.Embedding(config.vocab_size, config.d_model, self.padding_idx)\n",
        "\n",
        "        self.embed_positions = SinusoidalPositionalEmbedding(\n",
        "            config.max_position_embeddings,\n",
        "            config.d_model,\n",
        "            self.padding_idx,\n",
        "        )\n",
        "        self.layers = nn.ModuleList([DecoderLayer(config) for _ in range(config.decoder_layers)])\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embed_tokens = value\n",
        "\n",
        "    # Copied from transformers.models.bart.modeling_bart.BartDecoder._prepare_decoder_attention_mask\n",
        "    def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):\n",
        "        # create causal mask\n",
        "        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
        "        combined_attention_mask = None\n",
        "        if input_shape[-1] > 1:\n",
        "            combined_attention_mask = _make_causal_mask(\n",
        "                input_shape, inputs_embeds.dtype, past_key_values_length=past_key_values_length\n",
        "            ).to(self.device)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
        "            expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])\n",
        "            combined_attention_mask = (\n",
        "                expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n",
        "            )\n",
        "\n",
        "        return combined_attention_mask\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_head_mask=None,\n",
        "        past_key_values=None,\n",
        "        inputs_embeds=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        Args:\n",
        "            input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):\n",
        "                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n",
        "                provide it.\n",
        "\n",
        "                Indices can be obtained using :class:`~transformers.MarianTokenizer`. See\n",
        "                :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__`\n",
        "                for details.\n",
        "\n",
        "                `What are input IDs? <../glossary.html#input-ids>`__\n",
        "            attention_mask (:obj:`torch.Tensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "                Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "                - 1 for tokens that are **not masked**,\n",
        "                - 0 for tokens that are **masked**.\n",
        "\n",
        "                `What are attention masks? <../glossary.html#attention-mask>`__\n",
        "            encoder_hidden_states (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, encoder_sequence_length, hidden_size)`, `optional`):\n",
        "                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n",
        "                of the decoder.\n",
        "            encoder_attention_mask (:obj:`torch.LongTensor` of shape :obj:`(batch_size, encoder_sequence_length)`, `optional`):\n",
        "                Mask to avoid performing cross-attention on padding tokens indices of encoder input_ids. Mask values\n",
        "                selected in ``[0, 1]``:\n",
        "\n",
        "                - 1 for tokens that are **not masked**,\n",
        "                - 0 for tokens that are **masked**.\n",
        "\n",
        "                `What are attention masks? <../glossary.html#attention-mask>`__\n",
        "            head_mask (:obj:`torch.Tensor` of shape :obj:`(num_layers, num_heads)`, `optional`):\n",
        "                Mask to nullify selected heads of the attention modules. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "                - 1 indicates the head is **not masked**,\n",
        "                - 0 indicates the heas is **masked**.\n",
        "\n",
        "            encoder_head_mask (:obj:`torch.Tensor` of shape :obj:`(num_layers, num_heads)`, `optional`):\n",
        "                Mask to nullify selected heads of the attention modules in encoder to avoid performing cross-attention\n",
        "                on hidden heads. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "                - 1 indicates the head is **not masked**,\n",
        "                - 0 indicates the heas is **masked**.\n",
        "\n",
        "            past_key_values (:obj:`Tuple[Tuple[torch.Tensor]]` of length :obj:`config.n_layers` with each tuple having 2 tuples each of which has 2 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
        "                Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up\n",
        "                decoding.\n",
        "\n",
        "                If :obj:`past_key_values` are used, the user can optionally input only the last\n",
        "                :obj:`decoder_input_ids` (those that don't have their past key value states given to this model) of\n",
        "                shape :obj:`(batch_size, 1)` instead of all :obj:`decoder_input_ids`` of shape :obj:`(batch_size,\n",
        "                sequence_length)`.\n",
        "            inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
        "                Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded\n",
        "                representation. This is useful if you want more control over how to convert :obj:`input_ids` indices\n",
        "                into associated vectors than the model's internal embedding lookup matrix.\n",
        "            output_attentions (:obj:`bool`, `optional`):\n",
        "                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
        "                returned tensors for more detail.\n",
        "            output_hidden_states (:obj:`bool`, `optional`):\n",
        "                Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors\n",
        "                for more detail.\n",
        "            return_dict (:obj:`bool`, `optional`):\n",
        "                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # retrieve input_ids and inputs_embeds\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "            input_ids = input_ids.view(-1, input_shape[-1])\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n",
        "\n",
        "        # past_key_values_length\n",
        "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n",
        "\n",
        "        attention_mask = self._prepare_decoder_attention_mask(\n",
        "            attention_mask, input_shape, inputs_embeds, past_key_values_length\n",
        "        )\n",
        "\n",
        "        # expand encoder attention mask\n",
        "        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n",
        "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
        "            encoder_attention_mask = _expand_mask(encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])\n",
        "\n",
        "        # embed positions\n",
        "        positions = self.embed_positions(input_shape, past_key_values_length)\n",
        "\n",
        "        hidden_states = inputs_embeds + positions\n",
        "\n",
        "        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "\n",
        "        # decoder layers\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attns = () if output_attentions else None\n",
        "        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n",
        "        next_decoder_cache = () if use_cache else None\n",
        "\n",
        "        # check if head_mask has a correct number of layers specified if desired\n",
        "        if head_mask is not None:\n",
        "            assert head_mask.size()[0] == (\n",
        "                len(self.layers)\n",
        "            ), f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n",
        "        for idx, decoder_layer in enumerate(self.layers):\n",
        "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states += (hidden_states,)\n",
        "            dropout_probability = random.uniform(0, 1)\n",
        "            if self.training and (dropout_probability < self.layerdrop):\n",
        "                continue\n",
        "\n",
        "            past_key_value = past_key_values[idx] if past_key_values is not None else None\n",
        "\n",
        "            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n",
        "\n",
        "                if use_cache:\n",
        "                    logger.warn(\n",
        "                        \"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"\n",
        "                        \"`use_cache=False`...\"\n",
        "                    )\n",
        "                    use_cache = False\n",
        "\n",
        "                def create_custom_forward(module):\n",
        "                    def custom_forward(*inputs):\n",
        "                        # None for past_key_value\n",
        "                        return module(*inputs, output_attentions, use_cache)\n",
        "\n",
        "                    return custom_forward\n",
        "\n",
        "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                    create_custom_forward(decoder_layer),\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                    head_mask[idx] if head_mask is not None else None,\n",
        "                    encoder_head_mask[idx] if encoder_head_mask is not None else None,\n",
        "                    None,\n",
        "                )\n",
        "            else:\n",
        "\n",
        "                layer_outputs = decoder_layer(\n",
        "                    hidden_states,\n",
        "                    attention_mask=attention_mask,\n",
        "                    encoder_hidden_states=encoder_hidden_states,\n",
        "                    encoder_attention_mask=encoder_attention_mask,\n",
        "                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n",
        "                    encoder_layer_head_mask=(encoder_head_mask[idx] if encoder_head_mask is not None else None),\n",
        "                    past_key_value=past_key_value,\n",
        "                    output_attentions=output_attentions,\n",
        "                    use_cache=use_cache,\n",
        "                )\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "            if use_cache:\n",
        "                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n",
        "\n",
        "            if output_attentions:\n",
        "                all_self_attns += (layer_outputs[1],)\n",
        "\n",
        "                if encoder_hidden_states is not None:\n",
        "                    all_cross_attentions += (layer_outputs[2],)\n",
        "\n",
        "        # add hidden states from the last decoder layer\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states += (hidden_states,)\n",
        "\n",
        "        next_cache = next_decoder_cache if use_cache else None\n",
        "        if not return_dict:\n",
        "            return tuple(\n",
        "                v\n",
        "                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n",
        "                if v is not None\n",
        "            )\n",
        "        return BaseModelOutputWithPastAndCrossAttentions(\n",
        "            last_hidden_state=hidden_states,\n",
        "            past_key_values=next_cache,\n",
        "            hidden_states=all_hidden_states,\n",
        "            attentions=all_self_attns,\n",
        "            cross_attentions=all_cross_attentions,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5Q6Svy4-CCKQ",
      "metadata": {
        "id": "5Q6Svy4-CCKQ"
      },
      "source": [
        "There are 6 TODOs in the **Model**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ASvnc8ZCCKR",
      "metadata": {
        "id": "1ASvnc8ZCCKR"
      },
      "outputs": [],
      "source": [
        "class Model(PreTrainedModel):\n",
        "    def __init__(self, config: MyConfig):\n",
        "        super().__init__(config)\n",
        "\n",
        "        padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n",
        "\n",
        "        ##  TODO 1: initialise embedding layer with nn.Embedding ##\n",
        "        self.shared = \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##  TODO 2: define the encoder and decoder with previously defined classes ##\n",
        "        self.encoder = \n",
        "        self.decoder = \n",
        "\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.shared\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.shared = value\n",
        "        self.encoder.embed_tokens = self.shared\n",
        "        self.decoder.embed_tokens = self.shared\n",
        "\n",
        "    def get_encoder(self):\n",
        "        ##  TODO 3: return the encoder ##\n",
        "\n",
        "    def get_decoder(self):\n",
        "        ##  TODO 4: return the decoder ##\n",
        "      \n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        decoder_input_ids=None,\n",
        "        decoder_attention_mask=None,\n",
        "        head_mask=None,\n",
        "        decoder_head_mask=None,\n",
        "        encoder_outputs=None,\n",
        "        past_key_values=None,\n",
        "        inputs_embeds=None,\n",
        "        decoder_inputs_embeds=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if encoder_outputs is None:\n",
        "\n",
        "            ##  TODO 5: get the encoder outputs ##\n",
        "            encoder_outputs =\n",
        "\n",
        "\n",
        "\n",
        "        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n",
        "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
        "            encoder_outputs = BaseModelOutput(\n",
        "                last_hidden_state=encoder_outputs[0],\n",
        "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
        "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
        "            )\n",
        "\n",
        "        ##  TODO 6: get the decoder outputs ##\n",
        "        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n",
        "        decoder_outputs = \n",
        "\n",
        "\n",
        "        if not return_dict:\n",
        "            return decoder_outputs + encoder_outputs\n",
        "\n",
        "        return Seq2SeqModelOutput(\n",
        "            last_hidden_state=decoder_outputs.last_hidden_state,\n",
        "            past_key_values=decoder_outputs.past_key_values,\n",
        "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
        "            decoder_attentions=decoder_outputs.attentions,\n",
        "            cross_attentions=decoder_outputs.cross_attentions,\n",
        "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
        "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
        "            encoder_attentions=encoder_outputs.attentions,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0jhpLOXdCCKS",
      "metadata": {
        "id": "0jhpLOXdCCKS"
      },
      "source": [
        "In the following **MTModel**, fill in the missing parts in ***__init__*** and ***forward*** function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HS8-XB5zCCKS",
      "metadata": {
        "id": "HS8-XB5zCCKS"
      },
      "outputs": [],
      "source": [
        "class MTModel(PreTrainedModel):\n",
        "    base_model_prefix = \"model\"\n",
        "    _keys_to_ignore_on_load_missing = [\n",
        "        r\"final_logits_bias\",\n",
        "        r\"encoder\\.version\",\n",
        "        r\"decoder\\.version\",\n",
        "        r\"lm_head\\.weight\",\n",
        "        r\"embed_positions\",\n",
        "    ]\n",
        "\n",
        "    _keys_to_ignore_on_save = [\n",
        "        \"model.encoder.embed_positions.weight\",\n",
        "        \"model.decoder.embed_positions.weight\",\n",
        "    ]\n",
        "\n",
        "    def __init__(self, config: MyConfig):\n",
        "        super().__init__(config)\n",
        "\n",
        "        ##  TODO 7: Define the model ##\n",
        "        self.model = Model(config)\n",
        "\n",
        "        self.register_buffer(\"final_logits_bias\", torch.zeros((1, self.model.shared.num_embeddings)))\n",
        "\n",
        "        ##  TODO 8: Define the head ##\n",
        "        self.lm_head = nn.Linear(config.d_model, self.model.shared.num_embeddings, bias=False)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_encoder(self):\n",
        "\n",
        "        return self.model.get_encoder()\n",
        "\n",
        "    def get_decoder(self):\n",
        "        return self.model.get_decoder()\n",
        "\n",
        "    def resize_token_embeddings(self, new_num_tokens: int) -> nn.Embedding:\n",
        "        new_embeddings = super().resize_token_embeddings(new_num_tokens)\n",
        "        self._resize_final_logits_bias(new_num_tokens)\n",
        "        return new_embeddings\n",
        "\n",
        "    def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n",
        "        old_num_tokens = self.final_logits_bias.shape[-1]\n",
        "        if new_num_tokens <= old_num_tokens:\n",
        "            new_bias = self.final_logits_bias[:, :new_num_tokens]\n",
        "        else:\n",
        "            extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)\n",
        "            new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n",
        "        self.register_buffer(\"final_logits_bias\", new_bias)\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.lm_head = new_embeddings\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        decoder_input_ids=None,\n",
        "        decoder_attention_mask=None,\n",
        "        head_mask=None,\n",
        "        decoder_head_mask=None,\n",
        "        encoder_outputs=None,\n",
        "        past_key_values=None,\n",
        "        inputs_embeds=None,\n",
        "        decoder_inputs_embeds=None,\n",
        "        labels=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Labels for computing the masked language modeling loss. Indices should either be in ``[0, ...,\n",
        "            config.vocab_size]`` or -100 (see ``input_ids`` docstring). Tokens with indices set to ``-100`` are ignored\n",
        "            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``.\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if labels is not None:\n",
        "            if decoder_input_ids is None:\n",
        "                decoder_input_ids = shift_tokens_right(\n",
        "                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n",
        "                )\n",
        "        \n",
        "\n",
        "        ##  TODO 9: get the model output using self.model ##\n",
        "        outputs = \n",
        "\n",
        "\n",
        "        lm_logits = self.lm_head(outputs[0]) + self.final_logits_bias\n",
        "\n",
        "        masked_lm_loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (lm_logits,) + outputs[1:]\n",
        "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
        "\n",
        "        return Seq2SeqLMOutput(\n",
        "            loss=masked_lm_loss,\n",
        "            logits=lm_logits,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            decoder_hidden_states=outputs.decoder_hidden_states,\n",
        "            decoder_attentions=outputs.decoder_attentions,\n",
        "            cross_attentions=outputs.cross_attentions,\n",
        "            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n",
        "            encoder_hidden_states=outputs.encoder_hidden_states,\n",
        "            encoder_attentions=outputs.encoder_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "    def prepare_inputs_for_generation(\n",
        "        self,\n",
        "        decoder_input_ids,\n",
        "        past=None,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        use_cache=None,\n",
        "        encoder_outputs=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        # cut decoder_input_ids if past is used\n",
        "        if past is not None:\n",
        "            decoder_input_ids = decoder_input_ids[:, -1:]\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n",
        "            \"encoder_outputs\": encoder_outputs,\n",
        "            \"past_key_values\": past,\n",
        "            \"decoder_input_ids\": decoder_input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"head_mask\": head_mask,\n",
        "            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n",
        "        }\n",
        "\n",
        "    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n",
        "        return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n",
        "\n",
        "    def adjust_logits_during_generation(self, logits, cur_len):\n",
        "        logits[:, self.config.pad_token_id] = float(\"-inf\")  # never predict pad token.\n",
        "        return logits\n",
        "\n",
        "    @staticmethod\n",
        "    def _reorder_cache(past, beam_idx):\n",
        "        reordered_past = ()\n",
        "        for layer_past in past:\n",
        "            # cached cross_attention states don't have to be reordered -> they are always the same\n",
        "            reordered_past += (\n",
        "                tuple(past_state.index_select(0, beam_idx) for past_state in layer_past[:2]) + layer_past[2:],\n",
        "            )\n",
        "        return reordered_past"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KRBi6EPPCCKT",
      "metadata": {
        "id": "KRBi6EPPCCKT"
      },
      "source": [
        "Let's try the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y7SAzOnhCCKU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402,
          "referenced_widgets": [
            "d90b02be82ab42eca0908ed9a2c83f08",
            "798595cb96314ae7bc6e502c788f94b7",
            "e0354eb9172240539234a2559376ff8e",
            "eab6cd58869e452a8704cffcf1234a36",
            "70f56f9605cc4ef086164912440686d2",
            "7aaf330836db446ea1bef05f34884cb4",
            "58a5b881825b461e8d8c220d69f17734",
            "20fb0725d0e34e1d9c7a67d38b9bda2d"
          ]
        },
        "id": "Y7SAzOnhCCKU",
        "outputId": "92866e84-fe5e-4f10-9918-5520d5eb88d5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d90b02be82ab42eca0908ed9a2c83f08",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-28553142e8cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMTModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Helsinki-NLP/opus-mt-en-de\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mbleu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmt_test_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbleu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-b2da70d8a3a5>\u001b[0m in \u001b[0;36mevaluate_mt\u001b[0;34m(model, mt_test_dataset)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtarg_text_origin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmt_test_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_sents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtranslated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmt_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_seq2seq_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtranslated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmt_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtranslated\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)\u001b[0m\n\u001b[1;32m    972\u001b[0m                 \u001b[0moutput_scores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m                 \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m             )\n\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)\u001b[0m\n\u001b[1;32m   1608\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1609\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1610\u001b[0;31m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1611\u001b[0m             )\n\u001b[1;32m   1612\u001b[0m             \u001b[0mnext_token_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-c20e07086a7c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_logits_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mmasked_lm_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "mt_tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')\n",
        "mt_dataset = Multi30K(mt_tokenizer, 'train.en', 'train.de')\n",
        "mt_test_dataset = Multi30K(mt_tokenizer, 'test.en', 'test.de')\n",
        "\n",
        "\n",
        "model = MTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
        "\n",
        "bleu = evaluate_mt(model, mt_test_dataset)\n",
        "\n",
        "print(bleu)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "lab06.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0712735baf9b40dea3e3b38f00cb6f1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91971772860941ac8ab83cde9292fc3a",
            "max": 1132,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_968b7b1d37064914a5c00b00f500d9e2",
            "value": 1132
          }
        },
        "20fb0725d0e34e1d9c7a67d38b9bda2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25e987dead5e40cbafb71feb099cda81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36f7441e69d24a359d26ebf9fa4af07f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "3b078a8aa9b145dabf05a8ca93854fc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0712735baf9b40dea3e3b38f00cb6f1e",
              "IPY_MODEL_775b4ca6c0e94ba08c4ef32779311afa"
            ],
            "layout": "IPY_MODEL_25e987dead5e40cbafb71feb099cda81"
          }
        },
        "58a5b881825b461e8d8c220d69f17734": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70f56f9605cc4ef086164912440686d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "775b4ca6c0e94ba08c4ef32779311afa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6e9683c361240ec85d67aa39775b709",
            "placeholder": "​",
            "style": "IPY_MODEL_8a9c46189a7449939de1023699ce1ed0",
            "value": " 1.13k/1.13k [00:12&lt;00:00, 89.4B/s]"
          }
        },
        "798595cb96314ae7bc6e502c788f94b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7aaf330836db446ea1bef05f34884cb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f51eba2be474e5486ca9a3dc1487037": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_adba8794c6d6470083d805d42111b4d6",
              "IPY_MODEL_c0d35b06063c4113aa88d899fc26a28a"
            ],
            "layout": "IPY_MODEL_a6b2c87fafa04f3fa49c11cf47a0e527"
          }
        },
        "7f87a9f9fda245bab41de3aa49ac4c62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a9c46189a7449939de1023699ce1ed0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91971772860941ac8ab83cde9292fc3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "968b7b1d37064914a5c00b00f500d9e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "980bf5380ccc467284f68a701985951e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6b2c87fafa04f3fa49c11cf47a0e527": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6e9683c361240ec85d67aa39775b709": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adba8794c6d6470083d805d42111b4d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f87a9f9fda245bab41de3aa49ac4c62",
            "max": 297928209,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36f7441e69d24a359d26ebf9fa4af07f",
            "value": 297928209
          }
        },
        "c0d35b06063c4113aa88d899fc26a28a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f81ac79ac55e45c7908113bf8a69c365",
            "placeholder": "​",
            "style": "IPY_MODEL_980bf5380ccc467284f68a701985951e",
            "value": " 298M/298M [00:05&lt;00:00, 50.3MB/s]"
          }
        },
        "d90b02be82ab42eca0908ed9a2c83f08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0354eb9172240539234a2559376ff8e",
              "IPY_MODEL_eab6cd58869e452a8704cffcf1234a36"
            ],
            "layout": "IPY_MODEL_798595cb96314ae7bc6e502c788f94b7"
          }
        },
        "e0354eb9172240539234a2559376ff8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "  9%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7aaf330836db446ea1bef05f34884cb4",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_70f56f9605cc4ef086164912440686d2",
            "value": 93
          }
        },
        "eab6cd58869e452a8704cffcf1234a36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20fb0725d0e34e1d9c7a67d38b9bda2d",
            "placeholder": "​",
            "style": "IPY_MODEL_58a5b881825b461e8d8c220d69f17734",
            "value": " 93/1000 [01:28&lt;15:29,  1.03s/it]"
          }
        },
        "f81ac79ac55e45c7908113bf8a69c365": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
