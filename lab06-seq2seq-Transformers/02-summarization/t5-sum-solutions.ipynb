{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXk4O_B___rO"
      },
      "outputs": [],
      "source": [
        "! pip install datasets transformers rouge-score nltk torch numpy matplotlib      # << Uncomment to install packages\n",
        "import transformers\n",
        "print(transformers.__version__)     # Should be >= 4.11.0\n",
        "import torch\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEJBSTyZIrIb"
      },
      "source": [
        "# Fine-tuning a model on a summarization task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTCFado4IrIc"
      },
      "source": [
        "In this notebook, we will see how to fine-tune one of the [HuggingFace Transformers](https://github.com/huggingface/transformers) model for a summarization task. We will use the [XSum dataset](https://arxiv.org/pdf/1808.08745.pdf) (for extreme summarization) which contains BBC articles accompanied with single-sentence summaries.\n",
        "\n",
        "![Widget inference on a summarization task](https://github.com/huggingface/notebooks/blob/master/examples/images/summarization.png?raw=1)\n",
        "\n",
        "We will see how to easily load the dataset for this task using HuggingFace Datasets and how to fine-tune a model on it using the `Trainer` API.\n",
        "\n",
        "This tutorial is draws from the Huggingface Summarization Tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7QYTpxXIrIl"
      },
      "source": [
        "We will use the [HuggingFace Datasets](https://github.com/huggingface/datasets) library to download the data we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions `load_dataset`. The Datasets library is a great resource for conveniently working with common datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IreSlFmlIrIm",
        "outputId": "6564e8c9-ec95-4527-84ba-b83e3b5a2cbb"
      },
      "outputs": [],
      "source": [
        "from datasets import list_datasets, load_dataset\n",
        "\n",
        "# Can see all possible datasets as follows\n",
        "all_datasets = list_datasets()\n",
        "print(all_datasets[:10], len(all_datasets))\n",
        "\n",
        "raw_datasets = load_dataset(\"xsum\")\n",
        "\n",
        "# Each dataset has a train, val and test split\n",
        "print(raw_datasets.keys(), '\\n', raw_datasets['train'])\n",
        "\n",
        "# An XSum sample looks as follows\n",
        "print(raw_datasets[\"train\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHUmphG3IrI3"
      },
      "source": [
        "### EDA\n",
        "\n",
        "To get a sense of what the data looks like, we will write a function to display some random elements. We will also perform some basic EDA (exploratory data analysis) by computing the mean and standard deviation token counts for the source and target documents.\n",
        "\n",
        "1. Complete `show_random_elements` below. This should display a table containing `num_examples` samples from the specified `dataset`, with fields `Souce Document`, `Target Document` and `Document ID`\n",
        "2. EDA: \n",
        " - Tokenize the document (the best option would be to use the tokenizer but here we will just split on spaces) and print the mean count and standard deviation for source and target documents for each dataset\n",
        " - Plot histograms of the source and target token counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3j8APAoIrI3"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import display, HTML\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_random_elements(dataset, num_examples=5):\n",
        "    ## YOUR CODE HERE ##\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "    \n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, datasets.ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "    display(HTML(df.to_html()))\n",
        "\n",
        "\n",
        "def get_token_counts(dataset):\n",
        "    ## YOUR CODE HERE ##\n",
        "    src_counts = [len(x.split()) for x in dataset['document']]\n",
        "    tgt_counts = [len(x.split()) for x in dataset['summary']]\n",
        "    return src_counts, tgt_counts\n",
        "\n",
        "\n",
        "def token_counts_summary(raw_dataset):\n",
        "    ## YOUR CODE HERE ##\n",
        "    for name,dataset in raw_dataset.items():\n",
        "        src_counts, tgt_counts = get_token_counts(dataset)\n",
        "\n",
        "        mean_cnt = np.mean(src_counts)\n",
        "        std_cnt = np.std(src_counts)\n",
        "        print(f'Source documents from dataset: {name:10}\\tMean tokens: {mean_cnt:.2f}\\tStd tokens: {std_cnt:.2f}')\n",
        "\n",
        "        mean_cnt = np.mean(tgt_counts)\n",
        "        std_cnt = np.std(tgt_counts)\n",
        "        print(f'Target documents from dataset: {name:10}\\tMean tokens: {std_cnt:.2f}\\tStd tokens: {std_cnt:.2f}')\n",
        "\n",
        "\n",
        "def plot_token_counts(dataset):\n",
        "    ## YOUR CODE HERE ##\n",
        "    src_counts, tgt_counts = get_token_counts(dataset)\n",
        "    f, (ax0, ax1) = plt.subplots(1,2, figsize=(14,8))\n",
        "    ax0.hist(src_counts, bins=20)\n",
        "    ax1.hist(tgt_counts, bins=20)\n",
        "    ax0.set_title(\"Source Token Count Distribution\")\n",
        "    ax1.set_title(\"Target Token Count Distribution\")\n",
        "\n",
        "\n",
        "# show_random_elements(raw_datasets[\"val\"])\n",
        "\n",
        "plot_token_counts(raw_datasets[\"validation\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAWdqcUBIrJC"
      },
      "source": [
        "### Loading the metric\n",
        "\n",
        "To evaluate our model's performance, we will use the ROUGE summarization metric. This is provided natively within the dataset library and can be loaded similarly to how we loaded the dataset above. **Note** this is a big advantage in practise as i) metrics can be fiddly to implement manually and ii) difficult to align completely across implmentations as decisions like lemmatization, tokenization and punctation-handling can create large discrepancies in scores.\n",
        "\n",
        "You can call its `compute` method with your predictions and labels, which need to be list of decoded strings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XN1Rq0aIrJC",
        "outputId": "a4405435-a8a9-41ff-9f79-a13077b587c7"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "metric = load_metric(\"rouge\")\n",
        "\n",
        "# help(metric)      # << Uncomment to see more about the ROUGE eval metric\n",
        "\n",
        "# Try it out below\n",
        "# fake_preds = ## YOUR CODE HERE\n",
        "# fake_labels = ## YOUR CODE HERE\n",
        "## COMPUTE METRIC HERE ## \n",
        "\n",
        "fake_preds = [\"i love nlp\", \"nlp is great\"]\n",
        "fake_labels = [\"i love nlp\", \"nlp is fun\"]\n",
        "metric.compute(predictions=fake_preds, references=fake_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9qywopnIrJH"
      },
      "source": [
        "## Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVx71GdAIrJH"
      },
      "source": [
        "We will proprocess the data using the Huggingface `Tokenizer`. This tokenizes and indexes the inputs and put it in a format the model expects. It also generate the other inputs that the model requires.\n",
        "\n",
        "-------------- Question ---------------------\n",
        "\n",
        "1. Each Huggingface model has a paired tokenizer. Why is it important to use the appropriate tokenizer?\n",
        "\n",
        "Answer: \n",
        "Otherwise the tokenizer vocabulary may not be aligned with the model's embedding matrics. This could result in i) incorrect token embeddings (as token indices are incorrect), ii) index errors (if the vocbulary shrinks) iii) incorrect sub-word tokenization (e.g. if the model was trained on sentencepiece but the tokenizer uses byte-pair-encoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXNLu_-nIrJI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"t5-small\"       # Can find more options at https://huggingface.co/models?sort=downloads&search=t5    \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using the tokenizer\n",
        "\n",
        "Try out the tokenizer in the cell below.\n",
        "\n",
        "---------------- Question ---------------\n",
        "\n",
        "1. The tokenizer output has different fields for different tokenizers. \n",
        "\n",
        "i) Confirm this by writing code below to instantiate a BERT tokenizer and comparing the outputs across the two tokenizers of when tokenizing a string. You may find [this page](https://huggingface.co/models) helpful.\n",
        "\n",
        "ii) Why does the BERT tokenizer have an additional field to the T5 tokenizer?\n",
        "\n",
        "Answer: most models were trained to recognize sequences defined using special tokens, e.g. [CLS] seq1 [SEP] seq2 [SEP] is a common encoding pattern for tasks involving two sequences. BERT was trained using an additional `token_type_ids` field to which is a binary mask flagging to explicitly differentiate between the two sequences. BERT will likely perform poorly evaluated without the type ids. See [this tutorial](https://huggingface.co/transformers/preprocessing.html) if you're interested to find out more about tokenization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the tokenizer below\n",
        "print(tokenizer(\"We love NLP!\"))\n",
        "# print(tokenizer(## YOUR CODE HERE))\n",
        "\n",
        "## CODE FOR 1.i) HERE ##\n",
        "tokenizer_ = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokenizer_('We love NLP')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Additional tokenizer functionalities:\n",
        "\n",
        "- The tokenizer can be fed a list of strings\n",
        "- When tokenizing the target documents, using `tokenizer.as_target_tokenizer()` to ensure the target receives the appropriate special tokens (although here the source and target are tokenized identically)\n",
        "- We can convert back from ids to tokens by using `tokenizer.convert_ids_to_tokens()`\n",
        "\n",
        "--------------- Question --------------\n",
        "\n",
        "1. The output of `tokenizer.convert_ids_to_tokens()` is different from the inital string. Why is this?\n",
        "\n",
        "Answer: because of subword tokenization. Subwords are identified by sub-words not having an underscore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIaPIvgF__rs",
        "outputId": "3e87d343-a4a5-4d72-b147-9cd3d5f8efc9"
      },
      "outputs": [],
      "source": [
        "print(tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"]))\n",
        "\n",
        "with tokenizer.as_target_tokenizer():\n",
        "    print(tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"]))\n",
        "\n",
        "print(tokenizer.convert_ids_to_tokens(tokenizer(\"We love NLP!\")['input_ids']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C0hcmp9IrJQ"
      },
      "source": [
        "T5 was trained within a multitask framework such that it can perform multiple tasks out-of-the-box. We prefix the inputs with \"summarize: \" to prompt the model to deliver the correct outputs.\n",
        "\n",
        "We can then write the function that will preprocess our samples. We just feed them to the `tokenizer` with the argument `truncation=True`. This will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model. The padding will be dealt with later on (in a data collator) so we pad examples to the longest length in the batch and not the whole dataset.\n",
        "\n",
        "Complete the `preprocess_function` below to tokenize the text (**hint**: don't forget prefix or the context manager ;) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc0BSBLIIrJQ"
      },
      "outputs": [],
      "source": [
        "prefix = \"summarize: \" if model_checkpoint.startswith(\"t5-\") else \"\"\n",
        "\n",
        "max_input_length = 1024\n",
        "max_target_length = 128\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    ## YOUR CODE HERE ##\n",
        "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-b70jh26IrJS",
        "outputId": "acd3a42d-985b-44ee-9daa-af5d944ce1d9"
      },
      "outputs": [],
      "source": [
        "# Test it using the below call\n",
        "preprocess_function(raw_datasets['train'][:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS-6iXTkIrJT"
      },
      "source": [
        "This function can be applied to all our datasets using the `map` method of our `dataset` object. The results are automatically cached to avoid spending time on this step the next time you run your notebook. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDtsaJeVIrJT",
        "outputId": "aa4734bf-4ef5-4437-9948-2c16363da719"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = raw_datasets.map(\n",
        "    preprocess_function, \n",
        "    batched=True            # This employs multithreading to speed up tokenization\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using T5 out-of-the-box\n",
        "\n",
        "In the lecture slides you were shown the below image:\n",
        "\n",
        "![T5 img](figs/t5.png)\n",
        "\n",
        "Here, we will experiment with some of T5's capabilities without pre-training. First, we will download the model using `AutoModelForSeq2SeqLM` class using the `from_pretrained` method (this caches the model for us).\n",
        "\n",
        "To illustrate the impact of the prompt, we will try T5 using two different prompts using the same string. Note that we are using the small version without fine-tuning so the results may not be great."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your task\n",
        "Complete the `tok_and_gen` function below. This requires the following steps:\n",
        "- Tokenize the inputs\n",
        "- Generate output ids (hint: using model.generate. Google this for more details)\n",
        "- Convert the ids to string\n",
        "- Print the string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_str = 'Artificial general intelligence (AGI) is the hypothetical ability of an intelligent agent to understand or learn any intellectual task that a human being can.[1] It is a primary goal of some artificial intelligence research and a common topic in science fiction and futures studies.'\n",
        "\n",
        "prefixes = [\"translate English to German: \", \"summarize: \"]     # What happens if we make up a prompt? Why?\n",
        "\n",
        "def tok_and_gen(model, tokenizer, input, prefix=''):\n",
        "    ## YOUR CODE HERE ##\n",
        "    inputs = tokenizer(prefix + input, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    output_sequences = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"].cuda(),\n",
        "        attention_mask=inputs[\"attention_mask\"].cuda(),\n",
        "    )\n",
        "\n",
        "    print(f'Using prefix {task_prefix}: ', tokenizer.batch_decode(output_sequences, skip_special_tokens=True))\n",
        "\n",
        "\n",
        "for task_prefix in prefixes:\n",
        "    tok_and_gen(model, tokenizer, task_prefix, input_str)    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "545PP3o8IrJV"
      },
      "source": [
        "## Fine-tuning the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBiW8UpKIrJW"
      },
      "source": [
        "Now that our data is ready and we have played around with our model, we can fine-tune it. \n",
        "\n",
        "HuggingFace provides an API for training a seq2seq model: the `Seq2SeqTrainer`. To instantiate this, we will need to define three more things. The most important is the [`Seq2SeqTrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.Seq2SeqTrainingArguments), which is a class that contains all the attributes to customize the training. It requires a folder name for saving checkpoints of the model, and all other arguments are optional:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8WyIhF-__sL"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"{model_name}-finetuned-xsum\",     # Ouptut folder\n",
        "    # Eval strategy\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    # Could alternatively be the following to eval every epoch:\n",
        "    # evaluation_strategy=\"epoch\",\n",
        "\n",
        "    # LR. Should be small (<1e-4)\n",
        "    learning_rate=2e-5,\n",
        "\n",
        "    # Batch size during training and eval\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "\n",
        "    # Limits the number of models saved during training. Important to prevent memory clogging up!\n",
        "    save_total_limit=3,\n",
        "\n",
        "    # Properly generate summaries during eval\n",
        "    predict_with_generate=True,\n",
        "    \n",
        "    # Mixed precision training (speeds up training - see Nvidia-apex for more details)\n",
        "    fp16=True,\n",
        "\n",
        "    weight_decay=0.01,\n",
        "    num_train_epochs=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km3pGVdTIrJc"
      },
      "source": [
        "Then, we need a special kind of data collator, which will not only pad the inputs to the maximum length in the batch, but also the labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tMJC9gy__sL"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sZOdRlRIrJd"
      },
      "source": [
        "The last thing to define for our `Seq2SeqTrainer` is how to compute the metrics from the predictions. We need to define a function for this, which will just use the `metric` we loaded earlier, and we have to do a bit of pre-processing to decode the predictions into texts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmvbnJ9JIrJd"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    \n",
        "    # Rouge expects a newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "    \n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    # Extract a few results\n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "    \n",
        "    # Add mean generated length\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    \n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXuFTAzDIrJe"
      },
      "source": [
        "Then we just need to pass all of this along with our datasets to the `Seq2SeqTrainer`. This is the general Huggingface API adapted for training seq2seq models.\n",
        "\n",
        "## Your task \n",
        "- Instantiate the trainer class and begin finetuning on the XSum dataset. You may find [this page](https://huggingface.co/docs/transformers/main_classes/trainer) helpful.\n",
        "\n",
        "**Note:** training will likely take a while so you may want to end training once you are satisfied it is working correctly in order to progress to the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imY1oC3SIrJf"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE HERE ##\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complexity Analysis\n",
        "\n",
        "Here we will analyze the computational complexity of the Transformer. One of the main drawbacks of this architecture is that the time and memory complexity scales poorly with respect to input length. This is particularly problematic for summarization as long document summarization can become prohibitively expensive.\n",
        "\n",
        "---------------- Questions/tasks ----------------\n",
        "\n",
        "1. Which component of the Transformer does the poor complexity problems mentioned above referred to? Explain why this is the case.\n",
        "2. We will profile the time complexity of T5 for different input sequence lengths. Complete the function below which will plot the time of T5's forward pass. You can use the inbuilt pytorch profiler or a simpler method (e.g. `time.time`) as you prefer.\n",
        "- We will first do this up to 512 tokens on a log base 2 scale (i.e. 1, 2, 4, 8, ... 512)\n",
        "- Do this for the encoder (holding decoder input length fixed) and the decoder (holding encoder input length fixed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from time import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def profile_seq2seq(model, max_len, batch_size=1):\n",
        "    # Create dummy input tensors\n",
        "    base_encoder_input = torch.tensor([[10]]).cuda()\n",
        "    base_decoder_input = torch.tensor([[10]]).cuda()\n",
        "    # Make a larger batch to make trends more evident\n",
        "    base_encoder_input = base_encoder_input.repeat(batch_size,1)\n",
        "    base_decoder_input = base_decoder_input.repeat(batch_size,1)\n",
        "    \n",
        "    ## YOUR CODE HERE ##\n",
        "    # Benchmark encoder holding decoder fixed\n",
        "    run_time_profile(model, max_len, base_encoder_input, base_decoder_input, 'encoder', True)\n",
        "\n",
        "    # Benchmark decoder holding encoder fixed\n",
        "    run_time_profile(model, max_len, base_encoder_input, base_decoder_input, 'decoder', True)\n",
        "\n",
        "\n",
        "def run_time_profile(model, max_len, base_encoder_input, base_decoder_input, vary, do_plot=False, n_trials=2):\n",
        "    times = []\n",
        "    for i in range(torch.log2(torch.tensor(max_len)).int()+1):\n",
        "        len = 2 ** i\n",
        "        \n",
        "        encoder_input = base_encoder_input\n",
        "        decoder_input = base_decoder_input\n",
        "        if vary == 'encoder':\n",
        "           encoder_input = base_encoder_input.repeat(1,len)\n",
        "        elif vary == 'decoder':\n",
        "           decoder_input = base_decoder_input.repeat(1,len) \n",
        "\n",
        "        # print(encoder_input.shape, decoder_input.shape)\n",
        "        ts = time()\n",
        "        for i in range(n_trials):\n",
        "            model(input_ids=encoder_input, decoder_input_ids=decoder_input)\n",
        "        te = time()\n",
        "        times.append(te - ts)\n",
        "\n",
        "    if do_plot:\n",
        "        plt.plot(times)\n",
        "        plt.show()\n",
        "\n",
        "    return times\n",
        "\n",
        "\n",
        "profile_seq2seq(model, 512, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The above plots may or may not show an upward trajectory. Now try again with a larger batch size (e.g. 8). Now they should show an upward slope as the forward pass becomes more expensive relative to other overheads."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Summarization",
      "provenance": []
    },
    "interpreter": {
      "hash": "65bdeda9c857d200db339bc1461d10a02d342df00d855a84b263b8efa6d3dc02"
    },
    "kernelspec": {
      "display_name": "Python 3.7.11 64-bit ('NLPlabs': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
