{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab04_POStagging_with_solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc8hZvQ2u-Nb"
      },
      "source": [
        "# The Task - Part of Speech Tagging\n",
        "\n",
        "In this tutorial, we look at the structured prediction task of [Part Of Speech Tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging). This task consists of tagging/assigning the tokens in a sentence with Part of Speech (POS) tags/labels like Noun, Verb, Adjective, Adverb, Determiner, Pronoun, etcetera. \n",
        "\n",
        "We will,\n",
        "\n",
        "1.   Download and explore a POS tagged corpus (dataset).\n",
        "2.   Implement a couple of naive approaches.\n",
        "3.   Understand Hidden Markov Model\n",
        "4.   Understand Viterbi algorithm with an example.\n",
        "5.   Compute transition probabilities and emission probabilities.\n",
        "6.   Implement Viterbi algorithm.\n",
        "7.   (Optional) Implement a RNN-based Tagging model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdWej774F1Cs"
      },
      "source": [
        "# 1. The Corpus\n",
        "\n",
        "We will work with simple versions of the [Brown Corpus](https://en.wikipedia.org/wiki/Brown_Corpus) and [Penn Treebank Corpus](https://catalog.ldc.upenn.edu/docs/LDC95T7/cl93.html) which can be accessed using [NLTK](https://www.nltk.org/). \n",
        "\n",
        "**Let's begin by downloading the corpuses**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGb_dEPlzKia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdf2b10f-f411-400e-9f35-b966a88aa203"
      },
      "source": [
        "import nltk\n",
        "\n",
        "# Downloading the corpuses\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('brown')\n",
        "nltk.download('treebank')\n",
        "\n",
        "from nltk.corpus import brown\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "brown_corpus = list(brown.tagged_sents(tagset='universal'))\n",
        "treebank_corpus = list(treebank.tagged_sents(tagset='universal'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20mksE2cB8PI"
      },
      "source": [
        "**Exploring the corpuses** \n",
        "\n",
        "The downloaded corpuses are a list of sentences. Each sentence is a list of tagged tokens. The tagged tokens are tuples of the form ```(token, tag)```. Each token and each tag is a string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLu7_w7Uj_Wb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42016a70-0df8-48dd-9ea9-3cd6bf5178c5"
      },
      "source": [
        "print('Size of brown_corpus is', len(brown_corpus), 'sentences\\n')\n",
        "print('Size of treebank_corpus is', len(treebank_corpus), 'sentences\\n')\n",
        "print('A sample from the brown_corpus:\\n', brown_corpus[2])\n",
        "print('\\nA sample from the treebank_corpus:\\n', treebank_corpus[3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of brown_corpus is 57340 sentences\n",
            "\n",
            "Size of treebank_corpus is 3914 sentences\n",
            "\n",
            "A sample from the brown_corpus:\n",
            " [('The', 'DET'), ('September-October', 'NOUN'), ('term', 'NOUN'), ('jury', 'NOUN'), ('had', 'VERB'), ('been', 'VERB'), ('charged', 'VERB'), ('by', 'ADP'), ('Fulton', 'NOUN'), ('Superior', 'ADJ'), ('Court', 'NOUN'), ('Judge', 'NOUN'), ('Durwood', 'NOUN'), ('Pye', 'NOUN'), ('to', 'PRT'), ('investigate', 'VERB'), ('reports', 'NOUN'), ('of', 'ADP'), ('possible', 'ADJ'), ('``', '.'), ('irregularities', 'NOUN'), (\"''\", '.'), ('in', 'ADP'), ('the', 'DET'), ('hard-fought', 'ADJ'), ('primary', 'NOUN'), ('which', 'DET'), ('was', 'VERB'), ('won', 'VERB'), ('by', 'ADP'), ('Mayor-nominate', 'NOUN'), ('Ivan', 'NOUN'), ('Allen', 'NOUN'), ('Jr.', 'NOUN'), ('.', '.')]\n",
            "\n",
            "A sample from the treebank_corpus:\n",
            " [('A', 'DET'), ('form', 'NOUN'), ('of', 'ADP'), ('asbestos', 'NOUN'), ('once', 'ADV'), ('used', 'VERB'), ('*', 'X'), ('*', 'X'), ('to', 'PRT'), ('make', 'VERB'), ('Kent', 'NOUN'), ('cigarette', 'NOUN'), ('filters', 'NOUN'), ('has', 'VERB'), ('caused', 'VERB'), ('a', 'DET'), ('high', 'ADJ'), ('percentage', 'NOUN'), ('of', 'ADP'), ('cancer', 'NOUN'), ('deaths', 'NOUN'), ('among', 'ADP'), ('a', 'DET'), ('group', 'NOUN'), ('of', 'ADP'), ('workers', 'NOUN'), ('exposed', 'VERB'), ('*', 'X'), ('to', 'PRT'), ('it', 'PRON'), ('more', 'ADV'), ('than', 'ADP'), ('30', 'NUM'), ('years', 'NOUN'), ('ago', 'ADP'), (',', '.'), ('researchers', 'NOUN'), ('reported', 'VERB'), ('0', 'X'), ('*T*-1', 'X'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxhJbLhgM4t4"
      },
      "source": [
        "Note: The tokens in the corpuses are not pre-processed and we will continue for now without pre-processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czVkO_--xNWW"
      },
      "source": [
        "**Exploring POS tags** \n",
        "\n",
        "**Q1:** How many unique POS tags, also called the tagset, do we have in these corpuses? Save the unique POS tags in a list called ```tagset```.\n",
        "\n",
        "**Q2:** Enumerate all the POS tags and its frequencies/counts. Save it in a python dictionary ```count_tags_in_brown``` for the brown corpus and a python dictionary ```count_tags_in_treebank``` for the treebank corpus. (Sanity check - ```count_tags_in_brown['NUM']``` should be ```14874```) \n",
        "\n",
        "**Q3:** Which POS tag occurs most frequently and which POS tag occurs least frequently in both the corpuses?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7KgaiN_yT6A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bbfb28e-b472-40ce-c169-ef6928a5dbca"
      },
      "source": [
        "import operator\n",
        "\n",
        "# For brown, computing count_tags_in_brown\n",
        "count_tags_in_brown = {}\n",
        "for sentence in brown_corpus:\n",
        "  for (token, tag) in sentence:\n",
        "    count_tags_in_brown[tag] = count_tags_in_brown.get(tag, 0) + 1\n",
        "\n",
        "# Tagset\n",
        "tagset = count_tags_in_brown.keys()\n",
        "\n",
        "# For treebank, computing count_tags_in_treebank\n",
        "count_tags_in_treebank = {}\n",
        "for sentence in treebank_corpus:\n",
        "  for (token, tag) in sentence:\n",
        "    count_tags_in_treebank[tag] = count_tags_in_treebank.get(tag, 0) + 1\n",
        "\n",
        "# Sorting to find the most frequent and the least frequent POS tags\n",
        "sorted_count_tags_in_brown = sorted(count_tags_in_brown.items(),\\\n",
        "                                     key=operator.itemgetter(1))\n",
        "sorted_count_tags_in_treebank = sorted(count_tags_in_treebank.items(),\\\n",
        "                                     key=operator.itemgetter(1))\n",
        "\n",
        "\n",
        "# Answers\n",
        "print('For brown corpus')\n",
        "print('There are', len(sorted_count_tags_in_brown), 'POS tags')\n",
        "print('These are', count_tags_in_brown.keys())\n",
        "print('The frequencies are', sorted_count_tags_in_brown)\n",
        "print('The most frequent tag is', sorted_count_tags_in_brown[-1])\n",
        "print('The least frequent tag is', sorted_count_tags_in_brown[0])\n",
        "\n",
        "print('\\nFor treebank corpus')\n",
        "print('There are', len(sorted_count_tags_in_treebank), 'POS tags')\n",
        "print('These are', count_tags_in_treebank.keys())\n",
        "print('The frequencies are', sorted_count_tags_in_treebank)\n",
        "print('The most frequent tag is', sorted_count_tags_in_treebank[-1])\n",
        "print('The least frequent tag is', sorted_count_tags_in_treebank[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For brown corpus\n",
            "There are 12 POS tags\n",
            "These are dict_keys(['DET', 'NOUN', 'ADJ', 'VERB', 'ADP', '.', 'ADV', 'CONJ', 'PRT', 'PRON', 'NUM', 'X'])\n",
            "The frequencies are [('X', 1386), ('NUM', 14874), ('PRT', 29829), ('CONJ', 38151), ('PRON', 49334), ('ADV', 56239), ('ADJ', 83721), ('DET', 137019), ('ADP', 144766), ('.', 147565), ('VERB', 182750), ('NOUN', 275558)]\n",
            "The most frequent tag is ('NOUN', 275558)\n",
            "The least frequent tag is ('X', 1386)\n",
            "\n",
            "For treebank corpus\n",
            "There are 12 POS tags\n",
            "These are dict_keys(['NOUN', '.', 'NUM', 'ADJ', 'VERB', 'DET', 'ADP', 'CONJ', 'X', 'ADV', 'PRT', 'PRON'])\n",
            "The frequencies are [('CONJ', 2265), ('PRON', 2737), ('ADV', 3171), ('PRT', 3219), ('NUM', 3546), ('ADJ', 6397), ('X', 6613), ('DET', 8725), ('ADP', 9857), ('.', 11715), ('VERB', 13564), ('NOUN', 28867)]\n",
            "The most frequent tag is ('NOUN', 28867)\n",
            "The least frequent tag is ('CONJ', 2265)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0e2PCGl0aF7"
      },
      "source": [
        "**Splitting into Training and Test sets**\n",
        "\n",
        "We randomly sample 4000 sentences from the brown corpus as the ```test_set```. The rest of the brown corpus forms the ```train_set```. We will use the treebank corpus as the ```external_test_set```. \n",
        "\n",
        "Note: We could have tried other kinds of train-test splits too like 80-20 splits or mix the external test set with the brown test set, etcetera.\n",
        "\n",
        "Also, for convenience, we separate the tokens and the tags out from the tuples ```(token, tag)``` to create two separate sets of ```train_set_X``` and ```train_set_Y``` (X for tokens only, Y for POS tags only). We do the same with the ```test_set``` and the ```external_test_set```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6qbnouxkbOW"
      },
      "source": [
        "import random\n",
        "\n",
        "# For consistency of results everytime you run this cell\n",
        "copy_of_brown = brown_corpus.copy() \n",
        "random.seed(20)                \n",
        "\n",
        "# Splitting into train and test and external_test sets\n",
        "random.shuffle(copy_of_brown)\n",
        "train_set = copy_of_brown[4000:]\n",
        "test_set = copy_of_brown[:4000]  # 4000 samples in the test set\n",
        "external_test_set = treebank_corpus\n",
        "\n",
        "# Separating tokens and tags\n",
        "def _split_into_tokens_and_tags(given_set):\n",
        "  \"\"\" Separating tokens and tags out from a list of list of (token, tag) tuples\n",
        "\n",
        "  Arguments:\n",
        "    given_set = a list of list of (token, tag) tuples. token and tag are strings\n",
        "\n",
        "  Returns:\n",
        "    token_set = a list of list of tokens only. order and shape is maintained.  \n",
        "    tag_set = a list of list of tags only. order and shape is maintained.\n",
        "  \"\"\"\n",
        "  token_set = given_set.copy() \n",
        "  tag_set = given_set.copy() \n",
        "\n",
        "  for i in range(len(given_set)):\n",
        "    (token_only, tag_only) = zip(*given_set[i])\n",
        "    token_set[i] = list(token_only)  # 0th element of tuple is token \n",
        "    tag_set[i] = list(tag_only)      # 1st element of tuple is POS tag\n",
        "  \n",
        "  return (token_set, tag_set)\n",
        "\n",
        "# Creating x (sentences) and Y (tags) splits\n",
        "(train_set_x, train_set_Y) = _split_into_tokens_and_tags(train_set)\n",
        "(test_set_x, test_set_Y) = _split_into_tokens_and_tags(test_set)\n",
        "(external_test_set_x, external_test_set_Y) \\\n",
        "= _split_into_tokens_and_tags(external_test_set)\n",
        "\n",
        "# Example of how *_x and *_Y look like\n",
        "# * could be train_set or test_set or external_test_set\n",
        "#print(test_set_x) # List of sentences where each sentence is a list of tokens\n",
        "#print(test_set_Y) # List of list of tags of the same shape as test_set_x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5mKQmJb-3hq"
      },
      "source": [
        "# 2. Naive approaches to POS tagging\n",
        "\n",
        "One naive approach could be to tag all words/tokens with the most frequent POS tag found in the entire training set. We call this the ```_most_freq_tag_model_1```. In other words, if ```'NUM'``` is the most frequent POS tag in the training set then we tag every word/token in the test set to ```'NUM'```.  \n",
        "\n",
        "A better approach could be to tag each word/token to its most frequent POS tag as seen in the training set. We call this the ```_most_freq_tag_model_2```. In other words, for a word like ```'the'```, its most frequent POS tag as seen in the training set is ```'DET'```, so the model will tag ```'the'``` to ```'DET'``` whenever it sees it in the test set.\n",
        "\n",
        "In the following exercises we will build models for these approaches and also build a function to evaluate them.\n",
        "\n",
        "**We begin with approach 1: _most_freq_tag_model_1**\n",
        "\n",
        "In Q3, we had found the most frequent POS tag across the entire brown corpus (which included both the train and test sets).\n",
        "\n",
        "**Q4:** Check if the most frequent POS tag found in Q3 remains the most frequent POS tag in the ```train_set``` as well? We do this to ensure our naive approach is not informed by the ```test_set```. Save this tag to a variable called ```most_freq_tag```. Also, similar to Q2, save all the counts of tags in a python dictionary called ```count_tag```. Sanity check: ```count_tag['VERB']``` should be 170196)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B10kJHPD6RzY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "698b56df-1db9-48d7-9f97-6a75bba712df"
      },
      "source": [
        "count_tag = {}\n",
        "for sentence in train_set_Y:\n",
        "  for tag in sentence:\n",
        "    count_tag[tag] = count_tag.get(tag, 0) + 1\n",
        "\n",
        "sorted_count_tag = sorted(count_tag.items(), key=operator.itemgetter(1))\n",
        "\n",
        "most_freq_tag = sorted_count_tag[-1][0]\n",
        "print('The most frequent tag is', most_freq_tag)\n",
        "print(count_tag['VERB'])  # Sanity check"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The most frequent tag is NOUN\n",
            "170196\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA4UwwcdV2xq"
      },
      "source": [
        "**Q5:** Implement a naive model as a function ```_most_freq_tag_model_1(x)``` which tags every token in a given test set ```x``` with the most frequent POS tag found in Q4. Ensure the output of this function is of the same shape and size as the input so that it is easy to evaluate it later on. For example, the call ```_most_freq_tag_model_1(test_set_x)``` should return a list of tuples of most frequent POS tag of the same size and shape as ```test_set_Y```. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yW4p_ZP_VuFE"
      },
      "source": [
        "def _most_freq_tag_model_1(x):\n",
        "  \"\"\"Tags every token with the most frequent POS tag\"\"\"\n",
        "  Y = x.copy()\n",
        "  for i in range(len(Y)):\n",
        "    Y[i] = tuple([most_freq_tag]*len(x[i]))\n",
        "  return Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSrD17ksbOR8"
      },
      "source": [
        "**Q6:** Write a function ```_evaluate``` to evaluate the naive approach by measuring the accuracy of its predictions. Very simply, the function will take two list of lists of tags (of same shape) as arguments and count the number of times the tags are the same and divide this number with total number of tags. Sanity check: When you evaluate the naive approach ```_evaluate(_most_freq_tag_model_1(test_set_x), test_set_Y)``` you should get 23.8% accuracy. What is the accuracy of this model on the external test set?\n",
        "\n",
        "**Q6_optional:** You can extend the evaluation function to compute *precision* and *recall* and *F-measure* for each POS tag as a way to practice and understand these concepts. What is the precision, recall and F-measure for ```'NOUN'``` and any other tag?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TezC3duha6OS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cef79731-6695-43ec-dca6-b152e76c0cba"
      },
      "source": [
        "def _evaluate(predicted_Y, true_Y):\n",
        "  correct_tags = 0\n",
        "  total_tags = 0\n",
        "  no_error = True\n",
        "  if len(predicted_Y) == len(true_Y):\n",
        "    for i in range(len(predicted_Y)):\n",
        "      if len(predicted_Y[i]) != len(true_Y[i]):\n",
        "        print('ERROR: The predicted labels and the true labels are not of the same shape')\n",
        "        no_error = False\n",
        "        break\n",
        "      else:\n",
        "        for j in range(len(predicted_Y[i])):\n",
        "          total_tags += 1\n",
        "          if predicted_Y[i][j] == true_Y[i][j]:\n",
        "            correct_tags += 1\n",
        "  else:\n",
        "    print('ERROR: The predicted labels and the true labels are not of the same shape')\n",
        "    no_error = False\n",
        "\n",
        "  if no_error:\n",
        "    print('Accuracy =', float(correct_tags)/total_tags)\n",
        "\n",
        "_evaluate(_most_freq_tag_model_1(test_set_x), test_set_Y) # Sanity check\n",
        "\n",
        "print('\\nExternal test set:')\n",
        "_evaluate(_most_freq_tag_model_1(external_test_set_x), external_test_set_Y)\n",
        "\n",
        "print('\\nFor the optional question, for NOUN in the test_set \\nthe precision is the same as accuracy of 23.8% and recall is 100%.\\nFor other tags, precision and recall are 0.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 0.23814133591481124\n",
            "\n",
            "External test set:\n",
            "Accuracy = 0.2867316937502483\n",
            "\n",
            "For the optional question, for NOUN in the test_set \n",
            "the precision is the same as accuracy of 23.8% and recall is 100%.\n",
            "For other tags, precision and recall are 0.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXeuSAP-l8BD"
      },
      "source": [
        "**Now we build a model for approach 2: _most_freq_tag_model_2**\n",
        "\n",
        "For each word/token in the vocabulary, we need to retrieve the most frequent POS tag of that word as seen in the training set. Hence, we need to keep a count of every unique (token, tag) pair in the training set. We can do this using dictionaries in python. This will also prove to be useful later on in the exercise.\n",
        "\n",
        "**Q7:** Create a dictionary ```count_token_tag``` that stores the counts of every unique (token, tag) pair as seen in the ```train_set```. Sanity check: ```count_token_tag['the']['DET']``` should be 58337 and ```count_token_tag['the']['VERB']``` should return an error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db6-XG5MkHJc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "outputId": "07b89b69-c9a7-466b-b947-6ef4285dda01"
      },
      "source": [
        "count_token_tag = {}\n",
        "for i in range(len(train_set)):\n",
        "  for j in range(len(train_set[i])):\n",
        "    _token = train_set_x[i][j]\n",
        "    _tag = train_set_Y[i][j]\n",
        "    if _token not in count_token_tag.keys():\n",
        "      count_token_tag[_token] = {_tag: 1}\n",
        "    else:\n",
        "      count_token_tag[_token][_tag] \\\n",
        "      = count_token_tag[_token].get(_tag, 0) + 1\n",
        "\n",
        "# Sanity check\n",
        "print(count_token_tag['the']['DET'])\n",
        "print(count_token_tag['the']['VERB'])  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "58337\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-ace591d2cc47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Sanity check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_token_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'the'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DET'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_token_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'the'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'VERB'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: 'VERB'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYESHIXy5buL"
      },
      "source": [
        "**Q8:** Create a function ```_most_freq_tag``` which retrieves the most frequent POS tag for a given token as seen in the ```count_token_tag```. Sanity check: ```_most_freq_tag('the')``` is ```'DET'``` and ```_most_freq_tag('play')``` is ```'VERB'```. What about unseen words? What about ```_most_freq_tag('onomatopoeia')```? What can we do about it? One simple solution is to tag it with the ```most_freq_tag``` found in Q4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJdcCQgu4yvh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92c89e5d-c8d4-49b3-e84e-714302540443"
      },
      "source": [
        "def _most_freq_tag(_token):\n",
        "  \"\"\"Returns the most frequent POS tag of a _token as seen in emission_counts\"\"\"\n",
        "  if _token in count_token_tag.keys():\n",
        "    _tags = sorted(count_token_tag[_token].items(), key=operator.itemgetter(1))\n",
        "    return _tags[-1][0]\n",
        "  else:\n",
        "    return most_freq_tag # it is safer to bet that an unseen word is a NOUN. Why?\n",
        "\n",
        "print(_most_freq_tag('the'))\n",
        "print(_most_freq_tag('play'))\n",
        "print(_most_freq_tag('onomatopoeia'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DET\n",
            "VERB\n",
            "NOUN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uQ71dYK9p1p"
      },
      "source": [
        "**Q9:** Implement a naive model as a function ```_most_freq_tag_model_2(x)``` which tags every token in a test set ```x``` with its most frequent POS tag found in Q8. Ensure the output is of the same shape as ```x``` so that it is easy to evaluate. Evaluate the output of this model. Sanity check: ```_evaluate(_most_freq_tag_model_2(test_set_x), test_set_Y)``` should be 94.795%. Evaluate on the external test set too. Does the accuracy differ? Can you explain why?\r\n",
        "\r\n",
        "A: Treebank differs due to different distribution as compared to brown and many out of vocabulary words not seen in training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bp1xTvmy9NuO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f0210d5-4c53-4084-ba0d-258bf9e66a5c"
      },
      "source": [
        "def _most_freq_tag_model_2(x):\n",
        "  \"\"\"Tags every token with its most frequent POS tag\"\"\"\n",
        "  Y = x.copy()\n",
        "  for i in range(len(x)):\n",
        "    tag_tuple = [0]*len(x[i]) # 0 is just a placeholder\n",
        "    for j in range(len(x[i])):\n",
        "      _token = x[i][j]\n",
        "      tag_tuple[j] = _most_freq_tag(_token)\n",
        "    Y[i] = tuple(tag_tuple)\n",
        "  return Y\n",
        "\n",
        "_evaluate(_most_freq_tag_model_2(test_set_x), test_set_Y)\n",
        "_evaluate(_most_freq_tag_model_2(external_test_set_x), external_test_set_Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 0.9479513709910612\n",
            "Accuracy = 0.8311116850093369\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fgivqc8PC7gW"
      },
      "source": [
        "# 3. Hidden Markov Model (HMM)\n",
        "\n",
        "Let $W = (w_1, w_2, ..., w_n)$ be a sequence of observed words/tokens.\n",
        "\n",
        "Let $T = (t_1, t_2, ..., t_n)$ be a sequence of tags and $\\mathbb{T}_n$ be the set of all possible such sequences of tags of length $n$.\n",
        "\n",
        "**Q10:** How many such sequences of tags exist? In other words, whats the size of the set $\\mathbb{T}_n$?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uV87-ori8ls"
      },
      "source": [
        "# len(tagset) to the power n \n",
        "# which is 12^n in our case"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCgWpRE8jQMK"
      },
      "source": [
        "Our aim is to find the most probable sequence of tags $\\tilde{T}$ for the observed sequence of words $W$ such that the probability $P(\\tilde{T}|W) \\geq P(T|W)$ for all possible $T \\in \\mathbb{T}_n$.\n",
        "\n",
        "In other words, $\\tilde{T} = \\underset{T \\in \\mathbb{T}_n}{\\operatorname{argmax}}P(T|W)$\n",
        "\n",
        "Now, to estimate $P(T|W)$ we use the [Bayes' Theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem) followed by the [Chain Rule](https://en.wikipedia.org/wiki/Chain_rule_(probability)) followed by the Markov Assumption ($P(t_m|t_1, t_2, ..., t_{m-1}) \\approx P(t_m|t_{m-1})$) to get the following formulation for the HMM: \n",
        "\n",
        "$\\tilde{T} = \\underset{(t_1, t_2, ..., t_n) \\in \\mathbb{T}_n}{\\operatorname{argmax}}P(t_1)P(w_1|t_1)P(t_2|t_1)P(w_2|t_2)...P(t_{m}|t_{m-1})P(w_{m}|t_{m})...P(t_n|t_{n-1})P(w_n|t_n)$\n",
        "\n",
        "**Q11:** How do you interpret $P(t_m|t_{m-1})$ and $P(w_m|t_m)$? (A frequentist interpretation). How will you estimate these probabilities from the ```train_set```? How do you interpret $P(t_1)$ and estimate it from the ```train_set```?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqDA4FhysJZ-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "eaba7a61-cbb3-4117-b63e-0d237ed15e89"
      },
      "source": [
        "\"\"\"\n",
        "P(t_m|t_{m-1}) is the probability of making a transition \n",
        "from a state/tag t_{m-1} to the state/tag t_m.\n",
        "In other words, given that a state/tag is t_{m-1}\n",
        "then how often do we see the next state/tag to be t_m.\n",
        "We call this the state transition probability.\n",
        "We can estimate this by counting the number of times\n",
        "a state/tag t_m follows t_{m-1} in the training set and\n",
        "divide this number by the number of times we see t_{m-1}.\n",
        "\n",
        "P(t_m|t_{m-1}) = count(t_{m-1}, t_m) / count(t_{m-1})\n",
        "\n",
        "Eg; P(NOUN|VERB) = count(VERB, NOUN) / count(VERB)\n",
        "\n",
        "\n",
        "Similarly, P(w_m|t_m) is the probability of generating \n",
        "the word/token w_m given the state/tag is t_m.\n",
        "In other words, given that a state/tag is t_{m}\n",
        "then how often do we see it tagged to the word/token w_m.\n",
        "We call this the emission probability.\n",
        "We can estimate this by counting the number of times\n",
        "a word w_m is tagged t_m in the training set and divide\n",
        "this number by the number of times we see the state/tag t_m.\n",
        "\n",
        "P(w_m|t_m) = count(w_m, t_m) / count(t_m)\n",
        "\n",
        "Eg; P(the|VERB) = count(the, VERB) / count(VERB)\n",
        "\n",
        "\n",
        "P(t_1) is the probability of the first state/tag to be t1.\n",
        "We can estimate this by counting the number of times the\n",
        "first tag of a sequence is t_1 in the training set and \n",
        "divide it by the total number of sequences.\n",
        "\n",
        "P(t1) = count(t1 is the fist state/tag) / len(train_set)\n",
        "\n",
        "Eg; P(NOUN) = count(number of sentences that start with NOUN) /\n",
        "              total number of sentences in the training set\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nP(t_m|t_{m-1}) is the probability of making a transition \\nfrom a state/tag t_{m-1} to the state/tag t_m.\\nIn other words, given that a state/tag is t_{m-1}\\nthen how often do we see the next state/tag to be t_m.\\nWe call this the state transition probability.\\nWe can estimate this by counting the number of times\\na state/tag t_m follows t_{m-1} in the training set and\\ndivide this number by the number of times we see t_{m-1}.\\n\\nP(t_m|t_{m-1}) = count(t_{m-1}, t_m) / count(t_{m-1})\\n\\nEg; P(NOUN|VERB) = count(VERB, NOUN) / count(VERB)\\n\\n\\nSimilarly, P(w_m|t_m) is the probability of generating \\nthe word/token w_m given the state/tag is t_m.\\nIn other words, given that a state/tag is t_{m}\\nthen how often do we see it tagged to the word/token w_m.\\nWe call this the emission probability.\\nWe can estimate this by counting the number of times\\na word w_m is tagged t_m in the training set and divide\\nthis number by the number of times we see the state/tag t_m.\\n\\nP(w_m|t_m) = count(w_m, t_m) / count(t_m)\\n\\nEg; P(the|VERB) = count(the, VERB) / count(VERB)\\n\\n\\nP(t_1) is the probability of the first state/tag to be t1.\\nWe can estimate this by counting the number of times the\\nfirst tag of a sequence is t_1 in the training set and \\ndivide it by the total number of sequences.\\n\\nP(t1) = count(t1 is the fist state/tag) / len(train_set)\\n\\nEg; P(NOUN) = count(number of sentences that start with NOUN) /\\n              total number of sentences in the training set\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCxu56HN03GU"
      },
      "source": [
        "# 4. Viterbi algorithm (manual pen-paper example)\n",
        "Recall from Q10, the number of possible tag sequences (size of $\\mathbb{T}_n$) is enormous, especially for larger n and larger POS tagsets, which makes the brute force algorithm extremely inefficient. To find the solution $\\tilde{T}$ quickly, we use a dynamic programming algorithm called [Viterbi](https://en.wikipedia.org/wiki/Viterbi_algorithm).\n",
        "\n",
        "In this section, we will use the Viterbi algorithm to tag the following sentence manually:\n",
        "\n",
        "***time flies like an arrow***\n",
        "\n",
        "We assume some pre-computed transition probabilities and emission probabilities as follows:\n",
        "\n",
        "Initial probabilities:\n",
        "*   $P($NOUN$|start) = 0.5$\n",
        "*   $P($DET$|start) = 0.4$\n",
        "*   $P($VERB$|start) = 0.1$\n",
        "\n",
        "Transition probabilities:\n",
        "*   $P($NOUN$|$DET$) = 1.0$\n",
        "*   $P($NOUN$|$NOUN$) = 0.2$\n",
        "*   $P($VERB$|$NOUN$) = 0.7$\n",
        "*   $P($PREP$|$NOUN$) = 0.1$\n",
        "*   $P($DET$|$VERB$) = 0.4$\n",
        "*   $P($NOUN$|$VERB$) = 0.4$\n",
        "*   $P($PREP$|$VERB$) = 0.1$\n",
        "*   $P($VERB$|$VERB$) = 0.1$\n",
        "*   $P($DET$|$PREP$) = 0.6$\n",
        "*   $P($NOUN$|$PREP$) = 0.4$\n",
        "\n",
        "Emission probabilities:\n",
        "*   $P($time$|$NOUN$) = 0.7$\n",
        "*   $P($time$|$VERB$) = 0.1$\n",
        "*   $P($flies$|$NOUN$) = 0.4$\n",
        "*   $P($flies$|$VERB$) = 0.4$\n",
        "*   $P($like$|$VERB$) = 0.1$\n",
        "*   $P($like$|$PREP$) = 0.3$\n",
        "*   $P($an$|$DET$) = 0.6$\n",
        "*   $P($arrow$|$NOUN$) = 0.4$\n",
        "\n",
        "Next, we draw two tables called **viterbi** and **backpointer** of size (4, 5). (There are 4 unique POS tags in this tagset and there are 5 words in the observed sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8zEApuwFAng"
      },
      "source": [
        "**viterbi:**\n",
        "\n",
        "Tag\\Sent | ***time*** | ***flies*** | ***like*** | ***an*** | ***arrow***\n",
        "--- | --- | --- | --- | --- | ---\n",
        "DET | | | | | \n",
        "NOUN | | | | | \n",
        "VERB | | | | | \n",
        "PREP | | | | | "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5OFep45Gvol"
      },
      "source": [
        "**backpointer:**\n",
        "\n",
        "Tag\\Sent | ***time*** | ***flies*** | ***like*** | ***an*** | ***arrow***\n",
        "--- | --- | --- | --- | --- | ---\n",
        "DET | | | | | \n",
        "NOUN | | | | | \n",
        "VERB | | | | | \n",
        "PREP | | | | | "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNHPDwxgHGev"
      },
      "source": [
        "A cell, say corresponding to (***like***, VERB), in the ***viterbi*** table stores the maximum probability of all the sequences of tags until that point from left to right. For example,\n",
        "\n",
        "**viterbi**(***like***, VERB) $= \\underset{(t_1, t_2, \\text{VERB}) \\in \\mathbb{T}_n}{\\operatorname{max}}P(t_1)P($***time***$|t_1)P(t_2|t_1)P($***flies***$|t_2)P($VERB$|t_2)P($like$|$VERB$)$\n",
        "\n",
        "The same cell in the **backpointer** table stores the tag of the previous word from which the maximum viterbi score was arrived at. For example, if\n",
        "\n",
        "($\\tilde{t_1}$, $\\tilde{t_2}$, VERB) $= \\underset{(t_1, t_2, \\text{VERB}) \\in \\mathbb{T}_n}{\\operatorname{argmax}}P(t_1)P($***time***$|t_1)P(t_2|t_1)P($***flies***$|t_2)P($VERB$|t_2)P($like$|$VERB$)$\n",
        "\n",
        "then, **backpointer**(***like***, VERB) = $\\tilde{t_2}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTUUGYCCUFmV"
      },
      "source": [
        "**Q12: Let's begin by filling the first column of these tables.**\n",
        "\n",
        "As an example, **viterbi**(***time***, NOUN) = $\\underset{(\\text{NOUN}) \\in \\mathbb{T}_1}{\\operatorname{max}}P($NOUN$)P($***time***$|$NOUN) = $ P($NOUN$|start)P($***time***$|$NOUN) = $0.5 \\times 0.7$\n",
        "\n",
        "What is **viterbi**(***time***, VERB)?\n",
        "What is the first column of the **backpointer** table? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKg_V6rZUd_z"
      },
      "source": [
        "**viterbi:**\n",
        "\n",
        "Tag\\Sent | ***time*** | ***flies*** | ***like*** | ***an*** | ***arrow***\n",
        "--- | --- | --- | --- | --- | ---\n",
        "DET | ? | | | | \n",
        "NOUN |0.35 | | | | \n",
        "VERB | ? | | | | \n",
        "PREP | ? | | | | "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_Gs06yeUcbt"
      },
      "source": [
        "**backpointer:**\n",
        "\n",
        "Tag\\Sent | ***time*** | ***flies*** | ***like*** | ***an*** | ***arrow***\n",
        "--- | --- | --- | --- | --- | ---\n",
        "DET | - | | | | \n",
        "NOUN | start | | | | \n",
        "VERB | start | | | | \n",
        "PREP | - | | | | "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpTqLDliVsvN"
      },
      "source": [
        "**Now we fill the second column.**\n",
        "As an example, \n",
        "\n",
        "**viterbi**(***flies***, NOUN) = $\\underset{(t_1, \\text{NOUN}) \\in \\mathbb{T}_1}{\\operatorname{max}}P(t_1)P($***time***$|t_1)P($NOUN$|t_1)P($***flies***$|$NOUN) = $ \\underset{(t_1, \\text{NOUN}) \\in \\mathbb{T}_1}{\\operatorname{max}}$**viterbi**(***time***, $t_1$)$P($NOUN$|t_1)P($***flies***$|$NOUN)\n",
        "\n",
        "So all you need is the first column of the viterbi table and transition probabilites and the emission probabilities to compute the entries of the second column.\n",
        "\n",
        "**viterbi**(***time***, NOUN)$P($NOUN$|$NOUN$)P($***flies***$|$NOUN) = $0.35 \\times 0.2 \\times 0.4 = 0.028$\n",
        "\n",
        "**Q13:** Compute **viterbi**(***time***, VERB)$P($NOUN$|$VERB$)P($***flies***$|$NOUN). Is this greater than $0.028$? If yes, then enter the new computed value in the cell **viterbi**(***flies***, NOUN) and enter VERB in **backpointer**(***flies***, NOUN). If no, then enter $0.028$ in **viterbi**(***flies***, NOUN) and enter NOUN in **backpointer**(***flies***, NOUN). Do this for every cell in the second column.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JD7eSPEfyGT"
      },
      "source": [
        "**viterbi:**\n",
        "\n",
        "Tag\\Sent | ***time*** | ***flies*** | ***like*** | ***an*** | ***arrow***\n",
        "--- | --- | --- | --- | --- | ---\n",
        "DET | ? | | | | \n",
        "NOUN |0.35 | 0.028 or ? | | | \n",
        "VERB | ? | | | | \n",
        "PREP | ? | | | | "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx2ISuidgIMI"
      },
      "source": [
        "**backpointer:**\n",
        "\n",
        "Tag\\Sent | ***time*** | ***flies*** | ***like*** | ***an*** | ***arrow***\n",
        "--- | --- | --- | --- | --- | ---\n",
        "DET | - | | | | \n",
        "NOUN | start | NOUN or VERB?| | | \n",
        "VERB | start | | | | \n",
        "PREP | - | | | | "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkNRtexcgZTs"
      },
      "source": [
        "**Q14:** Now fill the third column using entries in the second column and the transition probabilities and emission probabilites provided.\n",
        "\n",
        "For your reference, the formulation is as follows:\n",
        "\n",
        "**viterbi**(***like***, VERB) = $ \\underset{(t_1, t_2, \\text{VERB}) \\in \\mathbb{T}_1}{\\operatorname{max}}$**viterbi**(***flies***, $t_2$)$P($VERB$|t_2)P($***like***$|$NOUN)\n",
        "\n",
        "And the **backpointer**(***like***, VERB) will be that tag $t_2$ which resulted in the maximum viterbi score of **viterbi**(***like***, VERB)\n",
        "\n",
        "**Q15:** Column by column fill the entire **viterbi** and **backpointer** tables below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZKPlfJginWp"
      },
      "source": [
        "**viterbi:**\n",
        "\n",
        "Tag\\Sent | ***time*** | ***flies*** | ***like*** | ***an*** | ***arrow***\n",
        "--- | --- | --- | --- | --- | ---\n",
        "DET | 0 | 0 | 0 | 0.0010584 | 0 \n",
        "NOUN | 0.35 | 0.028 | 0 | 0 | 0.00042336\n",
        "VERB | 0.01 | 0.098 | 0.00196 | 0 | 0\n",
        "PREP | 0 | 0 | 0.00294 | 0 | 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bMnSAIfjANw"
      },
      "source": [
        "**backpointer:**\n",
        "\n",
        "Tag\\Sent | ***time*** | ***flies*** | ***like*** | ***an*** | ***arrow***\n",
        "--- | --- | --- | --- | --- | ---\n",
        "DET | 0 | 0 | 0 | PREP | 0\n",
        "NOUN | s | NOUN | 0 | 0 | DET\n",
        "VERB | s | NOUN | NOUN | 0 | 0\n",
        "PREP | 0 | 0 | VERB | 0 | 0 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk5W1x3ajR0J"
      },
      "source": [
        "**Q16:** In the last column, **viterbi**(***arrow***, tag), find the tag $\\tilde{t}$ which has the greatest value. Select the corresponding cell in the other table **backpointer**(***arrow***, $\\tilde{t}$) and trace back the path to get the solution. What is the solution $\\tilde{T}$ sequence of tags according to the provided HMM model? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8YX5cAOlCrz"
      },
      "source": [
        "# solution = (NOUN, VERB, PREP, DET, NOUN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M9fIRnp0raw"
      },
      "source": [
        "**Optional Q:** What is the time-complexity of this Viterbi algorithm in terms of length of observed sentence and size of the tagset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi8JDUFK1PsT"
      },
      "source": [
        "# n = len(observed_sentence)\n",
        "# T = len(tagset) which 12 in our case\n",
        "\n",
        "# Time_Complexity == O(n x T^2)\n",
        "\n",
        "# Our version is less than that because we loop over only those states/tags \n",
        "# which have non-zero transition and non-zero emission probabilities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UU2XfLbDFQe"
      },
      "source": [
        "# 5. Computing transition probabilities and emission probabilities \n",
        "\n",
        "We will now compute the following from the training set,\n",
        "\n",
        "1.   Initial state probabilities ```p_initial```\n",
        "2.   Last state probabilities ```p_last``` (optional)\n",
        "3.   State transition probabilities ```p_transition```\n",
        "4.   Emission probabilities ```p_emission```\n",
        "\n",
        "Recall from Q11, how can we estimate these probabilities using counts from the training set?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7PV93uuFq0P"
      },
      "source": [
        "**Q17:** We will first compute the initial state probabilities for all tags and store it in ```p_initial``` dictionary. In other words, the probability of the first state/POS_tag is a ```'NOUN'``` is stored in ```p_initial['NOUN']```. The simple formula we use is:\n",
        "\n",
        "$P($NOUN$|start) = $ count(NOUN in first position in training set ) / size of training set\n",
        "\n",
        "What is the most probable and the least probable POS Tag of the first/initial state? (Sanity check: ```p_initial['PRON']``` should be 15.9%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLXC_2aXAVxw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb82c76d-4279-4f32-9657-16ad245a1c48"
      },
      "source": [
        "p_initial = {}\n",
        "for tags in train_set_Y:\n",
        "  first_tag = tags[0]\n",
        "  p_initial[first_tag] = p_initial.get(first_tag, 0) + 1\n",
        "\n",
        "denominator = len(train_set_Y) # this is also the sum of p_initials\n",
        "for tag in p_initial.keys():\n",
        "  p_initial[tag] = float(p_initial[tag]) / denominator\n",
        "\n",
        "sorted_p_initial = sorted(p_initial.items(), key=operator.itemgetter(1))\n",
        "\n",
        "print('Initial probabilities are', p_initial)\n",
        "print('The most probable tag is', sorted_p_initial[-1])\n",
        "print('The least probable tag is', sorted_p_initial[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial probabilities are {'PRON': 0.159111361079865, 'DET': 0.2127671541057368, 'ADP': 0.12268466441694788, 'NOUN': 0.14150731158605173, '.': 0.08845144356955381, 'ADV': 0.09193850768653919, 'NUM': 0.016666666666666666, 'VERB': 0.04559430071241095, 'CONJ': 0.048837645294338206, 'ADJ': 0.034683164604424443, 'PRT': 0.03723284589426322, 'X': 0.0005249343832020997}\n",
            "The most probable tag is ('DET', 0.2127671541057368)\n",
            "The least probable tag is ('X', 0.0005249343832020997)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8tQQO45K47k"
      },
      "source": [
        "**Q18 (optional):** Similarly, we can also compute the last state probabilities and store it in ```p_last``` dictionary. In other words, the probability of the last state/POS_tag is a 'NOUN' is stored in p_last['NOUN'].\n",
        "\n",
        "What is the most probable and the least probable POS tag of the last state? (Sanity check: ```p_initial['PRON']``` should be $9.3738 \\times 10^{-5}$)\n",
        "\n",
        "NOTE: In the current HMM formulation, we don't require ```p_last```. However, we could use it in another HMM formulation where it is an additional term. The same viterbi algorithm could be applied to find the tags of an observed sequence of words/tokens in that case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3FivzRaJxUn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15a1a241-0a8e-4bcc-be09-444774cd84db"
      },
      "source": [
        "p_last = {}\n",
        "for tags in train_set_Y:\n",
        "  last_tag = tags[-1]\n",
        "  p_last[last_tag] = p_last.get(last_tag, 0) + 1\n",
        "\n",
        "denominator = len(train_set_Y) # this is also the sum of p_lasts\n",
        "for tag in p_last.keys():\n",
        "  p_last[tag] = float(p_last[tag]) / denominator\n",
        "\n",
        "sorted_p_last = sorted(p_last.items(), key=operator.itemgetter(1))\n",
        "\n",
        "print('Last state probabilities are', p_last)\n",
        "print('The most probable tag is', sorted_p_last[-1])\n",
        "print('The least probable tag is', sorted_p_last[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Last state probabilities are {'.': 0.9791901012373453, 'NOUN': 0.016029246344206972, 'CONJ': 3.749531308586427e-05, 'VERB': 0.0017810273715785528, 'ADV': 0.00031871016122984625, 'NUM': 0.0013685789276340458, 'ADJ': 0.0005249343832020997, 'ADP': 0.00014998125234345707, 'PRT': 0.0001687289088863892, 'DET': 0.00031871016122984625, 'PRON': 9.373828271466067e-05, 'X': 1.8747656542932134e-05}\n",
            "The most probable tag is ('.', 0.9791901012373453)\n",
            "The least probable tag is ('X', 1.8747656542932134e-05)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvEKqbuxcf-W"
      },
      "source": [
        "**Q19:** To compute state transition probabilities, we first need to count the transitions and store it in a ```count_tag_tag``` dictionary of dictionaries. In other words, how often do we see a transition from ```NOUN``` to ```VERB``` in the training set will be stored in ```count_tag_tag['VERB']['NOUN']```. ***Should be read as count of Verb given Noun.***\n",
        "\n",
        "Count the transition counts in ```count_tag_tag``` dictionary of dictionaries. (Sanity check - ```count_tag_tag['VERB']['NOUN']``` should be 40760 )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_5fpnI1M66N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9db87936-c653-4160-d128-1d390b031e2e"
      },
      "source": [
        "count_tag_tag = {}\n",
        "for tags in train_set_Y:\n",
        "  if len(tags)>1: # if len(tags) == 1 then there is no transition of POS tags.\n",
        "    for j in range(len(tags)-1):\n",
        "      end_tag = tags[j]   # we count transiton from tag at j position (start_tag)\n",
        "      start_tag = tags[j+1]   # to the tag at j+1 position (end_tag)\n",
        "      if start_tag not in count_tag_tag.keys():\n",
        "        count_tag_tag[start_tag] = {end_tag: 1}\n",
        "      else:\n",
        "        count_tag_tag[start_tag][end_tag] \\\n",
        "        = count_tag_tag[start_tag].get(end_tag, 0) + 1\n",
        "\n",
        "print(count_tag_tag['VERB']['NOUN']) # count of transition from NOUN to VERB"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40760\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJbqYgBKkc4i"
      },
      "source": [
        "Now we have all the ingredients we need to compute the state transition probabilities ```p_transition``` and the emission probabilities ```p_emission```.\n",
        "\n",
        "As an example,\n",
        "\n",
        "```p_transition['VERB']['NOUN'] = count_tag_tag['VERB']['NOUN'] / count_tag['NOUN']``` \n",
        "\n",
        "***Should be read as probability of Verb given Noun.***\n",
        "\n",
        "**Q20:** Compute the state transition probabilities and store in ```p_transition``` dictionary of dictionaries which is of the same shape as ```count_tag_tag``` dictionary of dictionaries. (Sanity check - ```p_transition['VERB']['NOUN']``` should be 0.1588)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vy2KNaxhVkL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "660acfc7-2938-4050-c1be-2792d4e219bb"
      },
      "source": [
        "p_transition = {}\n",
        "for end_tag in count_tag_tag.keys():\n",
        "  for start_tag in count_tag_tag[end_tag].keys():\n",
        "    numerator = count_tag_tag[end_tag][start_tag]\n",
        "    denominator = count_tag[start_tag]\n",
        "    probability = float(numerator) / denominator\n",
        "    if end_tag not in p_transition.keys():\n",
        "      p_transition[end_tag] = {start_tag: probability}\n",
        "    else:\n",
        "      p_transition[end_tag][start_tag] = probability\n",
        "\n",
        "print(p_transition['VERB']['NOUN'])\n",
        "print(count_tag_tag['VERB']['NOUN'])\n",
        "print(count_tag['VERB'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.15883654955263896\n",
            "40760\n",
            "170196\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jR8InrhsRim"
      },
      "source": [
        "**Q21:** Similarly, compute the emission probabilities and store in ```p_emission``` dictionary of dictionaries which is of the same shape as ```count_token_tag``` from Q7. (Sanity check - ```p_emission['the']['DET']```, which is probability of 'the' given DET, should be 0.4574)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YzrpCwunpn5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdf4d717-776e-43c8-b2eb-9f9ac432cd7c"
      },
      "source": [
        "p_emission = {}\n",
        "for _token in count_token_tag.keys():\n",
        "  for _tag in count_token_tag[_token].keys():\n",
        "    numerator = count_token_tag[_token][_tag]\n",
        "    denominator = count_tag[_tag]\n",
        "    probability = float(numerator) / denominator\n",
        "    if _token not in p_emission.keys():\n",
        "      p_emission[_token] = {_tag: probability}\n",
        "    else:\n",
        "      p_emission[_token][_tag] = probability\n",
        "\n",
        "print(p_emission['the']['DET'])\n",
        "print(count_token_tag['the']['DET'])\n",
        "print(count_tag['DET'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.45743387882161984\n",
            "58337\n",
            "127531\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8_xFWak0Zwf"
      },
      "source": [
        "# 6. Viterbi algorithm\n",
        "\n",
        "Pseudo-code examples of the [Viterbi algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm) are available in the lecture slides and online.\n",
        "\n",
        "We will create a function ```_HMMtagger(sentence)``` which takes an input sequence/tuple of words/tokens ```sentence``` and returns an output sequence/tuple of POS tags of the same size. This output is the most probable tag sequence of the input sentence according to our HMM model. We will implement the Viterbi algorithm to find the most probable tag sequence and use the global variable python dictionaries ```p_initial```, ```p_transition```, and ```p_emission``` from the previous section 5 to do so.\n",
        "\n",
        "**Q22:** Complete the code below at specified places. Look out for ```## ENTER YOUR ANSWER HERE```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdVKXWIztzRp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97601fda-6eff-4f90-f718-af8590f56c0d"
      },
      "source": [
        "import numpy as np # for logarithms\n",
        "\n",
        "def _HMMtagger(sentence):\n",
        "  \"\"\"Returns the most probable tag sequece of a sentence as per HMM probabilites\"\"\"\n",
        "  # Creating viterbi and backpointer matrices. \n",
        "  # For simplicity, we create a list of dictionaries. Dictionaries appended later.  \n",
        "  viterbi = [] \n",
        "  backpointer = []\n",
        "\n",
        "  # Initialize \n",
        "  # Filling the first column which is the the first dictionary in the above lists\n",
        "  viterbi.append({})\n",
        "  backpointer.append({})\n",
        "  first_word = sentence[0]\n",
        "  if first_word in p_emission.keys():\n",
        "    for tag in p_emission[first_word].keys():\n",
        "      if tag in p_initial.keys():\n",
        "        viterbi[0][tag] = np.log(p_initial[tag]) + np.log(p_emission[first_word][tag])  \n",
        "        # we work with log to handle small numbers. multiplication becomes addition in log scale.\n",
        "        backpointer[0][tag] = '<s>'\n",
        "  else:\n",
        "    # If first_word not in p_emission.keys() then its because its an unseen word.\n",
        "    # For unseen words we assume it is equally likely to be emitted by any tag.\n",
        "    # We could have included an UNKNOWN token in the training set \n",
        "    # but that is suppose to be done during pre-processing step.  \n",
        "    # Hence, for now, we will ignore p_emission[first_word][tag] \n",
        "    # when computing viterbi[0][tag]\n",
        "    for tag in p_initial.keys():\n",
        "      viterbi[0][tag] = np.log(p_initial[tag]) ## ENTER YOUR ANSWER HERE  \n",
        "      backpointer[0][tag] = '<s>' \n",
        "  \n",
        "  # Filling other columns\n",
        "  for i in range(len(sentence)-1):\n",
        "    time_step = i+1\n",
        "    viterbi.append({})\n",
        "    backpointer.append({})\n",
        "    word = sentence[time_step]\n",
        "    if word in p_emission.keys():\n",
        "      for tag in p_emission[word].keys():\n",
        "        viterbi[time_step][tag] = float(\"-inf\")    # to be updated in the following loop\n",
        "        backpointer[time_step][tag] = 'placeholder'  # to be updated in following loop\n",
        "        for previous_tag in viterbi[time_step-1].keys():\n",
        "          if previous_tag in p_transition[tag].keys():\n",
        "            new_viterbi = viterbi[time_step-1][previous_tag] +\\\n",
        "                          np.log(p_transition[tag][previous_tag]) +\\\n",
        "                          np.log(p_emission[word][tag])  ## ENTER YOUR ANSWER HERE\n",
        "            if new_viterbi > viterbi[time_step][tag]:\n",
        "              viterbi[time_step][tag] = new_viterbi\n",
        "              backpointer[time_step][tag] = previous_tag  ## ENTER YOUR ANSWER HERE\n",
        "    else:\n",
        "      # Just like before, the word is not seen in the training set\n",
        "      # So we assume it is equally likely to be emitted by any tag\n",
        "      for tag in tagset:\n",
        "        # tagset from Q1\n",
        "        viterbi[time_step][tag] = float(\"-inf\")    # to be updated in the following loop\n",
        "        backpointer[time_step][tag] = 'placeholder'  # to be updated in following loop\n",
        "        for previous_tag in viterbi[time_step-1].keys():\n",
        "          if previous_tag in p_transition[tag].keys():\n",
        "            new_viterbi = viterbi[time_step-1][previous_tag] + np.log(p_transition[tag][previous_tag])  ## ENTER YOUR ANSWER HERE. ignore p_emission[word][tag]\n",
        "            if new_viterbi > viterbi[time_step][tag]:\n",
        "              viterbi[time_step][tag] = new_viterbi\n",
        "              backpointer[time_step][tag] = previous_tag  ## ENTER YOUR ANSWER HERE\n",
        "  \n",
        "  # Retracing the most probable path and returning it\n",
        "  solution = []\n",
        "  final_time_step = len(sentence)-1\n",
        "  final_probability = float(\"-inf\")  # to be updated in the following loop\n",
        "  final_tag = 'placeholder'  # to be updated in the following loop\n",
        "  for tag in viterbi[final_time_step].keys():\n",
        "    if viterbi[final_time_step][tag] > final_probability:\n",
        "      final_probability = viterbi[final_time_step][tag]\n",
        "      final_tag = tag\n",
        "  \n",
        "  while final_tag != '<s>':\n",
        "    solution.append(final_tag)\n",
        "    final_tag = backpointer[final_time_step][final_tag]  ## ENTER YOUR ANSWER HERE \n",
        "    final_time_step = final_time_step - 1\n",
        "  \n",
        "  solution.reverse()\n",
        "\n",
        "  return solution\n",
        "\n",
        "print(_HMMtagger(['This', 'is', 'just', 'an', 'example', 'to', 'check', 'if', 'the', 'tagger', 'works', 'okay', 'or', 'not']), 'Sanity check')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['DET', 'VERB', 'ADV', 'DET', 'NOUN', 'PRT', 'VERB', 'ADP', 'DET', 'NOUN', 'VERB', 'ADJ', 'CONJ', 'ADV'] Sanity check\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cfvpto7QoWn"
      },
      "source": [
        "**Q23:** Evaluate the HMM tagger against ```test_set``` and ```external_test_set```. How does it compare to the baseline naive models? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4bIo0x6Rjcq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb3895f6-f988-4a7b-95e8-ac0b044e111f"
      },
      "source": [
        "def _HMM(x):\n",
        "  \"\"\"Tags every sentence in x with the _HMMtagger\"\"\"\n",
        "  Y = x.copy()\n",
        "  for i in range(len(x)):\n",
        "    Y[i] = _HMMtagger(x[i])\n",
        "  return Y\n",
        "\n",
        "_evaluate(_HMM(test_set_x), test_set_Y)\n",
        "_evaluate(_HMM(external_test_set_x), external_test_set_Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 0.9625476169522637\n",
            "Accuracy = 0.8131729508522388\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSAr_Clej5XN"
      },
      "source": [
        "NOTE: \r\n",
        "\r\n",
        "To solve the previous pen-and-paper example with the above Viterbi algorithm code:\r\n",
        "\r\n",
        "*   change the function definition to: *def _HMMtagger(sentence, p_initial, p_transition, p_emission):*\r\n",
        "\r\n",
        "*   initialise probability tables as follows and run the _HMM Tagger function\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8Mor6Jzj2I2"
      },
      "source": [
        "# # initialise probability tables \r\n",
        "# tmp_initial = {'NOUN': 0.5, 'DET': 0.4, 'VERB': 0.1}\r\n",
        "# tmp_transition = {'NOUN':{'DET': 1.0, 'NOUN': 0.2, 'VERB': 0.4, 'PREP': 0.4}, 'VERB': {'NOUN': 0.7, 'VERB': 0.1}, 'PREP': {'NOUN': 0.1, 'VERB': 0.1}, 'DET': {'VERB': 0.4, 'PREP': 0.6}}\r\n",
        "# tmp_emission = {'time':{'NOUN': 0.7, 'VERB': 0.1}, 'flies':{'NOUN': 0.4, 'VERB': 0.4}, 'like':{'VERB': 0.1, 'PREP': 0.3}, 'an':{'DET': 0.6}, 'arrow':{'NOUN': 0.4}}"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz0arXRDkZty"
      },
      "source": [
        "# print(_HMMtagger(['time', 'flies', 'like', 'an', 'arrow'], tmp_initial, tmp_transition, tmp_emission))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4M4rihzkh9s"
      },
      "source": [
        "The solution should be: ['NOUN', 'VERB', 'PREP', 'DET', 'NOUN']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rMdBb9vzo-W"
      },
      "source": [
        "# 7. (optional) RNN-based Tagging model\n",
        "\n",
        "In the previous tutorial we explored a simple RNN-based Language Model which maximizes the likelihood of the next word given the history of previous words in the sentence. Pictorially, the model looks as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiwL2p9R1KqS"
      },
      "source": [
        "![RNNLM](https://raw.githubusercontent.com/torch/torch.github.io/master/blog/_posts/images/rnnlm.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4vokrZX1gqw"
      },
      "source": [
        "**Q24:** If we replace the target words (see the above picture) with the POS tags and then train a RNN model then we get an RNN-based POS Tagger. Can you implement such a tagger? How much does the accuracy change?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk6ROgY62ga8"
      },
      "source": [
        "# Doubts? Questions? Need help with this tutorial?\n",
        "Contact me at clala@imperial.ac.uk or chiraag.r.lala@gmail.com :)"
      ]
    }
  ]
}